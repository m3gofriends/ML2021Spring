{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Boss baseline : 0.85100\n",
        "### ---Public : 0.82322, Private : 0.83610---\n",
        "### Strong baseline : 0.80802\n",
        "### Medium baseline : 0.69398\n",
        "### Simple baseline : 0.43782"
      ],
      "metadata": {
        "id": "LTrV0vagf0Ao"
      },
      "id": "LTrV0vagf0Ao"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b1ad6c2",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2022-10-20T07:28:07.710139Z",
          "iopub.status.busy": "2022-10-20T07:28:07.709532Z",
          "iopub.status.idle": "2022-10-20T07:28:25.733025Z",
          "shell.execute_reply": "2022-10-20T07:28:25.731853Z"
        },
        "papermill": {
          "duration": 18.031697,
          "end_time": "2022-10-20T07:28:25.735774",
          "exception": false,
          "start_time": "2022-10-20T07:28:07.704077",
          "status": "completed"
        },
        "tags": [],
        "id": "0b1ad6c2",
        "outputId": "f88ce25a-9b64-438a-b779-754233ca6204"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers==4.5.0\r\n",
            "  Downloading transformers-4.5.0-py3-none-any.whl (2.1 MB)\r\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0) (1.21.6)\r\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0) (2021.11.10)\r\n",
            "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0) (4.12.0)\r\n",
            "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0) (0.0.53)\r\n",
            "Collecting tokenizers<0.11,>=0.10.1\r\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\r\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0) (2.28.1)\r\n",
            "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0) (3.7.1)\r\n",
            "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0) (21.3)\r\n",
            "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0) (4.64.0)\r\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.5.0) (4.3.0)\r\n",
            "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.5.0) (3.8.0)\r\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers==4.5.0) (3.0.9)\r\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0) (2022.6.15.2)\r\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0) (3.3)\r\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0) (2.1.0)\r\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0) (1.26.12)\r\n",
            "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.5.0) (1.15.0)\r\n",
            "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.5.0) (1.0.1)\r\n",
            "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.5.0) (8.0.4)\r\n",
            "Installing collected packages: tokenizers, transformers\r\n",
            "  Attempting uninstall: tokenizers\r\n",
            "    Found existing installation: tokenizers 0.12.1\r\n",
            "    Uninstalling tokenizers-0.12.1:\r\n",
            "      Successfully uninstalled tokenizers-0.12.1\r\n",
            "  Attempting uninstall: transformers\r\n",
            "    Found existing installation: transformers 4.20.1\r\n",
            "    Uninstalling transformers-4.20.1:\r\n",
            "      Successfully uninstalled transformers-4.20.1\r\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
            "allennlp 2.10.0 requires protobuf==3.20.0, but you have protobuf 3.19.4 which is incompatible.\u001b[0m\u001b[31m\r\n",
            "\u001b[0mSuccessfully installed tokenizers-0.10.3 transformers-4.5.0\r\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# You are allowed to change version of transformers or use other toolkits\n",
        "!pip install transformers==4.5.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b44bd976",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-20T07:28:25.747840Z",
          "iopub.status.busy": "2022-10-20T07:28:25.747525Z",
          "iopub.status.idle": "2022-10-20T07:28:32.345969Z",
          "shell.execute_reply": "2022-10-20T07:28:32.344692Z"
        },
        "papermill": {
          "duration": 6.612682,
          "end_time": "2022-10-20T07:28:32.353919",
          "exception": false,
          "start_time": "2022-10-20T07:28:25.741237",
          "status": "completed"
        },
        "tags": [],
        "id": "b44bd976"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset \n",
        "from transformers import AdamW, BertForQuestionAnswering, BertTokenizerFast, get_linear_schedule_with_warmup\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import random\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Fix random seed for reproducibility\n",
        "def same_seeds(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "same_seeds(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adfbd4a7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-20T07:28:32.374656Z",
          "iopub.status.busy": "2022-10-20T07:28:32.373586Z",
          "iopub.status.idle": "2022-10-20T07:28:43.450685Z",
          "shell.execute_reply": "2022-10-20T07:28:43.449427Z"
        },
        "papermill": {
          "duration": 11.089696,
          "end_time": "2022-10-20T07:28:43.453747",
          "exception": false,
          "start_time": "2022-10-20T07:28:32.364051",
          "status": "completed"
        },
        "tags": [],
        "id": "adfbd4a7",
        "outputId": "2680cae5-759a-4160-8415-58f2520af8d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting accelerate==0.2.0\r\n",
            "  Downloading accelerate-0.2.0-py3-none-any.whl (47 kB)\r\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m178.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hRequirement already satisfied: pyaml>=20.4.0 in /opt/conda/lib/python3.7/site-packages (from accelerate==0.2.0) (21.10.1)\r\n",
            "Requirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from accelerate==0.2.0) (1.11.0)\r\n",
            "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from pyaml>=20.4.0->accelerate==0.2.0) (6.0)\r\n",
            "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.4.0->accelerate==0.2.0) (4.3.0)\r\n",
            "Installing collected packages: accelerate\r\n",
            "  Attempting uninstall: accelerate\r\n",
            "    Found existing installation: accelerate 0.12.0\r\n",
            "    Uninstalling accelerate-0.12.0:\r\n",
            "      Successfully uninstalled accelerate-0.12.0\r\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
            "catalyst 22.4 requires accelerate>=0.5.1, but you have accelerate 0.2.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
            "\u001b[0mSuccessfully installed accelerate-0.2.0\r\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Change \"fp16_training\" to True to support automatic mixed precision training (fp16)\n",
        "fp16_training = True\n",
        "\n",
        "if fp16_training:\n",
        "    !pip install accelerate==0.2.0\n",
        "    from accelerate import Accelerator\n",
        "    accelerator = Accelerator(fp16=True)\n",
        "    device = accelerator.device\n",
        "\n",
        "# Documentation for the toolkit:  https://huggingface.co/docs/accelerate/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a88bb580",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-20T07:28:43.472553Z",
          "iopub.status.busy": "2022-10-20T07:28:43.472179Z",
          "iopub.status.idle": "2022-10-20T07:30:57.479800Z",
          "shell.execute_reply": "2022-10-20T07:30:57.478807Z"
        },
        "papermill": {
          "duration": 134.019649,
          "end_time": "2022-10-20T07:30:57.482230",
          "exception": false,
          "start_time": "2022-10-20T07:28:43.462581",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "c7641c5302b1429eaba8207783cb16b7",
            "1f2c206aeed6477db96718d677e44167",
            "2f63b415c4c44f3881507c06c4c6be36",
            "b747f913344e40cd987fb1606398ff62"
          ]
        },
        "id": "a88bb580",
        "outputId": "e34f8519-8f30-480e-c64e-d082604b81d7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c7641c5302b1429eaba8207783cb16b7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/634 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1f2c206aeed6477db96718d677e44167",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.30G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2f63b415c4c44f3881507c06c4c6be36",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/110k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b747f913344e40cd987fb1606398ff62",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/269k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model = BertForQuestionAnswering.from_pretrained(\"luhua/chinese_pretrain_mrc_roberta_wwm_ext_large\").to(device)\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"luhua/chinese_pretrain_mrc_roberta_wwm_ext_large\")\n",
        "\n",
        "# You can safely ignore the warning message (it pops up because new prediction heads for QA are initialized randomly)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e85c1f3",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-20T07:30:57.496470Z",
          "iopub.status.busy": "2022-10-20T07:30:57.496080Z",
          "iopub.status.idle": "2022-10-20T07:30:58.228291Z",
          "shell.execute_reply": "2022-10-20T07:30:58.227217Z"
        },
        "papermill": {
          "duration": 0.742792,
          "end_time": "2022-10-20T07:30:58.231349",
          "exception": false,
          "start_time": "2022-10-20T07:30:57.488557",
          "status": "completed"
        },
        "tags": [],
        "id": "9e85c1f3"
      },
      "outputs": [],
      "source": [
        "def read_data(file):\n",
        "    with open(file, 'r', encoding=\"utf-8\") as reader:\n",
        "        data = json.load(reader)\n",
        "    return data[\"questions\"], data[\"paragraphs\"]\n",
        "\n",
        "train_questions, train_paragraphs = read_data(\"../input/ml2021-spring-hw7/hw7_train.json\")\n",
        "dev_questions, dev_paragraphs = read_data(\"../input/ml2021-spring-hw7/hw7_dev.json\")\n",
        "test_questions, test_paragraphs = read_data(\"../input/ml2021-spring-hw7/hw7_test.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdb01a1a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-20T07:30:58.245046Z",
          "iopub.status.busy": "2022-10-20T07:30:58.244719Z",
          "iopub.status.idle": "2022-10-20T07:31:07.849055Z",
          "shell.execute_reply": "2022-10-20T07:31:07.848058Z"
        },
        "papermill": {
          "duration": 9.614091,
          "end_time": "2022-10-20T07:31:07.851833",
          "exception": false,
          "start_time": "2022-10-20T07:30:58.237742",
          "status": "completed"
        },
        "tags": [],
        "id": "bdb01a1a"
      },
      "outputs": [],
      "source": [
        "# Tokenize questions and paragraphs separately\n",
        "# 「add_special_tokens」 is set to False since special tokens will be added when tokenized questions and paragraphs are combined in datset __getitem__ \n",
        "\n",
        "train_questions_tokenized = tokenizer([train_question[\"question_text\"] for train_question in train_questions], add_special_tokens=False)\n",
        "dev_questions_tokenized = tokenizer([dev_question[\"question_text\"] for dev_question in dev_questions], add_special_tokens=False)\n",
        "test_questions_tokenized = tokenizer([test_question[\"question_text\"] for test_question in test_questions], add_special_tokens=False) \n",
        "\n",
        "train_paragraphs_tokenized = tokenizer(train_paragraphs, add_special_tokens=False)\n",
        "dev_paragraphs_tokenized = tokenizer(dev_paragraphs, add_special_tokens=False)\n",
        "test_paragraphs_tokenized = tokenizer(test_paragraphs, add_special_tokens=False)\n",
        "\n",
        "# You can safely ignore the warning message as tokenized sequences will be futher processed in datset __getitem__ before passing to model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d474be3",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-20T07:31:07.865681Z",
          "iopub.status.busy": "2022-10-20T07:31:07.865373Z",
          "iopub.status.idle": "2022-10-20T07:31:07.890162Z",
          "shell.execute_reply": "2022-10-20T07:31:07.889189Z"
        },
        "papermill": {
          "duration": 0.034094,
          "end_time": "2022-10-20T07:31:07.892137",
          "exception": false,
          "start_time": "2022-10-20T07:31:07.858043",
          "status": "completed"
        },
        "tags": [],
        "id": "0d474be3"
      },
      "outputs": [],
      "source": [
        "class QA_Dataset(Dataset):\n",
        "    def __init__(self, split, questions, tokenized_questions, tokenized_paragraphs):\n",
        "        self.split = split\n",
        "        self.questions = questions\n",
        "        self.tokenized_questions = tokenized_questions\n",
        "        self.tokenized_paragraphs = tokenized_paragraphs\n",
        "        self.max_question_len = 40\n",
        "        self.max_paragraph_len = 150\n",
        "        \n",
        "        ##### TODO: Change value of doc_stride #####\n",
        "        self.doc_stride = 50\n",
        "\n",
        "        # Input sequence length = [CLS] + question + [SEP] + paragraph + [SEP]\n",
        "        self.max_seq_len = 1 + self.max_question_len + 1 + self.max_paragraph_len + 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.questions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        question = self.questions[idx]\n",
        "        tokenized_question = self.tokenized_questions[idx]\n",
        "        tokenized_paragraph = self.tokenized_paragraphs[question[\"paragraph_id\"]]\n",
        "\n",
        "        ##### TODO: Preprocessing #####\n",
        "        # Hint: How to prevent model from learning something it should not learn\n",
        "\n",
        "        if self.split == \"train\":\n",
        "            # Convert answer's start/end positions in paragraph_text to start/end positions in tokenized_paragraph  \n",
        "            answer_start_token = tokenized_paragraph.char_to_token(question[\"answer_start\"])\n",
        "            answer_end_token = tokenized_paragraph.char_to_token(question[\"answer_end\"])\n",
        "\n",
        "            # A single window is obtained by slicing the portion of paragraph containing the answer\n",
        "            #mid = (answer_start_token + answer_end_token) // 2\n",
        "            #paragraph_start = max(0, min(mid - self.max_seq_len // 2, len(tokenized_paragraph) - self.max_seq_len))\n",
        "            paragraph_start = max(0, random.randint(answer_end_token-self.max_paragraph_len, answer_start_token))\n",
        "            paragraph_end = paragraph_start + self.max_paragraph_len\n",
        "            \n",
        "            # Slice question/paragraph and add special tokens (101: CLS, 102: SEP)\n",
        "            input_ids_question = [101] + tokenized_question.ids[:self.max_question_len] + [102] \n",
        "            input_ids_paragraph = tokenized_paragraph.ids[paragraph_start : paragraph_end] + [102]\n",
        "            \n",
        "            # Convert answer's start/end positions in tokenized_paragraph to start/end positions in the window  \n",
        "            answer_start_token += len(input_ids_question) - paragraph_start\n",
        "            answer_end_token += len(input_ids_question) - paragraph_start\n",
        "            \n",
        "            # Pad sequence and obtain inputs to model \n",
        "            input_ids, token_type_ids, attention_mask = self.padding(input_ids_question, input_ids_paragraph)\n",
        "            return torch.tensor(input_ids), torch.tensor(token_type_ids), torch.tensor(attention_mask), answer_start_token, answer_end_token\n",
        "\n",
        "        # Validation/Testing\n",
        "        else:\n",
        "            input_ids_list, token_type_ids_list, attention_mask_list = [], [], []\n",
        "            \n",
        "            # Paragraph is split into several windows, each with start positions separated by step \"doc_stride\"\n",
        "            for i in range(0, len(tokenized_paragraph), self.doc_stride):\n",
        "                \n",
        "                # Slice question/paragraph and add special tokens (101: CLS, 102: SEP)\n",
        "                input_ids_question = [101] + tokenized_question.ids[:self.max_question_len] + [102]\n",
        "                input_ids_paragraph = tokenized_paragraph.ids[i : i + self.max_paragraph_len] + [102]\n",
        "                \n",
        "                # Pad sequence and obtain inputs to model\n",
        "                input_ids, token_type_ids, attention_mask = self.padding(input_ids_question, input_ids_paragraph)\n",
        "                \n",
        "                input_ids_list.append(input_ids)\n",
        "                token_type_ids_list.append(token_type_ids)\n",
        "                attention_mask_list.append(attention_mask)\n",
        "            \n",
        "            return torch.tensor(input_ids_list), torch.tensor(token_type_ids_list), torch.tensor(attention_mask_list)\n",
        "\n",
        "    def padding(self, input_ids_question, input_ids_paragraph):\n",
        "        # Pad zeros if sequence length is shorter than max_seq_len\n",
        "        padding_len = self.max_seq_len - len(input_ids_question) - len(input_ids_paragraph)\n",
        "        # Indices of input sequence tokens in the vocabulary\n",
        "        input_ids = input_ids_question + input_ids_paragraph + [0] * padding_len\n",
        "        # Segment token indices to indicate first and second portions of the inputs. Indices are selected in [0, 1]\n",
        "        token_type_ids = [0] * len(input_ids_question) + [1] * len(input_ids_paragraph) + [0] * padding_len\n",
        "        # Mask to avoid performing attention on padding token indices. Mask values selected in [0, 1]\n",
        "        attention_mask = [1] * (len(input_ids_question) + len(input_ids_paragraph)) + [0] * padding_len\n",
        "        \n",
        "        return input_ids, token_type_ids, attention_mask\n",
        "\n",
        "train_set = QA_Dataset(\"train\", train_questions, train_questions_tokenized, train_paragraphs_tokenized)\n",
        "dev_set = QA_Dataset(\"dev\", dev_questions, dev_questions_tokenized, dev_paragraphs_tokenized)\n",
        "test_set = QA_Dataset(\"test\", test_questions, test_questions_tokenized, test_paragraphs_tokenized)\n",
        "\n",
        "train_batch_size = 16\n",
        "\n",
        "# Note: Do NOT change batch size of dev_loader / test_loader !\n",
        "# Although batch size=1, it is actually a batch consisting of several windows from the same QA pair\n",
        "train_loader = DataLoader(train_set, batch_size=train_batch_size, shuffle=True, pin_memory=True)\n",
        "dev_loader = DataLoader(dev_set, batch_size=1, shuffle=False, pin_memory=True)\n",
        "test_loader = DataLoader(test_set, batch_size=1, shuffle=False, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9657c081",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-20T07:31:07.905697Z",
          "iopub.status.busy": "2022-10-20T07:31:07.904889Z",
          "iopub.status.idle": "2022-10-20T07:31:07.912178Z",
          "shell.execute_reply": "2022-10-20T07:31:07.911310Z"
        },
        "papermill": {
          "duration": 0.016014,
          "end_time": "2022-10-20T07:31:07.914165",
          "exception": false,
          "start_time": "2022-10-20T07:31:07.898151",
          "status": "completed"
        },
        "tags": [],
        "id": "9657c081"
      },
      "outputs": [],
      "source": [
        "def evaluate(data, output):\n",
        "    ##### TODO: Postprocessing #####\n",
        "    # There is a bug and room for improvement in postprocessing \n",
        "    # Hint: Open your prediction file to see what is wrong \n",
        "    \n",
        "    answer = ''\n",
        "    max_prob = float('-inf')\n",
        "    num_of_windows = data[0].shape[1]\n",
        "    \n",
        "    for k in range(num_of_windows):\n",
        "        # Obtain answer by choosing the most probable start position / end position\n",
        "        start_prob, start_index = torch.max(output.start_logits[k], dim=0)\n",
        "        end_prob, end_index = torch.max(output.end_logits[k], dim=0)\n",
        "        \n",
        "        # Probability of answer is calculated as sum of start_prob and end_prob\n",
        "        prob = start_prob + end_prob\n",
        "        \n",
        "        # Replace answer if calculated probability is larger than previous windows\n",
        "        if prob > max_prob:\n",
        "            max_prob = prob\n",
        "            # Convert tokens to chars (e.g. [1920, 7032] --> \"大 金\")\n",
        "            answer = tokenizer.decode(data[0][0][k][start_index : end_index + 1])\n",
        "    \n",
        "    # Remove spaces in answer (e.g. \"大 金\" --> \"大金\")\n",
        "    answer = answer.replace('[UNK]','').replace('[SEP]','').replace(' ','')\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c558329",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-20T07:31:07.927210Z",
          "iopub.status.busy": "2022-10-20T07:31:07.926918Z",
          "iopub.status.idle": "2022-10-20T14:41:24.945855Z",
          "shell.execute_reply": "2022-10-20T14:41:24.944961Z"
        },
        "papermill": {
          "duration": 25817.02803,
          "end_time": "2022-10-20T14:41:24.947988",
          "exception": false,
          "start_time": "2022-10-20T07:31:07.919958",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "c38d2646367b4796b05c2c52dc77c0b1",
            "a9f589b4529b4d8995db859b4855c55a",
            "dfecbd78c8f048219d9f92aa8ee6a435",
            "af0ad5976e2c4f3799b5ad3524664a02",
            "e1b25207c0dc424e8478d5f619d53e86",
            "725caa955ec84cca859f79b1e7e63a90",
            "46a36bcf98a7499ea3ed3a924910fe24",
            "831495f1322d4524b184d0d703555c19",
            "8f5134e811c44aad92115c9aaab85c54",
            "a35861fae94a473fa6d7f241923509ee",
            "01ba8ee4dc5a4414a2df7309ff1e392c",
            "cf8870ffda6b4c99804a6ba0f9396c05",
            "e02721e2f2d540bf9c5b360bea581abb",
            "ff39147453e94a99b2064253136684a8",
            "151630a75627436996b1170021520c0c",
            "434fa03d400a40b9821df860a95beb6b",
            "10b931de306a4658832eea33ac5dccb1",
            "ad41c7a3616f4e13be7e68a8790a78c8",
            "4c5165fa951d49ee84a7f66f37dad8e4",
            "910c2e963d374cc3b75821f8d59303bb",
            "29f578febb134990a7f5c95b2e7d63b9",
            "339d3868784d434ea362ed13b4d5a9bd",
            "3a1e73ff554b4b66ae03da6535520698",
            "3dbc9f0d21cf406b9a81b21884bcd4ba",
            "daab6338c1f54688bf4a56b26a3545d5",
            "26ed81f0a25242dfb0a893573fdf7493",
            "b23cd16f3b8543739c118b4565c56023",
            "707c177dd6b747218738d12d73271b60",
            "8d7fe83bf3a144bb9769e00e8e67319c"
          ]
        },
        "id": "2c558329",
        "outputId": "aed64281-7635-42a2-fb75-0f17c7051a05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start Training ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c38d2646367b4796b05c2c52dc77c0b1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1684 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a9f589b4529b4d8995db859b4855c55a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1684 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dfecbd78c8f048219d9f92aa8ee6a435",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1684 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 | Step 200 | loss = 0.163, acc = 0.915\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "af0ad5976e2c4f3799b5ad3524664a02",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3524 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation | Epoch 3 | acc = 0.841\n",
            "Save best model!\n",
            "Epoch 3 | Step 400 | loss = 0.168, acc = 0.916\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e1b25207c0dc424e8478d5f619d53e86",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3524 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation | Epoch 3 | acc = 0.832\n",
            "Epoch 3 | Step 600 | loss = 0.165, acc = 0.909\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "725caa955ec84cca859f79b1e7e63a90",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3524 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation | Epoch 3 | acc = 0.836\n",
            "Epoch 3 | Step 800 | loss = 0.180, acc = 0.906\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "46a36bcf98a7499ea3ed3a924910fe24",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3524 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation | Epoch 3 | acc = 0.839\n",
            "Epoch 3 | Step 1000 | loss = 0.165, acc = 0.905\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "831495f1322d4524b184d0d703555c19",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3524 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation | Epoch 3 | acc = 0.838\n",
            "Epoch 3 | Step 1200 | loss = 0.170, acc = 0.910\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8f5134e811c44aad92115c9aaab85c54",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3524 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation | Epoch 3 | acc = 0.833\n",
            "Epoch 3 | Step 1400 | loss = 0.164, acc = 0.917\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a35861fae94a473fa6d7f241923509ee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3524 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation | Epoch 3 | acc = 0.833\n",
            "Epoch 3 | Step 1600 | loss = 0.163, acc = 0.917\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "01ba8ee4dc5a4414a2df7309ff1e392c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3524 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation | Epoch 3 | acc = 0.831\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cf8870ffda6b4c99804a6ba0f9396c05",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1684 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 | Step 200 | loss = 0.102, acc = 0.941\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e02721e2f2d540bf9c5b360bea581abb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3524 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation | Epoch 4 | acc = 0.841\n",
            "Save best model!\n",
            "Epoch 4 | Step 400 | loss = 0.097, acc = 0.949\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff39147453e94a99b2064253136684a8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3524 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation | Epoch 4 | acc = 0.836\n",
            "Epoch 4 | Step 600 | loss = 0.108, acc = 0.938\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "151630a75627436996b1170021520c0c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3524 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation | Epoch 4 | acc = 0.832\n",
            "Epoch 4 | Step 800 | loss = 0.106, acc = 0.943\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "434fa03d400a40b9821df860a95beb6b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3524 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation | Epoch 4 | acc = 0.831\n",
            "Epoch 4 | Step 1000 | loss = 0.092, acc = 0.950\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "10b931de306a4658832eea33ac5dccb1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3524 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation | Epoch 4 | acc = 0.832\n",
            "Epoch 4 | Step 1200 | loss = 0.100, acc = 0.949\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ad41c7a3616f4e13be7e68a8790a78c8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3524 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation | Epoch 4 | acc = 0.834\n",
            "Epoch 4 | Step 1400 | loss = 0.111, acc = 0.939\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4c5165fa951d49ee84a7f66f37dad8e4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3524 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation | Epoch 4 | acc = 0.829\n",
            "Epoch 4 | Step 1600 | loss = 0.087, acc = 0.949\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "910c2e963d374cc3b75821f8d59303bb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3524 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation | Epoch 4 | acc = 0.831\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "29f578febb134990a7f5c95b2e7d63b9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1684 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 | Step 200 | loss = 0.067, acc = 0.958\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "339d3868784d434ea362ed13b4d5a9bd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3524 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation | Epoch 5 | acc = 0.839\n",
            "Epoch 5 | Step 400 | loss = 0.068, acc = 0.964\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3a1e73ff554b4b66ae03da6535520698",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3524 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation | Epoch 5 | acc = 0.831\n",
            "Epoch 5 | Step 600 | loss = 0.075, acc = 0.960\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3dbc9f0d21cf406b9a81b21884bcd4ba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3524 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation | Epoch 5 | acc = 0.836\n",
            "Epoch 5 | Step 800 | loss = 0.070, acc = 0.966\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "daab6338c1f54688bf4a56b26a3545d5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3524 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation | Epoch 5 | acc = 0.831\n",
            "Epoch 5 | Step 1000 | loss = 0.070, acc = 0.960\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "26ed81f0a25242dfb0a893573fdf7493",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3524 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation | Epoch 5 | acc = 0.834\n",
            "Epoch 5 | Step 1200 | loss = 0.071, acc = 0.967\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b23cd16f3b8543739c118b4565c56023",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3524 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation | Epoch 5 | acc = 0.835\n",
            "Epoch 5 | Step 1400 | loss = 0.056, acc = 0.972\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "707c177dd6b747218738d12d73271b60",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3524 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation | Epoch 5 | acc = 0.836\n",
            "Epoch 5 | Step 1600 | loss = 0.062, acc = 0.967\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8d7fe83bf3a144bb9769e00e8e67319c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3524 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation | Epoch 5 | acc = 0.835\n"
          ]
        }
      ],
      "source": [
        "num_epoch = 5\n",
        "validation = True\n",
        "logging_step = 200\n",
        "learning_rate = 1e-5\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "warm_up_ratio = 0.1\n",
        "total_step = 1684*num_epoch\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = warm_up_ratio * total_step, num_training_steps = total_step)\n",
        "best_acc = 0\n",
        "\n",
        "if fp16_training:\n",
        "    model, optimizer, train_loader = accelerator.prepare(model, optimizer, train_loader) \n",
        "\n",
        "model.train()\n",
        "\n",
        "print(\"Start Training ...\")\n",
        "\n",
        "for epoch in range(num_epoch):\n",
        "    step = 1\n",
        "    train_loss = train_acc = 0\n",
        "    \n",
        "    for data in tqdm(train_loader):\t\n",
        "        # Load all data into GPU\n",
        "        data = [i.to(device) for i in data]\n",
        "        \n",
        "        # Model inputs: input_ids, token_type_ids, attention_mask, start_positions, end_positions (Note: only \"input_ids\" is mandatory)\n",
        "        # Model outputs: start_logits, end_logits, loss (return when start_positions/end_positions are provided)  \n",
        "        output = model(input_ids=data[0], token_type_ids=data[1], attention_mask=data[2], start_positions=data[3], end_positions=data[4])\n",
        "\n",
        "        # Choose the most probable start position / end position\n",
        "        start_index = torch.argmax(output.start_logits, dim=1)\n",
        "        end_index = torch.argmax(output.end_logits, dim=1)\n",
        "        \n",
        "        # Prediction is correct only if both start_index and end_index are correct\n",
        "        train_acc += ((start_index == data[3]) & (end_index == data[4])).float().mean()\n",
        "        train_loss += output.loss\n",
        "        \n",
        "        if fp16_training:\n",
        "            accelerator.backward(output.loss)\n",
        "        else:\n",
        "            output.loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        step += 1\n",
        "\n",
        "        ##### TODO: Apply linear learning rate decay #####\n",
        "        #print(optimizer.param_groups[0][\"lr\"])\n",
        "        scheduler.step()\n",
        "        \"\"\"\n",
        "        for i in range(total_step):\n",
        "          optimizer.param_groups[0]][\"lr\"] -= learning_rate/total_step\n",
        "        print(optimizer.param_groups[0]][\"lr\"])\n",
        "        \"\"\"\n",
        "        \n",
        "        # Print training loss and accuracy over past logging step\n",
        "        if epoch>1 and step % logging_step == 0:\n",
        "            print(f\"Epoch {epoch + 1} | Step {step} | loss = {train_loss.item() / logging_step:.3f}, acc = {train_acc / logging_step:.3f}\")\n",
        "            train_loss = train_acc = 0\n",
        "\n",
        "            if validation:\n",
        "                #print(\"Evaluating Dev Set ...\")\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    dev_acc = 0\n",
        "                    for i, data in enumerate(tqdm(dev_loader)):\n",
        "                        output = model(input_ids=data[0].squeeze(dim=0).to(device), token_type_ids=data[1].squeeze(dim=0).to(device),\n",
        "                                attention_mask=data[2].squeeze(dim=0).to(device))\n",
        "                        # prediction is correct only if answer text exactly matches\n",
        "                        dev_acc += evaluate(data, output) == dev_questions[i][\"answer_text\"]\n",
        "                    print(f\"Validation | Epoch {epoch + 1} | acc = {dev_acc / len(dev_loader):.3f}\")\n",
        "                    if best_acc < dev_acc / len(dev_loader):\n",
        "                            best_acc = dev_acc / len(dev_loader)\n",
        "                            print(f\"Save best model!\")\n",
        "                            model_save_dir = \"saved_model\" \n",
        "                            model.save_pretrained(model_save_dir)\n",
        "                model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "690a2fb2",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-20T14:41:24.974787Z",
          "iopub.status.busy": "2022-10-20T14:41:24.973910Z",
          "iopub.status.idle": "2022-10-20T14:53:03.919398Z",
          "shell.execute_reply": "2022-10-20T14:53:03.917986Z"
        },
        "papermill": {
          "duration": 698.960468,
          "end_time": "2022-10-20T14:53:03.921435",
          "exception": false,
          "start_time": "2022-10-20T14:41:24.960967",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "b10f02464610470188b2f5384007bd87"
          ]
        },
        "id": "690a2fb2",
        "outputId": "9f28bb01-51be-4561-c265-accda0b8b55f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating Test Set ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b10f02464610470188b2f5384007bd87",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3493 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed! Result is in submission.csv\n"
          ]
        }
      ],
      "source": [
        "print(\"Evaluating Test Set ...\")\n",
        "\n",
        "result = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for data in tqdm(test_loader):\n",
        "        output = model(input_ids=data[0].squeeze(dim=0).to(device), token_type_ids=data[1].squeeze(dim=0).to(device),\n",
        "                       attention_mask=data[2].squeeze(dim=0).to(device))\n",
        "        result.append(evaluate(data, output))\n",
        "\n",
        "result_file = \"submission.csv\"\n",
        "with open(result_file, 'w') as f:\n",
        "    f.write(\"ID,Answer\\n\")\n",
        "    for i, test_question in enumerate(test_questions):\n",
        "        # Replace commas in answers with empty strings (since csv is separated by comma)\n",
        "        # Answers in kaggle are processed in the same way\n",
        "        f.write(f\"{test_question['id']},{result[i].replace(',','')}\\n\")\n",
        "\n",
        "print(f\"Completed! Result is in {result_file}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 26707.953553,
      "end_time": "2022-10-20T14:53:07.741345",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2022-10-20T07:27:59.787792",
      "version": "2.3.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}