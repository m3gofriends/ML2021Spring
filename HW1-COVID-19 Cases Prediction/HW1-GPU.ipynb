{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "697ddb60",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-09-25T04:18:46.820111Z",
     "iopub.status.busy": "2022-09-25T04:18:46.818713Z",
     "iopub.status.idle": "2022-09-25T04:18:49.059198Z",
     "shell.execute_reply": "2022-09-25T04:18:49.057824Z"
    },
    "papermill": {
     "duration": 2.251293,
     "end_time": "2022-09-25T04:18:49.062531",
     "exception": false,
     "start_time": "2022-09-25T04:18:46.811238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# For data preprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import math\n",
    "\n",
    "# For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "myseed = 15 # set a random seed\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(myseed)\n",
    "torch.manual_seed(myseed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(myseed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11b6ff6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-25T04:18:49.073887Z",
     "iopub.status.busy": "2022-09-25T04:18:49.073069Z",
     "iopub.status.idle": "2022-09-25T04:18:49.240464Z",
     "shell.execute_reply": "2022-09-25T04:18:49.239028Z"
    },
    "papermill": {
     "duration": 0.176653,
     "end_time": "2022-09-25T04:18:49.244155",
     "exception": false,
     "start_time": "2022-09-25T04:18:49.067502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>AL</th>\n",
       "      <th>AK</th>\n",
       "      <th>AZ</th>\n",
       "      <th>AR</th>\n",
       "      <th>CA</th>\n",
       "      <th>CO</th>\n",
       "      <th>CT</th>\n",
       "      <th>FL</th>\n",
       "      <th>GA</th>\n",
       "      <th>...</th>\n",
       "      <th>restaurant.2</th>\n",
       "      <th>spent_time.2</th>\n",
       "      <th>large_event.2</th>\n",
       "      <th>public_transit.2</th>\n",
       "      <th>anxious.2</th>\n",
       "      <th>depressed.2</th>\n",
       "      <th>felt_isolated.2</th>\n",
       "      <th>worried_become_ill.2</th>\n",
       "      <th>worried_finances.2</th>\n",
       "      <th>tested_positive.2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>23.812411</td>\n",
       "      <td>43.430423</td>\n",
       "      <td>16.151527</td>\n",
       "      <td>1.602635</td>\n",
       "      <td>15.409449</td>\n",
       "      <td>12.088688</td>\n",
       "      <td>16.702086</td>\n",
       "      <td>53.991549</td>\n",
       "      <td>43.604229</td>\n",
       "      <td>20.704935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>23.682974</td>\n",
       "      <td>43.196313</td>\n",
       "      <td>16.123386</td>\n",
       "      <td>1.641863</td>\n",
       "      <td>15.230063</td>\n",
       "      <td>11.809047</td>\n",
       "      <td>16.506973</td>\n",
       "      <td>54.185521</td>\n",
       "      <td>42.665766</td>\n",
       "      <td>21.292911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>23.593983</td>\n",
       "      <td>43.362200</td>\n",
       "      <td>16.159971</td>\n",
       "      <td>1.677523</td>\n",
       "      <td>15.717207</td>\n",
       "      <td>12.355918</td>\n",
       "      <td>16.273294</td>\n",
       "      <td>53.637069</td>\n",
       "      <td>42.972417</td>\n",
       "      <td>21.166656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>22.576992</td>\n",
       "      <td>42.954574</td>\n",
       "      <td>15.544373</td>\n",
       "      <td>1.578030</td>\n",
       "      <td>15.295650</td>\n",
       "      <td>12.218123</td>\n",
       "      <td>16.045504</td>\n",
       "      <td>52.446223</td>\n",
       "      <td>42.907472</td>\n",
       "      <td>19.896607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>22.091433</td>\n",
       "      <td>43.290957</td>\n",
       "      <td>15.214655</td>\n",
       "      <td>1.641667</td>\n",
       "      <td>14.778802</td>\n",
       "      <td>12.417256</td>\n",
       "      <td>16.134238</td>\n",
       "      <td>52.560315</td>\n",
       "      <td>43.321985</td>\n",
       "      <td>20.178428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   AL   AK   AZ   AR   CA   CO   CT   FL   GA  ...  restaurant.2  \\\n",
       "0   0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...     23.812411   \n",
       "1   1  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...     23.682974   \n",
       "2   2  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...     23.593983   \n",
       "3   3  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...     22.576992   \n",
       "4   4  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...     22.091433   \n",
       "\n",
       "   spent_time.2  large_event.2  public_transit.2  anxious.2  depressed.2  \\\n",
       "0     43.430423      16.151527          1.602635  15.409449    12.088688   \n",
       "1     43.196313      16.123386          1.641863  15.230063    11.809047   \n",
       "2     43.362200      16.159971          1.677523  15.717207    12.355918   \n",
       "3     42.954574      15.544373          1.578030  15.295650    12.218123   \n",
       "4     43.290957      15.214655          1.641667  14.778802    12.417256   \n",
       "\n",
       "   felt_isolated.2  worried_become_ill.2  worried_finances.2  \\\n",
       "0        16.702086             53.991549           43.604229   \n",
       "1        16.506973             54.185521           42.665766   \n",
       "2        16.273294             53.637069           42.972417   \n",
       "3        16.045504             52.446223           42.907472   \n",
       "4        16.134238             52.560315           43.321985   \n",
       "\n",
       "   tested_positive.2  \n",
       "0          20.704935  \n",
       "1          21.292911  \n",
       "2          21.166656  \n",
       "3          19.896607  \n",
       "4          20.178428  \n",
       "\n",
       "[5 rows x 95 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#import numpy as np\n",
    "\n",
    "df = pd.read_csv('/kaggle/input/ml2021spring-hw1/covid.train.csv')\n",
    "test_df = pd.read_csv('/kaggle/input/ml2021spring-hw1/covid.test.csv')\n",
    "combine_df = pd.concat([df, test_df])\n",
    "\n",
    "x = df[df.columns[1:94]]\n",
    "y = df[df.columns[94]]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cec2efdb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-25T04:18:49.255267Z",
     "iopub.status.busy": "2022-09-25T04:18:49.254846Z",
     "iopub.status.idle": "2022-09-25T04:18:50.402850Z",
     "shell.execute_reply": "2022-09-25T04:18:50.401400Z"
    },
    "papermill": {
     "duration": 1.157794,
     "end_time": "2022-09-25T04:18:50.407007",
     "exception": false,
     "start_time": "2022-09-25T04:18:49.249213",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WI</th>\n",
       "      <th>cli</th>\n",
       "      <th>ili</th>\n",
       "      <th>hh_cmnty_cli</th>\n",
       "      <th>worried_finances</th>\n",
       "      <th>tested_positive</th>\n",
       "      <th>cli.1</th>\n",
       "      <th>ili.1</th>\n",
       "      <th>hh_cmnty_cli.1</th>\n",
       "      <th>worried_finances.1</th>\n",
       "      <th>tested_positive.1</th>\n",
       "      <th>cli.2</th>\n",
       "      <th>ili.2</th>\n",
       "      <th>hh_cmnty_cli.2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.814610</td>\n",
       "      <td>0.771356</td>\n",
       "      <td>25.648907</td>\n",
       "      <td>43.279629</td>\n",
       "      <td>19.586492</td>\n",
       "      <td>0.838995</td>\n",
       "      <td>0.807767</td>\n",
       "      <td>25.679101</td>\n",
       "      <td>43.622728</td>\n",
       "      <td>20.151838</td>\n",
       "      <td>0.897802</td>\n",
       "      <td>0.887893</td>\n",
       "      <td>26.060544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.838995</td>\n",
       "      <td>0.807767</td>\n",
       "      <td>25.679101</td>\n",
       "      <td>43.622728</td>\n",
       "      <td>20.151838</td>\n",
       "      <td>0.897802</td>\n",
       "      <td>0.887893</td>\n",
       "      <td>26.060544</td>\n",
       "      <td>43.604229</td>\n",
       "      <td>20.704935</td>\n",
       "      <td>0.972842</td>\n",
       "      <td>0.965496</td>\n",
       "      <td>25.754087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.897802</td>\n",
       "      <td>0.887893</td>\n",
       "      <td>26.060544</td>\n",
       "      <td>43.604229</td>\n",
       "      <td>20.704935</td>\n",
       "      <td>0.972842</td>\n",
       "      <td>0.965496</td>\n",
       "      <td>25.754087</td>\n",
       "      <td>42.665766</td>\n",
       "      <td>21.292911</td>\n",
       "      <td>0.955306</td>\n",
       "      <td>0.963079</td>\n",
       "      <td>25.947015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.972842</td>\n",
       "      <td>0.965496</td>\n",
       "      <td>25.754087</td>\n",
       "      <td>42.665766</td>\n",
       "      <td>21.292911</td>\n",
       "      <td>0.955306</td>\n",
       "      <td>0.963079</td>\n",
       "      <td>25.947015</td>\n",
       "      <td>42.972417</td>\n",
       "      <td>21.166656</td>\n",
       "      <td>0.947513</td>\n",
       "      <td>0.968764</td>\n",
       "      <td>26.350501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.955306</td>\n",
       "      <td>0.963079</td>\n",
       "      <td>25.947015</td>\n",
       "      <td>42.972417</td>\n",
       "      <td>21.166656</td>\n",
       "      <td>0.947513</td>\n",
       "      <td>0.968764</td>\n",
       "      <td>26.350501</td>\n",
       "      <td>42.907472</td>\n",
       "      <td>19.896607</td>\n",
       "      <td>0.883833</td>\n",
       "      <td>0.893020</td>\n",
       "      <td>26.480624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    WI       cli       ili  hh_cmnty_cli  worried_finances  tested_positive  \\\n",
       "0  0.0  0.814610  0.771356     25.648907         43.279629        19.586492   \n",
       "1  0.0  0.838995  0.807767     25.679101         43.622728        20.151838   \n",
       "2  0.0  0.897802  0.887893     26.060544         43.604229        20.704935   \n",
       "3  0.0  0.972842  0.965496     25.754087         42.665766        21.292911   \n",
       "4  0.0  0.955306  0.963079     25.947015         42.972417        21.166656   \n",
       "\n",
       "      cli.1     ili.1  hh_cmnty_cli.1  worried_finances.1  tested_positive.1  \\\n",
       "0  0.838995  0.807767       25.679101           43.622728          20.151838   \n",
       "1  0.897802  0.887893       26.060544           43.604229          20.704935   \n",
       "2  0.972842  0.965496       25.754087           42.665766          21.292911   \n",
       "3  0.955306  0.963079       25.947015           42.972417          21.166656   \n",
       "4  0.947513  0.968764       26.350501           42.907472          19.896607   \n",
       "\n",
       "      cli.2     ili.2  hh_cmnty_cli.2  \n",
       "0  0.897802  0.887893       26.060544  \n",
       "1  0.972842  0.965496       25.754087  \n",
       "2  0.955306  0.963079       25.947015  \n",
       "3  0.947513  0.968764       26.350501  \n",
       "4  0.883833  0.893020       26.480624  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "bestfeatures = SelectKBest(score_func=f_regression, k=14)\n",
    "fit = bestfeatures.fit(x,y)\n",
    "cols = bestfeatures.get_support(indices=True)\n",
    "\n",
    "df = df.iloc[:,cols]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9edd963",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-25T04:18:50.442091Z",
     "iopub.status.busy": "2022-09-25T04:18:50.441333Z",
     "iopub.status.idle": "2022-09-25T04:18:50.463242Z",
     "shell.execute_reply": "2022-09-25T04:18:50.461851Z"
    },
    "papermill": {
     "duration": 0.04713,
     "end_time": "2022-09-25T04:18:50.470090",
     "exception": false,
     "start_time": "2022-09-25T04:18:50.422960",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Specs          Score\n",
      "75   tested_positive.1  148069.658278\n",
      "57     tested_positive   69603.872591\n",
      "42        hh_cmnty_cli    9235.492094\n",
      "60      hh_cmnty_cli.1    9209.019558\n",
      "78      hh_cmnty_cli.2    9097.375172\n",
      "43      nohh_cmnty_cli    8395.421300\n",
      "61    nohh_cmnty_cli.1    8343.255927\n",
      "79    nohh_cmnty_cli.2    8208.176435\n",
      "40                 cli    6388.906849\n",
      "58               cli.1    6374.548000\n",
      "76               cli.2    6250.008702\n",
      "41                 ili    5998.922880\n",
      "59               ili.1    5937.588576\n",
      "77               ili.2    5796.947672\n",
      "92  worried_finances.2     833.613191\n"
     ]
    }
   ],
   "source": [
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(x.columns)\n",
    "\n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Specs','Score'] \n",
    "\n",
    "print(featureScores.nlargest(15,'Score')) # Print 15 best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "793aecec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-25T04:18:50.488381Z",
     "iopub.status.busy": "2022-09-25T04:18:50.488056Z",
     "iopub.status.idle": "2022-09-25T04:18:54.307059Z",
     "shell.execute_reply": "2022-09-25T04:18:54.305784Z"
    },
    "papermill": {
     "duration": 3.828516,
     "end_time": "2022-09-25T04:18:54.310255",
     "exception": false,
     "start_time": "2022-09-25T04:18:50.481739",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_std={}\n",
    "\n",
    "# Standardization Feature\n",
    "for col in df.columns:\n",
    "  mean_std[col] = (combine_df[col].mean(), combine_df[col].std())\n",
    "  df[col] = (df[col] - mean_std[col][0]) / mean_std[col][1]\n",
    "\n",
    "# Pandas to Numpy\n",
    "x = df[df.columns].values\n",
    "y = y.values\n",
    "\n",
    "# Numpy to Tensor\n",
    "x = torch.Tensor(x).cuda()\n",
    "y = torch.Tensor(y).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91bec2b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-25T04:18:54.321920Z",
     "iopub.status.busy": "2022-09-25T04:18:54.321569Z",
     "iopub.status.idle": "2022-09-25T04:18:54.345904Z",
     "shell.execute_reply": "2022-09-25T04:18:54.344103Z"
    },
    "papermill": {
     "duration": 0.033553,
     "end_time": "2022-09-25T04:18:54.349186",
     "exception": false,
     "start_time": "2022-09-25T04:18:54.315633",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WI</th>\n",
       "      <th>cli</th>\n",
       "      <th>ili</th>\n",
       "      <th>hh_cmnty_cli</th>\n",
       "      <th>worried_finances</th>\n",
       "      <th>tested_positive</th>\n",
       "      <th>cli.1</th>\n",
       "      <th>ili.1</th>\n",
       "      <th>hh_cmnty_cli.1</th>\n",
       "      <th>worried_finances.1</th>\n",
       "      <th>tested_positive.1</th>\n",
       "      <th>cli.2</th>\n",
       "      <th>ili.2</th>\n",
       "      <th>hh_cmnty_cli.2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.160266</td>\n",
       "      <td>-0.411747</td>\n",
       "      <td>-0.566136</td>\n",
       "      <td>-0.401512</td>\n",
       "      <td>-0.236095</td>\n",
       "      <td>0.438190</td>\n",
       "      <td>-0.361583</td>\n",
       "      <td>-0.487464</td>\n",
       "      <td>-0.407982</td>\n",
       "      <td>-0.173065</td>\n",
       "      <td>0.505859</td>\n",
       "      <td>-0.229376</td>\n",
       "      <td>-0.305425</td>\n",
       "      <td>-0.375758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.160266</td>\n",
       "      <td>-0.353448</td>\n",
       "      <td>-0.479792</td>\n",
       "      <td>-0.398237</td>\n",
       "      <td>-0.168869</td>\n",
       "      <td>0.511783</td>\n",
       "      <td>-0.221076</td>\n",
       "      <td>-0.297780</td>\n",
       "      <td>-0.366502</td>\n",
       "      <td>-0.176694</td>\n",
       "      <td>0.577989</td>\n",
       "      <td>-0.050064</td>\n",
       "      <td>-0.121692</td>\n",
       "      <td>-0.409173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.160266</td>\n",
       "      <td>-0.212855</td>\n",
       "      <td>-0.289779</td>\n",
       "      <td>-0.356871</td>\n",
       "      <td>-0.172493</td>\n",
       "      <td>0.583781</td>\n",
       "      <td>-0.041779</td>\n",
       "      <td>-0.114071</td>\n",
       "      <td>-0.399828</td>\n",
       "      <td>-0.360786</td>\n",
       "      <td>0.654668</td>\n",
       "      <td>-0.091968</td>\n",
       "      <td>-0.127415</td>\n",
       "      <td>-0.388136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.160266</td>\n",
       "      <td>-0.033449</td>\n",
       "      <td>-0.105750</td>\n",
       "      <td>-0.390105</td>\n",
       "      <td>-0.356376</td>\n",
       "      <td>0.660320</td>\n",
       "      <td>-0.083680</td>\n",
       "      <td>-0.119793</td>\n",
       "      <td>-0.378848</td>\n",
       "      <td>-0.300632</td>\n",
       "      <td>0.638203</td>\n",
       "      <td>-0.110588</td>\n",
       "      <td>-0.113955</td>\n",
       "      <td>-0.344142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.160266</td>\n",
       "      <td>-0.075375</td>\n",
       "      <td>-0.111482</td>\n",
       "      <td>-0.369183</td>\n",
       "      <td>-0.296291</td>\n",
       "      <td>0.643885</td>\n",
       "      <td>-0.102298</td>\n",
       "      <td>-0.106335</td>\n",
       "      <td>-0.334971</td>\n",
       "      <td>-0.313372</td>\n",
       "      <td>0.472574</td>\n",
       "      <td>-0.262754</td>\n",
       "      <td>-0.293286</td>\n",
       "      <td>-0.329954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         WI       cli       ili  hh_cmnty_cli  worried_finances  \\\n",
       "0 -0.160266 -0.411747 -0.566136     -0.401512         -0.236095   \n",
       "1 -0.160266 -0.353448 -0.479792     -0.398237         -0.168869   \n",
       "2 -0.160266 -0.212855 -0.289779     -0.356871         -0.172493   \n",
       "3 -0.160266 -0.033449 -0.105750     -0.390105         -0.356376   \n",
       "4 -0.160266 -0.075375 -0.111482     -0.369183         -0.296291   \n",
       "\n",
       "   tested_positive     cli.1     ili.1  hh_cmnty_cli.1  worried_finances.1  \\\n",
       "0         0.438190 -0.361583 -0.487464       -0.407982           -0.173065   \n",
       "1         0.511783 -0.221076 -0.297780       -0.366502           -0.176694   \n",
       "2         0.583781 -0.041779 -0.114071       -0.399828           -0.360786   \n",
       "3         0.660320 -0.083680 -0.119793       -0.378848           -0.300632   \n",
       "4         0.643885 -0.102298 -0.106335       -0.334971           -0.313372   \n",
       "\n",
       "   tested_positive.1     cli.2     ili.2  hh_cmnty_cli.2  \n",
       "0           0.505859 -0.229376 -0.305425       -0.375758  \n",
       "1           0.577989 -0.050064 -0.121692       -0.409173  \n",
       "2           0.654668 -0.091968 -0.127415       -0.388136  \n",
       "3           0.638203 -0.110588 -0.113955       -0.344142  \n",
       "4           0.472574 -0.262754 -0.293286       -0.329954  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50bc32f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-25T04:18:54.361981Z",
     "iopub.status.busy": "2022-09-25T04:18:54.360950Z",
     "iopub.status.idle": "2022-09-25T04:18:54.368896Z",
     "shell.execute_reply": "2022-09-25T04:18:54.367600Z"
    },
    "papermill": {
     "duration": 0.016664,
     "end_time": "2022-09-25T04:18:54.371389",
     "exception": false,
     "start_time": "2022-09-25T04:18:54.354725",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(14, 32)\n",
    "        #self.bn1 = nn.BatchNorm1d(32) # 使用BatchNorm1d效果不好\n",
    "        #self.dropout1 = nn.Dropout(0.05) # 使用Dropout效果不好\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        #x = self.bn1(x)\n",
    "        #x = self.dropout1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c51d8334",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-25T04:18:54.383907Z",
     "iopub.status.busy": "2022-09-25T04:18:54.382696Z",
     "iopub.status.idle": "2022-09-25T04:18:54.394897Z",
     "shell.execute_reply": "2022-09-25T04:18:54.393611Z"
    },
    "papermill": {
     "duration": 0.021178,
     "end_time": "2022-09-25T04:18:54.397820",
     "exception": false,
     "start_time": "2022-09-25T04:18:54.376642",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "network = Net()\n",
    "network.cuda()\n",
    "optimizer = torch.optim.RAdam(network.parameters(), lr=7.5e-5, weight_decay=7e-3) # 使用RAdam效果比Adam更好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "462976a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-25T04:18:54.410207Z",
     "iopub.status.busy": "2022-09-25T04:18:54.409235Z",
     "iopub.status.idle": "2022-09-25T04:18:54.415834Z",
     "shell.execute_reply": "2022-09-25T04:18:54.414486Z"
    },
    "papermill": {
     "duration": 0.015864,
     "end_time": "2022-09-25T04:18:54.418878",
     "exception": false,
     "start_time": "2022-09-25T04:18:54.403014",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 10000\n",
    "batch_size = 128\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# Early Stopping parameters\n",
    "ES_patience = 500\n",
    "ES_counter = 0 \n",
    "best_epoch = 0\n",
    "best_loss = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76707331",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-25T04:18:54.430917Z",
     "iopub.status.busy": "2022-09-25T04:18:54.429851Z",
     "iopub.status.idle": "2022-09-25T04:18:54.437734Z",
     "shell.execute_reply": "2022-09-25T04:18:54.436545Z"
    },
    "papermill": {
     "duration": 0.0164,
     "end_time": "2022-09-25T04:18:54.440186",
     "exception": false,
     "start_time": "2022-09-25T04:18:54.423786",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "  network.train()\n",
    "  for i in range(len(x_train)//batch_size):\n",
    "    optimizer.zero_grad()\n",
    "    pred = network(x_train[batch_size*i:batch_size*(i+1)])\n",
    "    loss = F.mse_loss(pred.view(-1), x_test[batch_size*i:batch_size*(i+1)])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  train_losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a6c9d5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-25T04:18:54.452245Z",
     "iopub.status.busy": "2022-09-25T04:18:54.451915Z",
     "iopub.status.idle": "2022-09-25T04:18:54.459801Z",
     "shell.execute_reply": "2022-09-25T04:18:54.458309Z"
    },
    "papermill": {
     "duration": 0.016956,
     "end_time": "2022-09-25T04:18:54.462228",
     "exception": false,
     "start_time": "2022-09-25T04:18:54.445272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "  global best_epoch, best_loss, ES_counter\n",
    "\n",
    "  network.eval()\n",
    "  with torch.no_grad():\n",
    "    pred = network(y_train)\n",
    "    loss = F.mse_loss(pred.view(-1), y_test)\n",
    "    \n",
    "  # Early Stopping\n",
    "  if best_loss > loss:\n",
    "    ES_counter, best_epoch, best_loss = 0, epoch, loss\n",
    "    # Save best model\n",
    "    torch.save(network.state_dict(), '/kaggle/model.pth')\n",
    "    print('Saving model (epoch = {:4d}, MSE loss = {:.4f})'.format(epoch, loss))\n",
    "  else:\n",
    "    ES_counter += 1\n",
    "    if ES_counter == ES_patience:\n",
    "      print('---Early Stopping--- (Best epoch = {:4d}, Best MSE loss = {:.4f})'.format(best_epoch, best_loss))\n",
    "  # Early Stopping\n",
    "\n",
    "  test_losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87467b15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-25T04:18:54.474463Z",
     "iopub.status.busy": "2022-09-25T04:18:54.474137Z",
     "iopub.status.idle": "2022-09-25T04:23:37.529931Z",
     "shell.execute_reply": "2022-09-25T04:23:37.528552Z"
    },
    "papermill": {
     "duration": 283.065629,
     "end_time": "2022-09-25T04:23:37.533049",
     "exception": false,
     "start_time": "2022-09-25T04:18:54.467420",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model (epoch =    1, MSE loss = 317.7927)\n",
      "Saving model (epoch =    6, MSE loss = 315.9405)\n",
      "Saving model (epoch =   16, MSE loss = 313.8748)\n",
      "Saving model (epoch =   20, MSE loss = 313.6397)\n",
      "Saving model (epoch =   26, MSE loss = 310.8149)\n",
      "Saving model (epoch =   30, MSE loss = 310.4000)\n",
      "Saving model (epoch =   36, MSE loss = 306.8941)\n",
      "Saving model (epoch =   40, MSE loss = 306.3477)\n",
      "Saving model (epoch =   41, MSE loss = 306.1722)\n",
      "Saving model (epoch =   46, MSE loss = 302.0892)\n",
      "Saving model (epoch =   50, MSE loss = 301.4042)\n",
      "Saving model (epoch =   51, MSE loss = 300.9808)\n",
      "Saving model (epoch =   56, MSE loss = 296.2608)\n",
      "Saving model (epoch =   60, MSE loss = 295.4333)\n",
      "Saving model (epoch =   61, MSE loss = 294.7320)\n",
      "Saving model (epoch =   66, MSE loss = 289.3549)\n",
      "Saving model (epoch =   70, MSE loss = 288.4584)\n",
      "Saving model (epoch =   71, MSE loss = 287.4515)\n",
      "Saving model (epoch =   74, MSE loss = 287.1920)\n",
      "Saving model (epoch =   76, MSE loss = 281.4252)\n",
      "Saving model (epoch =   80, MSE loss = 280.5255)\n",
      "Saving model (epoch =   81, MSE loss = 279.1893)\n",
      "Saving model (epoch =   84, MSE loss = 278.1521)\n",
      "Saving model (epoch =   86, MSE loss = 272.4867)\n",
      "Saving model (epoch =   90, MSE loss = 271.6357)\n",
      "Saving model (epoch =   91, MSE loss = 269.9362)\n",
      "Saving model (epoch =   94, MSE loss = 268.0659)\n",
      "Saving model (epoch =   96, MSE loss = 262.5493)\n",
      "Saving model (epoch =  100, MSE loss = 261.8000)\n",
      "Saving model (epoch =  101, MSE loss = 259.7058)\n",
      "Saving model (epoch =  104, MSE loss = 256.9775)\n",
      "Saving model (epoch =  106, MSE loss = 251.6584)\n",
      "Saving model (epoch =  110, MSE loss = 251.0676)\n",
      "Saving model (epoch =  111, MSE loss = 248.5660)\n",
      "Saving model (epoch =  114, MSE loss = 244.9724)\n",
      "Saving model (epoch =  116, MSE loss = 239.9053)\n",
      "Saving model (epoch =  120, MSE loss = 239.5535)\n",
      "Saving model (epoch =  121, MSE loss = 236.6411)\n",
      "Saving model (epoch =  124, MSE loss = 232.2076)\n",
      "Saving model (epoch =  126, MSE loss = 227.4531)\n",
      "Saving model (epoch =  130, MSE loss = 227.4352)\n",
      "Saving model (epoch =  131, MSE loss = 224.1214)\n",
      "Saving model (epoch =  134, MSE loss = 218.8979)\n",
      "Saving model (epoch =  136, MSE loss = 214.5111)\n",
      "Saving model (epoch =  141, MSE loss = 211.2235)\n",
      "Saving model (epoch =  144, MSE loss = 205.2750)\n",
      "Saving model (epoch =  146, MSE loss = 201.3008)\n",
      "Saving model (epoch =  151, MSE loss = 198.1621)\n",
      "Saving model (epoch =  154, MSE loss = 191.5704)\n",
      "Saving model (epoch =  156, MSE loss = 188.0395)\n",
      "Saving model (epoch =  161, MSE loss = 185.1502)\n",
      "Saving model (epoch =  164, MSE loss = 178.0059)\n",
      "Saving model (epoch =  166, MSE loss = 174.9398)\n",
      "Saving model (epoch =  171, MSE loss = 172.3934)\n",
      "Saving model (epoch =  174, MSE loss = 164.7942)\n",
      "Saving model (epoch =  176, MSE loss = 162.2070)\n",
      "Saving model (epoch =  181, MSE loss = 160.0875)\n",
      "Saving model (epoch =  184, MSE loss = 152.1418)\n",
      "Saving model (epoch =  186, MSE loss = 150.0374)\n",
      "Saving model (epoch =  191, MSE loss = 148.4180)\n",
      "Saving model (epoch =  194, MSE loss = 140.2413)\n",
      "Saving model (epoch =  196, MSE loss = 138.6138)\n",
      "Saving model (epoch =  201, MSE loss = 137.5557)\n",
      "Saving model (epoch =  204, MSE loss = 129.2668)\n",
      "Saving model (epoch =  206, MSE loss = 128.0951)\n",
      "Saving model (epoch =  211, MSE loss = 127.6400)\n",
      "Saving model (epoch =  214, MSE loss = 119.3490)\n",
      "Saving model (epoch =  216, MSE loss = 118.6025)\n",
      "Saving model (epoch =  224, MSE loss = 110.5781)\n",
      "Saving model (epoch =  226, MSE loss = 110.2175)\n",
      "Saving model (epoch =  234, MSE loss = 102.9934)\n",
      "Saving model (epoch =  236, MSE loss = 102.9698)\n",
      "Saving model (epoch =  244, MSE loss = 96.5833)\n",
      "Saving model (epoch =  254, MSE loss = 91.2871)\n",
      "Saving model (epoch =  264, MSE loss = 87.0007)\n",
      "Saving model (epoch =  274, MSE loss = 83.5877)\n",
      "Saving model (epoch =  284, MSE loss = 80.8900)\n",
      "Saving model (epoch =  294, MSE loss = 78.7396)\n",
      "Saving model (epoch =  304, MSE loss = 76.9863)\n",
      "Saving model (epoch =  314, MSE loss = 75.4974)\n",
      "Saving model (epoch =  324, MSE loss = 74.1693)\n",
      "Saving model (epoch =  334, MSE loss = 72.9243)\n",
      "Saving model (epoch =  344, MSE loss = 71.7096)\n",
      "Saving model (epoch =  354, MSE loss = 70.4917)\n",
      "Saving model (epoch =  364, MSE loss = 69.2451)\n",
      "Saving model (epoch =  374, MSE loss = 67.9566)\n",
      "Saving model (epoch =  384, MSE loss = 66.6186)\n",
      "Saving model (epoch =  394, MSE loss = 65.2299)\n",
      "Saving model (epoch =  404, MSE loss = 63.7890)\n",
      "Saving model (epoch =  414, MSE loss = 62.2922)\n",
      "Saving model (epoch =  424, MSE loss = 60.7444)\n",
      "Saving model (epoch =  434, MSE loss = 59.1450)\n",
      "Saving model (epoch =  444, MSE loss = 57.4931)\n",
      "Saving model (epoch =  448, MSE loss = 57.4529)\n",
      "Saving model (epoch =  454, MSE loss = 55.7835)\n",
      "Saving model (epoch =  458, MSE loss = 55.6835)\n",
      "Saving model (epoch =  464, MSE loss = 54.0177)\n",
      "Saving model (epoch =  468, MSE loss = 53.8666)\n",
      "Saving model (epoch =  474, MSE loss = 52.2028)\n",
      "Saving model (epoch =  478, MSE loss = 52.0053)\n",
      "Saving model (epoch =  484, MSE loss = 50.3515)\n",
      "Saving model (epoch =  488, MSE loss = 50.1143)\n",
      "Saving model (epoch =  494, MSE loss = 48.4671)\n",
      "Saving model (epoch =  498, MSE loss = 48.1995)\n",
      "Saving model (epoch =  504, MSE loss = 46.5576)\n",
      "Saving model (epoch =  508, MSE loss = 46.2657)\n",
      "Saving model (epoch =  514, MSE loss = 44.6293)\n",
      "Saving model (epoch =  518, MSE loss = 44.3184)\n",
      "Saving model (epoch =  524, MSE loss = 42.6905)\n",
      "Saving model (epoch =  528, MSE loss = 42.3676)\n",
      "Saving model (epoch =  534, MSE loss = 40.7457)\n",
      "Saving model (epoch =  538, MSE loss = 40.4168)\n",
      "Saving model (epoch =  544, MSE loss = 38.7969)\n",
      "Saving model (epoch =  548, MSE loss = 38.4735)\n",
      "Saving model (epoch =  554, MSE loss = 36.8581)\n",
      "Saving model (epoch =  558, MSE loss = 36.5377)\n",
      "Saving model (epoch =  564, MSE loss = 34.9348)\n",
      "Saving model (epoch =  568, MSE loss = 34.6235)\n",
      "Saving model (epoch =  572, MSE loss = 34.5345)\n",
      "Saving model (epoch =  574, MSE loss = 33.0358)\n",
      "Saving model (epoch =  578, MSE loss = 32.7385)\n",
      "Saving model (epoch =  582, MSE loss = 32.5153)\n",
      "Saving model (epoch =  584, MSE loss = 31.1707)\n",
      "Saving model (epoch =  588, MSE loss = 30.8873)\n",
      "Saving model (epoch =  592, MSE loss = 30.5456)\n",
      "Saving model (epoch =  594, MSE loss = 29.3428)\n",
      "Saving model (epoch =  598, MSE loss = 29.0714)\n",
      "Saving model (epoch =  602, MSE loss = 28.6330)\n",
      "Saving model (epoch =  604, MSE loss = 27.5625)\n",
      "Saving model (epoch =  608, MSE loss = 27.3043)\n",
      "Saving model (epoch =  612, MSE loss = 26.7870)\n",
      "Saving model (epoch =  614, MSE loss = 25.8391)\n",
      "Saving model (epoch =  618, MSE loss = 25.5908)\n",
      "Saving model (epoch =  622, MSE loss = 25.0139)\n",
      "Saving model (epoch =  624, MSE loss = 24.1741)\n",
      "Saving model (epoch =  628, MSE loss = 23.9334)\n",
      "Saving model (epoch =  632, MSE loss = 23.3191)\n",
      "Saving model (epoch =  634, MSE loss = 22.5771)\n",
      "Saving model (epoch =  638, MSE loss = 22.3449)\n",
      "Saving model (epoch =  642, MSE loss = 21.7086)\n",
      "Saving model (epoch =  644, MSE loss = 21.0592)\n",
      "Saving model (epoch =  648, MSE loss = 20.8317)\n",
      "Saving model (epoch =  652, MSE loss = 20.1797)\n",
      "Saving model (epoch =  654, MSE loss = 19.6116)\n",
      "Saving model (epoch =  658, MSE loss = 19.3951)\n",
      "Saving model (epoch =  662, MSE loss = 18.7412)\n",
      "Saving model (epoch =  664, MSE loss = 18.2405)\n",
      "Saving model (epoch =  668, MSE loss = 18.0359)\n",
      "Saving model (epoch =  672, MSE loss = 17.3834)\n",
      "Saving model (epoch =  674, MSE loss = 16.9503)\n",
      "Saving model (epoch =  678, MSE loss = 16.7467)\n",
      "Saving model (epoch =  682, MSE loss = 16.1032)\n",
      "Saving model (epoch =  684, MSE loss = 15.7332)\n",
      "Saving model (epoch =  688, MSE loss = 15.5273)\n",
      "Saving model (epoch =  692, MSE loss = 14.9000)\n",
      "Saving model (epoch =  694, MSE loss = 14.5852)\n",
      "Saving model (epoch =  698, MSE loss = 14.3884)\n",
      "Saving model (epoch =  702, MSE loss = 13.7853)\n",
      "Saving model (epoch =  704, MSE loss = 13.5227)\n",
      "Saving model (epoch =  708, MSE loss = 13.3441)\n",
      "Saving model (epoch =  712, MSE loss = 12.7811)\n",
      "Saving model (epoch =  714, MSE loss = 12.5714)\n",
      "Saving model (epoch =  718, MSE loss = 12.4006)\n",
      "Saving model (epoch =  722, MSE loss = 11.8708)\n",
      "Saving model (epoch =  724, MSE loss = 11.6998)\n",
      "Saving model (epoch =  728, MSE loss = 11.5276)\n",
      "Saving model (epoch =  732, MSE loss = 11.0320)\n",
      "Saving model (epoch =  734, MSE loss = 10.8969)\n",
      "Saving model (epoch =  738, MSE loss = 10.7187)\n",
      "Saving model (epoch =  742, MSE loss = 10.2524)\n",
      "Saving model (epoch =  744, MSE loss = 10.1443)\n",
      "Saving model (epoch =  748, MSE loss = 9.9531)\n",
      "Saving model (epoch =  752, MSE loss = 9.5091)\n",
      "Saving model (epoch =  754, MSE loss = 9.4213)\n",
      "Saving model (epoch =  758, MSE loss = 9.2244)\n",
      "Saving model (epoch =  762, MSE loss = 8.8138)\n",
      "Saving model (epoch =  764, MSE loss = 8.7445)\n",
      "Saving model (epoch =  768, MSE loss = 8.5414)\n",
      "Saving model (epoch =  772, MSE loss = 8.1665)\n",
      "Saving model (epoch =  774, MSE loss = 8.1166)\n",
      "Saving model (epoch =  778, MSE loss = 7.9066)\n",
      "Saving model (epoch =  782, MSE loss = 7.5620)\n",
      "Saving model (epoch =  784, MSE loss = 7.5353)\n",
      "Saving model (epoch =  788, MSE loss = 7.3111)\n",
      "Saving model (epoch =  792, MSE loss = 6.9935)\n",
      "Saving model (epoch =  794, MSE loss = 6.9873)\n",
      "Saving model (epoch =  798, MSE loss = 6.7492)\n",
      "Saving model (epoch =  802, MSE loss = 6.4619)\n",
      "Saving model (epoch =  808, MSE loss = 6.2238)\n",
      "Saving model (epoch =  812, MSE loss = 5.9649)\n",
      "Saving model (epoch =  818, MSE loss = 5.7326)\n",
      "Saving model (epoch =  822, MSE loss = 5.4982)\n",
      "Saving model (epoch =  826, MSE loss = 5.4861)\n",
      "Saving model (epoch =  828, MSE loss = 5.2753)\n",
      "Saving model (epoch =  832, MSE loss = 5.0654)\n",
      "Saving model (epoch =  836, MSE loss = 5.0307)\n",
      "Saving model (epoch =  838, MSE loss = 4.8510)\n",
      "Saving model (epoch =  842, MSE loss = 4.6653)\n",
      "Saving model (epoch =  846, MSE loss = 4.6090)\n",
      "Saving model (epoch =  848, MSE loss = 4.4599)\n",
      "Saving model (epoch =  852, MSE loss = 4.2975)\n",
      "Saving model (epoch =  856, MSE loss = 4.2246)\n",
      "Saving model (epoch =  858, MSE loss = 4.1044)\n",
      "Saving model (epoch =  862, MSE loss = 3.9625)\n",
      "Saving model (epoch =  866, MSE loss = 3.8772)\n",
      "Saving model (epoch =  868, MSE loss = 3.7838)\n",
      "Saving model (epoch =  872, MSE loss = 3.6623)\n",
      "Saving model (epoch =  876, MSE loss = 3.5672)\n",
      "Saving model (epoch =  878, MSE loss = 3.4951)\n",
      "Saving model (epoch =  882, MSE loss = 3.3906)\n",
      "Saving model (epoch =  886, MSE loss = 3.2866)\n",
      "Saving model (epoch =  888, MSE loss = 3.2315)\n",
      "Saving model (epoch =  892, MSE loss = 3.1438)\n",
      "Saving model (epoch =  896, MSE loss = 3.0299)\n",
      "Saving model (epoch =  898, MSE loss = 2.9900)\n",
      "Saving model (epoch =  902, MSE loss = 2.9186)\n",
      "Saving model (epoch =  906, MSE loss = 2.7978)\n",
      "Saving model (epoch =  908, MSE loss = 2.7725)\n",
      "Saving model (epoch =  912, MSE loss = 2.7171)\n",
      "Saving model (epoch =  916, MSE loss = 2.5914)\n",
      "Saving model (epoch =  918, MSE loss = 2.5782)\n",
      "Saving model (epoch =  922, MSE loss = 2.5390)\n",
      "Saving model (epoch =  926, MSE loss = 2.4080)\n",
      "Saving model (epoch =  928, MSE loss = 2.4052)\n",
      "Saving model (epoch =  932, MSE loss = 2.3807)\n",
      "Saving model (epoch =  936, MSE loss = 2.2477)\n",
      "Saving model (epoch =  942, MSE loss = 2.2412)\n",
      "Saving model (epoch =  946, MSE loss = 2.1078)\n",
      "Saving model (epoch =  956, MSE loss = 1.9850)\n",
      "Saving model (epoch =  966, MSE loss = 1.8781)\n",
      "Saving model (epoch =  976, MSE loss = 1.7852)\n",
      "Saving model (epoch =  986, MSE loss = 1.7049)\n",
      "Saving model (epoch =  996, MSE loss = 1.6353)\n",
      "Saving model (epoch = 1006, MSE loss = 1.5739)\n",
      "Saving model (epoch = 1016, MSE loss = 1.5185)\n",
      "Saving model (epoch = 1026, MSE loss = 1.4696)\n",
      "Saving model (epoch = 1036, MSE loss = 1.4273)\n",
      "Saving model (epoch = 1046, MSE loss = 1.3904)\n",
      "Saving model (epoch = 1056, MSE loss = 1.3580)\n",
      "Saving model (epoch = 1066, MSE loss = 1.3290)\n",
      "Saving model (epoch = 1068, MSE loss = 1.3284)\n",
      "Saving model (epoch = 1076, MSE loss = 1.3023)\n",
      "Saving model (epoch = 1078, MSE loss = 1.2971)\n",
      "Saving model (epoch = 1086, MSE loss = 1.2779)\n",
      "Saving model (epoch = 1088, MSE loss = 1.2682)\n",
      "Saving model (epoch = 1096, MSE loss = 1.2555)\n",
      "Saving model (epoch = 1098, MSE loss = 1.2412)\n",
      "Saving model (epoch = 1106, MSE loss = 1.2355)\n",
      "Saving model (epoch = 1108, MSE loss = 1.2167)\n",
      "Saving model (epoch = 1118, MSE loss = 1.1939)\n",
      "Saving model (epoch = 1128, MSE loss = 1.1726)\n",
      "Saving model (epoch = 1138, MSE loss = 1.1531)\n",
      "Saving model (epoch = 1148, MSE loss = 1.1350)\n",
      "Saving model (epoch = 1158, MSE loss = 1.1182)\n",
      "Saving model (epoch = 1168, MSE loss = 1.1024)\n",
      "Saving model (epoch = 1178, MSE loss = 1.0874)\n",
      "Saving model (epoch = 1188, MSE loss = 1.0735)\n",
      "Saving model (epoch = 1198, MSE loss = 1.0605)\n",
      "Saving model (epoch = 1208, MSE loss = 1.0484)\n",
      "Saving model (epoch = 1218, MSE loss = 1.0369)\n",
      "Saving model (epoch = 1228, MSE loss = 1.0264)\n",
      "Saving model (epoch = 1238, MSE loss = 1.0166)\n",
      "Saving model (epoch = 1248, MSE loss = 1.0075)\n",
      "Saving model (epoch = 1258, MSE loss = 0.9989)\n",
      "Saving model (epoch = 1268, MSE loss = 0.9908)\n",
      "Saving model (epoch = 1278, MSE loss = 0.9829)\n",
      "Saving model (epoch = 1288, MSE loss = 0.9755)\n",
      "Saving model (epoch = 1298, MSE loss = 0.9688)\n",
      "Saving model (epoch = 1308, MSE loss = 0.9625)\n",
      "Saving model (epoch = 1318, MSE loss = 0.9565)\n",
      "Saving model (epoch = 1328, MSE loss = 0.9509)\n",
      "Saving model (epoch = 1338, MSE loss = 0.9455)\n",
      "Saving model (epoch = 1348, MSE loss = 0.9403)\n",
      "Saving model (epoch = 1358, MSE loss = 0.9355)\n",
      "Saving model (epoch = 1368, MSE loss = 0.9311)\n",
      "Saving model (epoch = 1378, MSE loss = 0.9269)\n",
      "Saving model (epoch = 1388, MSE loss = 0.9229)\n",
      "Saving model (epoch = 1398, MSE loss = 0.9188)\n",
      "Saving model (epoch = 1408, MSE loss = 0.9149)\n",
      "Saving model (epoch = 1418, MSE loss = 0.9112)\n",
      "Saving model (epoch = 1428, MSE loss = 0.9077)\n",
      "Saving model (epoch = 1438, MSE loss = 0.9045)\n",
      "Saving model (epoch = 1448, MSE loss = 0.9014)\n",
      "Saving model (epoch = 1458, MSE loss = 0.8986)\n",
      "Saving model (epoch = 1468, MSE loss = 0.8958)\n",
      "Saving model (epoch = 1478, MSE loss = 0.8932)\n",
      "Saving model (epoch = 1488, MSE loss = 0.8906)\n",
      "Saving model (epoch = 1498, MSE loss = 0.8881)\n",
      "Saving model (epoch = 1508, MSE loss = 0.8857)\n",
      "Saving model (epoch = 1518, MSE loss = 0.8835)\n",
      "Saving model (epoch = 1528, MSE loss = 0.8815)\n",
      "Saving model (epoch = 1538, MSE loss = 0.8795)\n",
      "Saving model (epoch = 1548, MSE loss = 0.8777)\n",
      "Saving model (epoch = 1558, MSE loss = 0.8759)\n",
      "Saving model (epoch = 1568, MSE loss = 0.8741)\n",
      "Saving model (epoch = 1578, MSE loss = 0.8723)\n",
      "Saving model (epoch = 1588, MSE loss = 0.8707)\n",
      "Saving model (epoch = 1598, MSE loss = 0.8692)\n",
      "Saving model (epoch = 1608, MSE loss = 0.8678)\n",
      "Saving model (epoch = 1618, MSE loss = 0.8664)\n",
      "Saving model (epoch = 1628, MSE loss = 0.8651)\n",
      "Saving model (epoch = 1638, MSE loss = 0.8638)\n",
      "Saving model (epoch = 1648, MSE loss = 0.8625)\n",
      "Saving model (epoch = 1658, MSE loss = 0.8613)\n",
      "Saving model (epoch = 1668, MSE loss = 0.8602)\n",
      "Saving model (epoch = 1678, MSE loss = 0.8590)\n",
      "Saving model (epoch = 1688, MSE loss = 0.8580)\n",
      "Saving model (epoch = 1698, MSE loss = 0.8569)\n",
      "Saving model (epoch = 1708, MSE loss = 0.8559)\n",
      "Saving model (epoch = 1718, MSE loss = 0.8550)\n",
      "Saving model (epoch = 1728, MSE loss = 0.8540)\n",
      "Saving model (epoch = 1738, MSE loss = 0.8531)\n",
      "Saving model (epoch = 1748, MSE loss = 0.8522)\n",
      "Saving model (epoch = 1758, MSE loss = 0.8513)\n",
      "Saving model (epoch = 1768, MSE loss = 0.8505)\n",
      "Saving model (epoch = 1778, MSE loss = 0.8497)\n",
      "Saving model (epoch = 1788, MSE loss = 0.8489)\n",
      "Saving model (epoch = 1798, MSE loss = 0.8482)\n",
      "Saving model (epoch = 1808, MSE loss = 0.8475)\n",
      "Saving model (epoch = 1818, MSE loss = 0.8468)\n",
      "Saving model (epoch = 1828, MSE loss = 0.8461)\n",
      "Saving model (epoch = 1838, MSE loss = 0.8453)\n",
      "Saving model (epoch = 1848, MSE loss = 0.8447)\n",
      "Saving model (epoch = 1858, MSE loss = 0.8441)\n",
      "Saving model (epoch = 1868, MSE loss = 0.8434)\n",
      "Saving model (epoch = 1878, MSE loss = 0.8429)\n",
      "Saving model (epoch = 1888, MSE loss = 0.8423)\n",
      "Saving model (epoch = 1898, MSE loss = 0.8418)\n",
      "Saving model (epoch = 1908, MSE loss = 0.8412)\n",
      "Saving model (epoch = 1918, MSE loss = 0.8407)\n",
      "Saving model (epoch = 1928, MSE loss = 0.8403)\n",
      "Saving model (epoch = 1938, MSE loss = 0.8398)\n",
      "Saving model (epoch = 1948, MSE loss = 0.8393)\n",
      "Saving model (epoch = 1958, MSE loss = 0.8388)\n",
      "Saving model (epoch = 1968, MSE loss = 0.8384)\n",
      "Saving model (epoch = 1978, MSE loss = 0.8380)\n",
      "Saving model (epoch = 1988, MSE loss = 0.8375)\n",
      "Saving model (epoch = 1998, MSE loss = 0.8371)\n",
      "Saving model (epoch = 2008, MSE loss = 0.8367)\n",
      "Saving model (epoch = 2018, MSE loss = 0.8364)\n",
      "Saving model (epoch = 2028, MSE loss = 0.8360)\n",
      "Saving model (epoch = 2038, MSE loss = 0.8357)\n",
      "Saving model (epoch = 2048, MSE loss = 0.8354)\n",
      "Saving model (epoch = 2058, MSE loss = 0.8351)\n",
      "Saving model (epoch = 2068, MSE loss = 0.8348)\n",
      "Saving model (epoch = 2078, MSE loss = 0.8345)\n",
      "Saving model (epoch = 2088, MSE loss = 0.8342)\n",
      "Saving model (epoch = 2098, MSE loss = 0.8339)\n",
      "Saving model (epoch = 2108, MSE loss = 0.8336)\n",
      "Saving model (epoch = 2118, MSE loss = 0.8333)\n",
      "Saving model (epoch = 2128, MSE loss = 0.8331)\n",
      "Saving model (epoch = 2138, MSE loss = 0.8328)\n",
      "Saving model (epoch = 2148, MSE loss = 0.8326)\n",
      "Saving model (epoch = 2158, MSE loss = 0.8324)\n",
      "Saving model (epoch = 2168, MSE loss = 0.8321)\n",
      "Saving model (epoch = 2178, MSE loss = 0.8319)\n",
      "Saving model (epoch = 2188, MSE loss = 0.8317)\n",
      "Saving model (epoch = 2198, MSE loss = 0.8315)\n",
      "Saving model (epoch = 2208, MSE loss = 0.8313)\n",
      "Saving model (epoch = 2218, MSE loss = 0.8311)\n",
      "Saving model (epoch = 2228, MSE loss = 0.8309)\n",
      "Saving model (epoch = 2238, MSE loss = 0.8307)\n",
      "Saving model (epoch = 2248, MSE loss = 0.8305)\n",
      "Saving model (epoch = 2258, MSE loss = 0.8303)\n",
      "Saving model (epoch = 2268, MSE loss = 0.8302)\n",
      "Saving model (epoch = 2278, MSE loss = 0.8300)\n",
      "Saving model (epoch = 2288, MSE loss = 0.8298)\n",
      "Saving model (epoch = 2298, MSE loss = 0.8297)\n",
      "Saving model (epoch = 2308, MSE loss = 0.8295)\n",
      "Saving model (epoch = 2318, MSE loss = 0.8294)\n",
      "Saving model (epoch = 2328, MSE loss = 0.8292)\n",
      "Saving model (epoch = 2338, MSE loss = 0.8291)\n",
      "Saving model (epoch = 2348, MSE loss = 0.8290)\n",
      "Saving model (epoch = 2358, MSE loss = 0.8289)\n",
      "Saving model (epoch = 2368, MSE loss = 0.8287)\n",
      "Saving model (epoch = 2378, MSE loss = 0.8286)\n",
      "Saving model (epoch = 2388, MSE loss = 0.8285)\n",
      "Saving model (epoch = 2398, MSE loss = 0.8284)\n",
      "Saving model (epoch = 2408, MSE loss = 0.8283)\n",
      "Saving model (epoch = 2418, MSE loss = 0.8282)\n",
      "Saving model (epoch = 2428, MSE loss = 0.8281)\n",
      "Saving model (epoch = 2438, MSE loss = 0.8280)\n",
      "Saving model (epoch = 2448, MSE loss = 0.8279)\n",
      "Saving model (epoch = 2458, MSE loss = 0.8278)\n",
      "Saving model (epoch = 2468, MSE loss = 0.8277)\n",
      "Saving model (epoch = 2478, MSE loss = 0.8276)\n",
      "Saving model (epoch = 2488, MSE loss = 0.8274)\n",
      "Saving model (epoch = 2498, MSE loss = 0.8273)\n",
      "Saving model (epoch = 2508, MSE loss = 0.8273)\n",
      "Saving model (epoch = 2518, MSE loss = 0.8272)\n",
      "Saving model (epoch = 2528, MSE loss = 0.8271)\n",
      "Saving model (epoch = 2538, MSE loss = 0.8270)\n",
      "Saving model (epoch = 2548, MSE loss = 0.8269)\n",
      "Saving model (epoch = 2558, MSE loss = 0.8268)\n",
      "Saving model (epoch = 2568, MSE loss = 0.8267)\n",
      "Saving model (epoch = 2578, MSE loss = 0.8266)\n",
      "Saving model (epoch = 2588, MSE loss = 0.8266)\n",
      "Saving model (epoch = 2598, MSE loss = 0.8265)\n",
      "Saving model (epoch = 2608, MSE loss = 0.8264)\n",
      "Saving model (epoch = 2618, MSE loss = 0.8263)\n",
      "Saving model (epoch = 2628, MSE loss = 0.8262)\n",
      "Saving model (epoch = 2638, MSE loss = 0.8261)\n",
      "Saving model (epoch = 2648, MSE loss = 0.8260)\n",
      "Saving model (epoch = 2658, MSE loss = 0.8258)\n",
      "Saving model (epoch = 2668, MSE loss = 0.8257)\n",
      "Saving model (epoch = 2678, MSE loss = 0.8256)\n",
      "Saving model (epoch = 2688, MSE loss = 0.8256)\n",
      "Saving model (epoch = 2698, MSE loss = 0.8255)\n",
      "Saving model (epoch = 2708, MSE loss = 0.8254)\n",
      "Saving model (epoch = 2718, MSE loss = 0.8253)\n",
      "Saving model (epoch = 2728, MSE loss = 0.8252)\n",
      "Saving model (epoch = 2738, MSE loss = 0.8251)\n",
      "Saving model (epoch = 2748, MSE loss = 0.8251)\n",
      "Saving model (epoch = 2758, MSE loss = 0.8250)\n",
      "Saving model (epoch = 2768, MSE loss = 0.8249)\n",
      "Saving model (epoch = 2778, MSE loss = 0.8248)\n",
      "Saving model (epoch = 2788, MSE loss = 0.8248)\n",
      "Saving model (epoch = 2798, MSE loss = 0.8247)\n",
      "Saving model (epoch = 2808, MSE loss = 0.8246)\n",
      "Saving model (epoch = 2818, MSE loss = 0.8246)\n",
      "Saving model (epoch = 2828, MSE loss = 0.8245)\n",
      "Saving model (epoch = 2838, MSE loss = 0.8244)\n",
      "Saving model (epoch = 2848, MSE loss = 0.8244)\n",
      "Saving model (epoch = 2858, MSE loss = 0.8243)\n",
      "Saving model (epoch = 2868, MSE loss = 0.8242)\n",
      "Saving model (epoch = 2878, MSE loss = 0.8242)\n",
      "Saving model (epoch = 2888, MSE loss = 0.8241)\n",
      "Saving model (epoch = 2898, MSE loss = 0.8241)\n",
      "Saving model (epoch = 2908, MSE loss = 0.8240)\n",
      "Saving model (epoch = 2918, MSE loss = 0.8240)\n",
      "Saving model (epoch = 2928, MSE loss = 0.8239)\n",
      "Saving model (epoch = 2938, MSE loss = 0.8239)\n",
      "Saving model (epoch = 2948, MSE loss = 0.8238)\n",
      "Saving model (epoch = 2958, MSE loss = 0.8238)\n",
      "Saving model (epoch = 2968, MSE loss = 0.8237)\n",
      "Saving model (epoch = 2978, MSE loss = 0.8237)\n",
      "Saving model (epoch = 2988, MSE loss = 0.8236)\n",
      "Saving model (epoch = 2998, MSE loss = 0.8236)\n",
      "Saving model (epoch = 3008, MSE loss = 0.8235)\n",
      "Saving model (epoch = 3018, MSE loss = 0.8235)\n",
      "Saving model (epoch = 3028, MSE loss = 0.8234)\n",
      "Saving model (epoch = 3038, MSE loss = 0.8234)\n",
      "Saving model (epoch = 3048, MSE loss = 0.8233)\n",
      "Saving model (epoch = 3058, MSE loss = 0.8233)\n",
      "Saving model (epoch = 3068, MSE loss = 0.8232)\n",
      "Saving model (epoch = 3078, MSE loss = 0.8232)\n",
      "Saving model (epoch = 3088, MSE loss = 0.8231)\n",
      "Saving model (epoch = 3098, MSE loss = 0.8230)\n",
      "Saving model (epoch = 3108, MSE loss = 0.8230)\n",
      "Saving model (epoch = 3118, MSE loss = 0.8230)\n",
      "Saving model (epoch = 3128, MSE loss = 0.8229)\n",
      "Saving model (epoch = 3138, MSE loss = 0.8229)\n",
      "Saving model (epoch = 3148, MSE loss = 0.8229)\n",
      "Saving model (epoch = 3158, MSE loss = 0.8228)\n",
      "Saving model (epoch = 3168, MSE loss = 0.8228)\n",
      "Saving model (epoch = 3178, MSE loss = 0.8227)\n",
      "Saving model (epoch = 3188, MSE loss = 0.8227)\n",
      "Saving model (epoch = 3198, MSE loss = 0.8226)\n",
      "Saving model (epoch = 3208, MSE loss = 0.8226)\n",
      "Saving model (epoch = 3218, MSE loss = 0.8225)\n",
      "Saving model (epoch = 3228, MSE loss = 0.8225)\n",
      "Saving model (epoch = 3238, MSE loss = 0.8225)\n",
      "Saving model (epoch = 3248, MSE loss = 0.8224)\n",
      "Saving model (epoch = 3258, MSE loss = 0.8224)\n",
      "Saving model (epoch = 3268, MSE loss = 0.8223)\n",
      "Saving model (epoch = 3278, MSE loss = 0.8223)\n",
      "Saving model (epoch = 3288, MSE loss = 0.8222)\n",
      "Saving model (epoch = 3298, MSE loss = 0.8222)\n",
      "Saving model (epoch = 3308, MSE loss = 0.8222)\n",
      "Saving model (epoch = 3318, MSE loss = 0.8222)\n",
      "Saving model (epoch = 3328, MSE loss = 0.8221)\n",
      "Saving model (epoch = 3338, MSE loss = 0.8220)\n",
      "Saving model (epoch = 3348, MSE loss = 0.8220)\n",
      "Saving model (epoch = 3358, MSE loss = 0.8219)\n",
      "Saving model (epoch = 3368, MSE loss = 0.8219)\n",
      "Saving model (epoch = 3378, MSE loss = 0.8219)\n",
      "Saving model (epoch = 3388, MSE loss = 0.8218)\n",
      "Saving model (epoch = 3398, MSE loss = 0.8217)\n",
      "Saving model (epoch = 3408, MSE loss = 0.8217)\n",
      "Saving model (epoch = 3418, MSE loss = 0.8217)\n",
      "Saving model (epoch = 3428, MSE loss = 0.8216)\n",
      "Saving model (epoch = 3438, MSE loss = 0.8216)\n",
      "Saving model (epoch = 3448, MSE loss = 0.8215)\n",
      "Saving model (epoch = 3458, MSE loss = 0.8214)\n",
      "Saving model (epoch = 3468, MSE loss = 0.8214)\n",
      "Saving model (epoch = 3478, MSE loss = 0.8213)\n",
      "Saving model (epoch = 3488, MSE loss = 0.8213)\n",
      "Saving model (epoch = 3498, MSE loss = 0.8212)\n",
      "Saving model (epoch = 3508, MSE loss = 0.8212)\n",
      "Saving model (epoch = 3518, MSE loss = 0.8211)\n",
      "Saving model (epoch = 3528, MSE loss = 0.8211)\n",
      "Saving model (epoch = 3538, MSE loss = 0.8210)\n",
      "Saving model (epoch = 3548, MSE loss = 0.8209)\n",
      "Saving model (epoch = 3558, MSE loss = 0.8209)\n",
      "Saving model (epoch = 3568, MSE loss = 0.8209)\n",
      "Saving model (epoch = 3578, MSE loss = 0.8208)\n",
      "Saving model (epoch = 3588, MSE loss = 0.8208)\n",
      "Saving model (epoch = 3598, MSE loss = 0.8207)\n",
      "Saving model (epoch = 3608, MSE loss = 0.8206)\n",
      "Saving model (epoch = 3618, MSE loss = 0.8206)\n",
      "Saving model (epoch = 3628, MSE loss = 0.8206)\n",
      "Saving model (epoch = 3638, MSE loss = 0.8205)\n",
      "Saving model (epoch = 3648, MSE loss = 0.8205)\n",
      "Saving model (epoch = 3658, MSE loss = 0.8204)\n",
      "Saving model (epoch = 3668, MSE loss = 0.8204)\n",
      "Saving model (epoch = 3678, MSE loss = 0.8204)\n",
      "Saving model (epoch = 3688, MSE loss = 0.8203)\n",
      "Saving model (epoch = 3698, MSE loss = 0.8203)\n",
      "Saving model (epoch = 3708, MSE loss = 0.8203)\n",
      "Saving model (epoch = 3718, MSE loss = 0.8202)\n",
      "Saving model (epoch = 3728, MSE loss = 0.8202)\n",
      "Saving model (epoch = 3738, MSE loss = 0.8201)\n",
      "Saving model (epoch = 3748, MSE loss = 0.8201)\n",
      "Saving model (epoch = 3758, MSE loss = 0.8201)\n",
      "Saving model (epoch = 3768, MSE loss = 0.8200)\n",
      "Saving model (epoch = 3778, MSE loss = 0.8200)\n",
      "Saving model (epoch = 3788, MSE loss = 0.8200)\n",
      "Saving model (epoch = 3798, MSE loss = 0.8199)\n",
      "Saving model (epoch = 3808, MSE loss = 0.8199)\n",
      "Saving model (epoch = 3818, MSE loss = 0.8199)\n",
      "Saving model (epoch = 3828, MSE loss = 0.8198)\n",
      "Saving model (epoch = 3838, MSE loss = 0.8198)\n",
      "Saving model (epoch = 3848, MSE loss = 0.8198)\n",
      "Saving model (epoch = 3858, MSE loss = 0.8198)\n",
      "Saving model (epoch = 3868, MSE loss = 0.8198)\n",
      "Saving model (epoch = 3878, MSE loss = 0.8198)\n",
      "Saving model (epoch = 3888, MSE loss = 0.8197)\n",
      "Saving model (epoch = 3898, MSE loss = 0.8197)\n",
      "Saving model (epoch = 3908, MSE loss = 0.8197)\n",
      "Saving model (epoch = 3918, MSE loss = 0.8196)\n",
      "Saving model (epoch = 3928, MSE loss = 0.8196)\n",
      "Saving model (epoch = 3938, MSE loss = 0.8196)\n",
      "Saving model (epoch = 3948, MSE loss = 0.8195)\n",
      "Saving model (epoch = 3958, MSE loss = 0.8195)\n",
      "Saving model (epoch = 3968, MSE loss = 0.8195)\n",
      "Saving model (epoch = 3978, MSE loss = 0.8195)\n",
      "Saving model (epoch = 3988, MSE loss = 0.8195)\n",
      "Saving model (epoch = 3998, MSE loss = 0.8194)\n",
      "Saving model (epoch = 4008, MSE loss = 0.8194)\n",
      "Saving model (epoch = 4028, MSE loss = 0.8194)\n",
      "Saving model (epoch = 4038, MSE loss = 0.8194)\n",
      "Saving model (epoch = 4048, MSE loss = 0.8193)\n",
      "Saving model (epoch = 4058, MSE loss = 0.8193)\n",
      "Saving model (epoch = 4068, MSE loss = 0.8193)\n",
      "Saving model (epoch = 4088, MSE loss = 0.8192)\n",
      "Saving model (epoch = 4098, MSE loss = 0.8192)\n",
      "Saving model (epoch = 4108, MSE loss = 0.8192)\n",
      "Saving model (epoch = 4118, MSE loss = 0.8192)\n",
      "Saving model (epoch = 4128, MSE loss = 0.8191)\n",
      "Saving model (epoch = 4138, MSE loss = 0.8191)\n",
      "Saving model (epoch = 4148, MSE loss = 0.8191)\n",
      "Saving model (epoch = 4158, MSE loss = 0.8191)\n",
      "Saving model (epoch = 4168, MSE loss = 0.8191)\n",
      "Saving model (epoch = 4178, MSE loss = 0.8190)\n",
      "Saving model (epoch = 4188, MSE loss = 0.8190)\n",
      "Saving model (epoch = 4198, MSE loss = 0.8190)\n",
      "Saving model (epoch = 4218, MSE loss = 0.8189)\n",
      "Saving model (epoch = 4228, MSE loss = 0.8189)\n",
      "Saving model (epoch = 4248, MSE loss = 0.8189)\n",
      "Saving model (epoch = 4258, MSE loss = 0.8189)\n",
      "Saving model (epoch = 4278, MSE loss = 0.8189)\n",
      "Saving model (epoch = 4288, MSE loss = 0.8189)\n",
      "Saving model (epoch = 4308, MSE loss = 0.8188)\n",
      "Saving model (epoch = 4318, MSE loss = 0.8188)\n",
      "Saving model (epoch = 4338, MSE loss = 0.8188)\n",
      "Saving model (epoch = 4368, MSE loss = 0.8188)\n",
      "Saving model (epoch = 4398, MSE loss = 0.8188)\n",
      "Saving model (epoch = 4418, MSE loss = 0.8188)\n",
      "Saving model (epoch = 4438, MSE loss = 0.8188)\n",
      "Saving model (epoch = 4448, MSE loss = 0.8188)\n",
      "Saving model (epoch = 4458, MSE loss = 0.8187)\n",
      "Saving model (epoch = 4468, MSE loss = 0.8187)\n",
      "Saving model (epoch = 4478, MSE loss = 0.8187)\n",
      "Saving model (epoch = 4488, MSE loss = 0.8187)\n",
      "Saving model (epoch = 4498, MSE loss = 0.8187)\n",
      "Saving model (epoch = 4518, MSE loss = 0.8186)\n",
      "Saving model (epoch = 4528, MSE loss = 0.8186)\n",
      "Saving model (epoch = 4538, MSE loss = 0.8186)\n",
      "Saving model (epoch = 4548, MSE loss = 0.8186)\n",
      "Saving model (epoch = 4568, MSE loss = 0.8185)\n",
      "Saving model (epoch = 4588, MSE loss = 0.8185)\n",
      "Saving model (epoch = 4608, MSE loss = 0.8185)\n",
      "Saving model (epoch = 4618, MSE loss = 0.8185)\n",
      "Saving model (epoch = 4628, MSE loss = 0.8184)\n",
      "Saving model (epoch = 4648, MSE loss = 0.8184)\n",
      "Saving model (epoch = 4658, MSE loss = 0.8184)\n",
      "Saving model (epoch = 4668, MSE loss = 0.8184)\n",
      "Saving model (epoch = 4678, MSE loss = 0.8183)\n",
      "Saving model (epoch = 4688, MSE loss = 0.8183)\n",
      "Saving model (epoch = 4698, MSE loss = 0.8183)\n",
      "Saving model (epoch = 4708, MSE loss = 0.8183)\n",
      "Saving model (epoch = 4718, MSE loss = 0.8182)\n",
      "Saving model (epoch = 4728, MSE loss = 0.8182)\n",
      "Saving model (epoch = 4738, MSE loss = 0.8182)\n",
      "Saving model (epoch = 4748, MSE loss = 0.8182)\n",
      "Saving model (epoch = 4758, MSE loss = 0.8181)\n",
      "Saving model (epoch = 4768, MSE loss = 0.8181)\n",
      "Saving model (epoch = 4778, MSE loss = 0.8181)\n",
      "Saving model (epoch = 4788, MSE loss = 0.8180)\n",
      "Saving model (epoch = 4798, MSE loss = 0.8180)\n",
      "Saving model (epoch = 4808, MSE loss = 0.8180)\n",
      "Saving model (epoch = 4818, MSE loss = 0.8179)\n",
      "Saving model (epoch = 4828, MSE loss = 0.8179)\n",
      "Saving model (epoch = 4838, MSE loss = 0.8179)\n",
      "Saving model (epoch = 4848, MSE loss = 0.8178)\n",
      "Saving model (epoch = 4858, MSE loss = 0.8178)\n",
      "Saving model (epoch = 4868, MSE loss = 0.8178)\n",
      "Saving model (epoch = 4878, MSE loss = 0.8177)\n",
      "Saving model (epoch = 4888, MSE loss = 0.8177)\n",
      "Saving model (epoch = 4898, MSE loss = 0.8177)\n",
      "Saving model (epoch = 4908, MSE loss = 0.8176)\n",
      "Saving model (epoch = 4918, MSE loss = 0.8176)\n",
      "Saving model (epoch = 4928, MSE loss = 0.8175)\n",
      "Saving model (epoch = 4938, MSE loss = 0.8175)\n",
      "Saving model (epoch = 4948, MSE loss = 0.8174)\n",
      "Saving model (epoch = 4958, MSE loss = 0.8174)\n",
      "Saving model (epoch = 4968, MSE loss = 0.8173)\n",
      "Saving model (epoch = 4978, MSE loss = 0.8173)\n",
      "Saving model (epoch = 4988, MSE loss = 0.8172)\n",
      "Saving model (epoch = 4998, MSE loss = 0.8172)\n",
      "Saving model (epoch = 5008, MSE loss = 0.8171)\n",
      "Saving model (epoch = 5018, MSE loss = 0.8171)\n",
      "Saving model (epoch = 5028, MSE loss = 0.8170)\n",
      "Saving model (epoch = 5038, MSE loss = 0.8170)\n",
      "Saving model (epoch = 5048, MSE loss = 0.8169)\n",
      "Saving model (epoch = 5058, MSE loss = 0.8169)\n",
      "Saving model (epoch = 5068, MSE loss = 0.8168)\n",
      "Saving model (epoch = 5078, MSE loss = 0.8168)\n",
      "Saving model (epoch = 5088, MSE loss = 0.8168)\n",
      "Saving model (epoch = 5098, MSE loss = 0.8167)\n",
      "Saving model (epoch = 5108, MSE loss = 0.8167)\n",
      "Saving model (epoch = 5118, MSE loss = 0.8166)\n",
      "Saving model (epoch = 5128, MSE loss = 0.8166)\n",
      "Saving model (epoch = 5138, MSE loss = 0.8165)\n",
      "Saving model (epoch = 5148, MSE loss = 0.8165)\n",
      "Saving model (epoch = 5158, MSE loss = 0.8164)\n",
      "Saving model (epoch = 5168, MSE loss = 0.8164)\n",
      "Saving model (epoch = 5178, MSE loss = 0.8163)\n",
      "Saving model (epoch = 5188, MSE loss = 0.8163)\n",
      "Saving model (epoch = 5198, MSE loss = 0.8163)\n",
      "Saving model (epoch = 5208, MSE loss = 0.8162)\n",
      "Saving model (epoch = 5218, MSE loss = 0.8161)\n",
      "Saving model (epoch = 5228, MSE loss = 0.8161)\n",
      "Saving model (epoch = 5238, MSE loss = 0.8160)\n",
      "Saving model (epoch = 5248, MSE loss = 0.8160)\n",
      "Saving model (epoch = 5258, MSE loss = 0.8159)\n",
      "Saving model (epoch = 5268, MSE loss = 0.8159)\n",
      "Saving model (epoch = 5278, MSE loss = 0.8158)\n",
      "Saving model (epoch = 5288, MSE loss = 0.8158)\n",
      "Saving model (epoch = 5298, MSE loss = 0.8157)\n",
      "Saving model (epoch = 5308, MSE loss = 0.8157)\n",
      "Saving model (epoch = 5318, MSE loss = 0.8156)\n",
      "Saving model (epoch = 5328, MSE loss = 0.8156)\n",
      "Saving model (epoch = 5338, MSE loss = 0.8155)\n",
      "Saving model (epoch = 5348, MSE loss = 0.8155)\n",
      "Saving model (epoch = 5358, MSE loss = 0.8154)\n",
      "Saving model (epoch = 5368, MSE loss = 0.8154)\n",
      "Saving model (epoch = 5378, MSE loss = 0.8154)\n",
      "Saving model (epoch = 5388, MSE loss = 0.8153)\n",
      "Saving model (epoch = 5398, MSE loss = 0.8153)\n",
      "Saving model (epoch = 5408, MSE loss = 0.8153)\n",
      "Saving model (epoch = 5418, MSE loss = 0.8152)\n",
      "Saving model (epoch = 5428, MSE loss = 0.8152)\n",
      "Saving model (epoch = 5438, MSE loss = 0.8151)\n",
      "Saving model (epoch = 5448, MSE loss = 0.8151)\n",
      "Saving model (epoch = 5458, MSE loss = 0.8150)\n",
      "Saving model (epoch = 5468, MSE loss = 0.8150)\n",
      "Saving model (epoch = 5478, MSE loss = 0.8150)\n",
      "Saving model (epoch = 5488, MSE loss = 0.8149)\n",
      "Saving model (epoch = 5498, MSE loss = 0.8149)\n",
      "Saving model (epoch = 5508, MSE loss = 0.8149)\n",
      "Saving model (epoch = 5518, MSE loss = 0.8148)\n",
      "Saving model (epoch = 5528, MSE loss = 0.8148)\n",
      "Saving model (epoch = 5538, MSE loss = 0.8148)\n",
      "Saving model (epoch = 5548, MSE loss = 0.8147)\n",
      "Saving model (epoch = 5558, MSE loss = 0.8147)\n",
      "Saving model (epoch = 5568, MSE loss = 0.8147)\n",
      "Saving model (epoch = 5578, MSE loss = 0.8146)\n",
      "Saving model (epoch = 5588, MSE loss = 0.8146)\n",
      "Saving model (epoch = 5598, MSE loss = 0.8146)\n",
      "Saving model (epoch = 5608, MSE loss = 0.8145)\n",
      "Saving model (epoch = 5618, MSE loss = 0.8145)\n",
      "Saving model (epoch = 5628, MSE loss = 0.8144)\n",
      "Saving model (epoch = 5638, MSE loss = 0.8144)\n",
      "Saving model (epoch = 5648, MSE loss = 0.8143)\n",
      "Saving model (epoch = 5658, MSE loss = 0.8142)\n",
      "Saving model (epoch = 5668, MSE loss = 0.8142)\n",
      "Saving model (epoch = 5678, MSE loss = 0.8141)\n",
      "Saving model (epoch = 5688, MSE loss = 0.8141)\n",
      "Saving model (epoch = 5698, MSE loss = 0.8140)\n",
      "Saving model (epoch = 5708, MSE loss = 0.8140)\n",
      "Saving model (epoch = 5718, MSE loss = 0.8140)\n",
      "Saving model (epoch = 5728, MSE loss = 0.8139)\n",
      "Saving model (epoch = 5738, MSE loss = 0.8139)\n",
      "Saving model (epoch = 5748, MSE loss = 0.8139)\n",
      "Saving model (epoch = 5758, MSE loss = 0.8138)\n",
      "Saving model (epoch = 5768, MSE loss = 0.8138)\n",
      "Saving model (epoch = 5778, MSE loss = 0.8137)\n",
      "Saving model (epoch = 5788, MSE loss = 0.8137)\n",
      "Saving model (epoch = 5798, MSE loss = 0.8136)\n",
      "Saving model (epoch = 5808, MSE loss = 0.8136)\n",
      "Saving model (epoch = 5818, MSE loss = 0.8136)\n",
      "Saving model (epoch = 5828, MSE loss = 0.8135)\n",
      "Saving model (epoch = 5838, MSE loss = 0.8135)\n",
      "Saving model (epoch = 5848, MSE loss = 0.8135)\n",
      "Saving model (epoch = 5858, MSE loss = 0.8135)\n",
      "Saving model (epoch = 5898, MSE loss = 0.8134)\n",
      "Saving model (epoch = 5908, MSE loss = 0.8134)\n",
      "Saving model (epoch = 5918, MSE loss = 0.8134)\n",
      "Saving model (epoch = 5928, MSE loss = 0.8134)\n",
      "Saving model (epoch = 5938, MSE loss = 0.8134)\n",
      "Saving model (epoch = 5948, MSE loss = 0.8134)\n",
      "Saving model (epoch = 5958, MSE loss = 0.8133)\n",
      "Saving model (epoch = 5968, MSE loss = 0.8133)\n",
      "Saving model (epoch = 5978, MSE loss = 0.8133)\n",
      "Saving model (epoch = 5988, MSE loss = 0.8133)\n",
      "Saving model (epoch = 5998, MSE loss = 0.8132)\n",
      "Saving model (epoch = 6008, MSE loss = 0.8132)\n",
      "Saving model (epoch = 6018, MSE loss = 0.8132)\n",
      "Saving model (epoch = 6028, MSE loss = 0.8132)\n",
      "Saving model (epoch = 6038, MSE loss = 0.8132)\n",
      "Saving model (epoch = 6048, MSE loss = 0.8131)\n",
      "Saving model (epoch = 6058, MSE loss = 0.8131)\n",
      "Saving model (epoch = 6068, MSE loss = 0.8131)\n",
      "Saving model (epoch = 6078, MSE loss = 0.8131)\n",
      "Saving model (epoch = 6088, MSE loss = 0.8131)\n",
      "Saving model (epoch = 6098, MSE loss = 0.8130)\n",
      "Saving model (epoch = 6108, MSE loss = 0.8130)\n",
      "Saving model (epoch = 6118, MSE loss = 0.8130)\n",
      "Saving model (epoch = 6128, MSE loss = 0.8130)\n",
      "Saving model (epoch = 6138, MSE loss = 0.8129)\n",
      "Saving model (epoch = 6148, MSE loss = 0.8129)\n",
      "Saving model (epoch = 6158, MSE loss = 0.8129)\n",
      "Saving model (epoch = 6168, MSE loss = 0.8129)\n",
      "Saving model (epoch = 6178, MSE loss = 0.8129)\n",
      "Saving model (epoch = 6188, MSE loss = 0.8129)\n",
      "Saving model (epoch = 6208, MSE loss = 0.8129)\n",
      "Saving model (epoch = 6218, MSE loss = 0.8129)\n",
      "Saving model (epoch = 6228, MSE loss = 0.8128)\n",
      "Saving model (epoch = 6238, MSE loss = 0.8128)\n",
      "Saving model (epoch = 6258, MSE loss = 0.8128)\n",
      "Saving model (epoch = 6278, MSE loss = 0.8128)\n",
      "Saving model (epoch = 6298, MSE loss = 0.8128)\n",
      "Saving model (epoch = 6308, MSE loss = 0.8127)\n",
      "Saving model (epoch = 6328, MSE loss = 0.8127)\n",
      "Saving model (epoch = 6348, MSE loss = 0.8127)\n",
      "Saving model (epoch = 6358, MSE loss = 0.8127)\n",
      "Saving model (epoch = 6368, MSE loss = 0.8127)\n",
      "Saving model (epoch = 6378, MSE loss = 0.8127)\n",
      "Saving model (epoch = 6428, MSE loss = 0.8127)\n",
      "Saving model (epoch = 6448, MSE loss = 0.8127)\n",
      "Saving model (epoch = 6468, MSE loss = 0.8126)\n",
      "Saving model (epoch = 6498, MSE loss = 0.8126)\n",
      "Saving model (epoch = 6518, MSE loss = 0.8126)\n",
      "Saving model (epoch = 6538, MSE loss = 0.8126)\n",
      "Saving model (epoch = 6578, MSE loss = 0.8126)\n",
      "Saving model (epoch = 6588, MSE loss = 0.8126)\n",
      "Saving model (epoch = 6598, MSE loss = 0.8126)\n",
      "Saving model (epoch = 6608, MSE loss = 0.8126)\n",
      "Saving model (epoch = 6618, MSE loss = 0.8126)\n",
      "Saving model (epoch = 6628, MSE loss = 0.8126)\n",
      "Saving model (epoch = 6648, MSE loss = 0.8125)\n",
      "Saving model (epoch = 6668, MSE loss = 0.8125)\n",
      "Saving model (epoch = 6688, MSE loss = 0.8125)\n",
      "Saving model (epoch = 6708, MSE loss = 0.8125)\n",
      "Saving model (epoch = 6718, MSE loss = 0.8125)\n",
      "Saving model (epoch = 6728, MSE loss = 0.8125)\n",
      "Saving model (epoch = 6738, MSE loss = 0.8125)\n",
      "Saving model (epoch = 6758, MSE loss = 0.8125)\n",
      "Saving model (epoch = 6778, MSE loss = 0.8125)\n",
      "Saving model (epoch = 6798, MSE loss = 0.8125)\n",
      "Saving model (epoch = 6828, MSE loss = 0.8125)\n",
      "Saving model (epoch = 6848, MSE loss = 0.8125)\n",
      "Saving model (epoch = 6858, MSE loss = 0.8125)\n",
      "Saving model (epoch = 6878, MSE loss = 0.8125)\n",
      "Saving model (epoch = 6908, MSE loss = 0.8124)\n",
      "Saving model (epoch = 6918, MSE loss = 0.8124)\n",
      "Saving model (epoch = 6928, MSE loss = 0.8124)\n",
      "Saving model (epoch = 6938, MSE loss = 0.8124)\n",
      "Saving model (epoch = 6948, MSE loss = 0.8123)\n",
      "Saving model (epoch = 6958, MSE loss = 0.8123)\n",
      "Saving model (epoch = 6968, MSE loss = 0.8123)\n",
      "Saving model (epoch = 6978, MSE loss = 0.8123)\n",
      "Saving model (epoch = 6988, MSE loss = 0.8123)\n",
      "Saving model (epoch = 6998, MSE loss = 0.8123)\n",
      "Saving model (epoch = 7008, MSE loss = 0.8123)\n",
      "Saving model (epoch = 7018, MSE loss = 0.8123)\n",
      "Saving model (epoch = 7028, MSE loss = 0.8123)\n",
      "Saving model (epoch = 7038, MSE loss = 0.8123)\n",
      "Saving model (epoch = 7048, MSE loss = 0.8122)\n",
      "Saving model (epoch = 7058, MSE loss = 0.8122)\n",
      "Saving model (epoch = 7068, MSE loss = 0.8122)\n",
      "Saving model (epoch = 7078, MSE loss = 0.8122)\n",
      "Saving model (epoch = 7098, MSE loss = 0.8122)\n",
      "Saving model (epoch = 7108, MSE loss = 0.8122)\n",
      "Saving model (epoch = 7118, MSE loss = 0.8122)\n",
      "Saving model (epoch = 7128, MSE loss = 0.8122)\n",
      "Saving model (epoch = 7138, MSE loss = 0.8121)\n",
      "Saving model (epoch = 7148, MSE loss = 0.8121)\n",
      "Saving model (epoch = 7158, MSE loss = 0.8121)\n",
      "Saving model (epoch = 7168, MSE loss = 0.8121)\n",
      "Saving model (epoch = 7188, MSE loss = 0.8121)\n",
      "Saving model (epoch = 7198, MSE loss = 0.8121)\n",
      "Saving model (epoch = 7208, MSE loss = 0.8121)\n",
      "Saving model (epoch = 7238, MSE loss = 0.8121)\n",
      "Saving model (epoch = 7248, MSE loss = 0.8120)\n",
      "Saving model (epoch = 7258, MSE loss = 0.8120)\n",
      "Saving model (epoch = 7268, MSE loss = 0.8120)\n",
      "Saving model (epoch = 7278, MSE loss = 0.8120)\n",
      "Saving model (epoch = 7298, MSE loss = 0.8119)\n",
      "Saving model (epoch = 7308, MSE loss = 0.8119)\n",
      "Saving model (epoch = 7318, MSE loss = 0.8119)\n",
      "Saving model (epoch = 7328, MSE loss = 0.8119)\n",
      "Saving model (epoch = 7338, MSE loss = 0.8119)\n",
      "Saving model (epoch = 7348, MSE loss = 0.8119)\n",
      "Saving model (epoch = 7358, MSE loss = 0.8118)\n",
      "Saving model (epoch = 7378, MSE loss = 0.8118)\n",
      "Saving model (epoch = 7388, MSE loss = 0.8118)\n",
      "Saving model (epoch = 7398, MSE loss = 0.8118)\n",
      "Saving model (epoch = 7408, MSE loss = 0.8118)\n",
      "Saving model (epoch = 7418, MSE loss = 0.8118)\n",
      "Saving model (epoch = 7428, MSE loss = 0.8117)\n",
      "Saving model (epoch = 7438, MSE loss = 0.8117)\n",
      "Saving model (epoch = 7448, MSE loss = 0.8117)\n",
      "Saving model (epoch = 7458, MSE loss = 0.8117)\n",
      "Saving model (epoch = 7468, MSE loss = 0.8117)\n",
      "Saving model (epoch = 7478, MSE loss = 0.8117)\n",
      "Saving model (epoch = 7488, MSE loss = 0.8116)\n",
      "Saving model (epoch = 7508, MSE loss = 0.8116)\n",
      "Saving model (epoch = 7528, MSE loss = 0.8116)\n",
      "Saving model (epoch = 7538, MSE loss = 0.8116)\n",
      "Saving model (epoch = 7578, MSE loss = 0.8116)\n",
      "Saving model (epoch = 7598, MSE loss = 0.8115)\n",
      "Saving model (epoch = 7618, MSE loss = 0.8115)\n",
      "Saving model (epoch = 7628, MSE loss = 0.8115)\n",
      "Saving model (epoch = 7648, MSE loss = 0.8115)\n",
      "Saving model (epoch = 7668, MSE loss = 0.8115)\n",
      "Saving model (epoch = 7678, MSE loss = 0.8114)\n",
      "Saving model (epoch = 7688, MSE loss = 0.8114)\n",
      "Saving model (epoch = 7698, MSE loss = 0.8114)\n",
      "Saving model (epoch = 7718, MSE loss = 0.8114)\n",
      "Saving model (epoch = 7738, MSE loss = 0.8114)\n",
      "Saving model (epoch = 7748, MSE loss = 0.8114)\n",
      "Saving model (epoch = 7768, MSE loss = 0.8114)\n",
      "Saving model (epoch = 7778, MSE loss = 0.8114)\n",
      "Saving model (epoch = 7798, MSE loss = 0.8113)\n",
      "Saving model (epoch = 7818, MSE loss = 0.8113)\n",
      "Saving model (epoch = 7828, MSE loss = 0.8113)\n",
      "Saving model (epoch = 7848, MSE loss = 0.8113)\n",
      "Saving model (epoch = 7868, MSE loss = 0.8113)\n",
      "Saving model (epoch = 7878, MSE loss = 0.8113)\n",
      "Saving model (epoch = 7898, MSE loss = 0.8113)\n",
      "Saving model (epoch = 7938, MSE loss = 0.8113)\n",
      "Saving model (epoch = 7958, MSE loss = 0.8113)\n",
      "Saving model (epoch = 7978, MSE loss = 0.8112)\n",
      "Saving model (epoch = 7998, MSE loss = 0.8112)\n",
      "Saving model (epoch = 8008, MSE loss = 0.8112)\n",
      "Saving model (epoch = 8018, MSE loss = 0.8112)\n",
      "Saving model (epoch = 8028, MSE loss = 0.8112)\n",
      "Saving model (epoch = 8038, MSE loss = 0.8112)\n",
      "Saving model (epoch = 8058, MSE loss = 0.8112)\n",
      "Saving model (epoch = 8078, MSE loss = 0.8112)\n",
      "Saving model (epoch = 8098, MSE loss = 0.8111)\n",
      "Saving model (epoch = 8108, MSE loss = 0.8111)\n",
      "Saving model (epoch = 8118, MSE loss = 0.8111)\n",
      "Saving model (epoch = 8128, MSE loss = 0.8111)\n",
      "Saving model (epoch = 8138, MSE loss = 0.8111)\n",
      "Saving model (epoch = 8148, MSE loss = 0.8111)\n",
      "Saving model (epoch = 8168, MSE loss = 0.8111)\n",
      "Saving model (epoch = 8178, MSE loss = 0.8110)\n",
      "Saving model (epoch = 8188, MSE loss = 0.8110)\n",
      "Saving model (epoch = 8198, MSE loss = 0.8110)\n",
      "Saving model (epoch = 8238, MSE loss = 0.8110)\n",
      "Saving model (epoch = 8268, MSE loss = 0.8110)\n",
      "Saving model (epoch = 8278, MSE loss = 0.8110)\n",
      "Saving model (epoch = 8298, MSE loss = 0.8110)\n",
      "Saving model (epoch = 8318, MSE loss = 0.8110)\n",
      "Saving model (epoch = 8328, MSE loss = 0.8110)\n",
      "Saving model (epoch = 8348, MSE loss = 0.8109)\n",
      "Saving model (epoch = 8378, MSE loss = 0.8109)\n",
      "Saving model (epoch = 8468, MSE loss = 0.8109)\n",
      "Saving model (epoch = 8498, MSE loss = 0.8109)\n",
      "Saving model (epoch = 8508, MSE loss = 0.8109)\n",
      "Saving model (epoch = 8518, MSE loss = 0.8109)\n",
      "Saving model (epoch = 8528, MSE loss = 0.8109)\n",
      "Saving model (epoch = 8538, MSE loss = 0.8108)\n",
      "Saving model (epoch = 8548, MSE loss = 0.8108)\n",
      "Saving model (epoch = 8558, MSE loss = 0.8108)\n",
      "Saving model (epoch = 8568, MSE loss = 0.8108)\n",
      "Saving model (epoch = 8588, MSE loss = 0.8108)\n",
      "Saving model (epoch = 8598, MSE loss = 0.8108)\n",
      "Saving model (epoch = 8608, MSE loss = 0.8107)\n",
      "Saving model (epoch = 8628, MSE loss = 0.8107)\n",
      "Saving model (epoch = 8638, MSE loss = 0.8107)\n",
      "Saving model (epoch = 8648, MSE loss = 0.8107)\n",
      "Saving model (epoch = 8668, MSE loss = 0.8106)\n",
      "Saving model (epoch = 8678, MSE loss = 0.8106)\n",
      "Saving model (epoch = 8688, MSE loss = 0.8105)\n",
      "Saving model (epoch = 8698, MSE loss = 0.8105)\n",
      "Saving model (epoch = 8708, MSE loss = 0.8104)\n",
      "Saving model (epoch = 8718, MSE loss = 0.8104)\n",
      "Saving model (epoch = 8738, MSE loss = 0.8104)\n",
      "Saving model (epoch = 8748, MSE loss = 0.8104)\n",
      "Saving model (epoch = 8758, MSE loss = 0.8103)\n",
      "Saving model (epoch = 8768, MSE loss = 0.8103)\n",
      "Saving model (epoch = 8778, MSE loss = 0.8103)\n",
      "Saving model (epoch = 8788, MSE loss = 0.8103)\n",
      "Saving model (epoch = 8798, MSE loss = 0.8103)\n",
      "Saving model (epoch = 8808, MSE loss = 0.8103)\n",
      "Saving model (epoch = 8818, MSE loss = 0.8103)\n",
      "Saving model (epoch = 8828, MSE loss = 0.8102)\n",
      "Saving model (epoch = 8848, MSE loss = 0.8102)\n",
      "Saving model (epoch = 8858, MSE loss = 0.8102)\n",
      "Saving model (epoch = 8868, MSE loss = 0.8102)\n",
      "Saving model (epoch = 8878, MSE loss = 0.8102)\n",
      "Saving model (epoch = 8888, MSE loss = 0.8101)\n",
      "Saving model (epoch = 8898, MSE loss = 0.8101)\n",
      "Saving model (epoch = 8918, MSE loss = 0.8101)\n",
      "Saving model (epoch = 8928, MSE loss = 0.8101)\n",
      "Saving model (epoch = 8938, MSE loss = 0.8101)\n",
      "Saving model (epoch = 8948, MSE loss = 0.8101)\n",
      "Saving model (epoch = 8958, MSE loss = 0.8100)\n",
      "Saving model (epoch = 8968, MSE loss = 0.8100)\n",
      "Saving model (epoch = 8978, MSE loss = 0.8100)\n",
      "Saving model (epoch = 8988, MSE loss = 0.8100)\n",
      "Saving model (epoch = 8998, MSE loss = 0.8100)\n",
      "Saving model (epoch = 9008, MSE loss = 0.8099)\n",
      "Saving model (epoch = 9018, MSE loss = 0.8099)\n",
      "Saving model (epoch = 9028, MSE loss = 0.8099)\n",
      "Saving model (epoch = 9048, MSE loss = 0.8099)\n",
      "Saving model (epoch = 9068, MSE loss = 0.8099)\n",
      "Saving model (epoch = 9078, MSE loss = 0.8099)\n",
      "Saving model (epoch = 9088, MSE loss = 0.8098)\n",
      "Saving model (epoch = 9098, MSE loss = 0.8098)\n",
      "Saving model (epoch = 9108, MSE loss = 0.8098)\n",
      "Saving model (epoch = 9128, MSE loss = 0.8098)\n",
      "Saving model (epoch = 9138, MSE loss = 0.8098)\n",
      "Saving model (epoch = 9148, MSE loss = 0.8098)\n",
      "Saving model (epoch = 9158, MSE loss = 0.8098)\n",
      "Saving model (epoch = 9178, MSE loss = 0.8097)\n",
      "Saving model (epoch = 9188, MSE loss = 0.8097)\n",
      "Saving model (epoch = 9198, MSE loss = 0.8097)\n",
      "Saving model (epoch = 9208, MSE loss = 0.8097)\n",
      "Saving model (epoch = 9218, MSE loss = 0.8097)\n",
      "Saving model (epoch = 9238, MSE loss = 0.8097)\n",
      "Saving model (epoch = 9248, MSE loss = 0.8097)\n",
      "Saving model (epoch = 9268, MSE loss = 0.8097)\n",
      "Saving model (epoch = 9288, MSE loss = 0.8096)\n",
      "Saving model (epoch = 9298, MSE loss = 0.8096)\n",
      "Saving model (epoch = 9308, MSE loss = 0.8096)\n",
      "Saving model (epoch = 9328, MSE loss = 0.8096)\n",
      "Saving model (epoch = 9348, MSE loss = 0.8096)\n",
      "Saving model (epoch = 9368, MSE loss = 0.8096)\n",
      "Saving model (epoch = 9388, MSE loss = 0.8096)\n",
      "Saving model (epoch = 9408, MSE loss = 0.8095)\n",
      "Saving model (epoch = 9428, MSE loss = 0.8095)\n",
      "Saving model (epoch = 9448, MSE loss = 0.8095)\n",
      "Saving model (epoch = 9468, MSE loss = 0.8095)\n",
      "Saving model (epoch = 9488, MSE loss = 0.8095)\n",
      "Saving model (epoch = 9508, MSE loss = 0.8094)\n",
      "Saving model (epoch = 9528, MSE loss = 0.8094)\n",
      "Saving model (epoch = 9538, MSE loss = 0.8094)\n",
      "Saving model (epoch = 9548, MSE loss = 0.8094)\n",
      "Saving model (epoch = 9568, MSE loss = 0.8094)\n",
      "Saving model (epoch = 9578, MSE loss = 0.8094)\n",
      "Saving model (epoch = 9588, MSE loss = 0.8094)\n",
      "Saving model (epoch = 9608, MSE loss = 0.8093)\n",
      "Saving model (epoch = 9628, MSE loss = 0.8093)\n",
      "Saving model (epoch = 9648, MSE loss = 0.8093)\n",
      "Saving model (epoch = 9658, MSE loss = 0.8093)\n",
      "Saving model (epoch = 9678, MSE loss = 0.8093)\n",
      "Saving model (epoch = 9728, MSE loss = 0.8093)\n",
      "Saving model (epoch = 9748, MSE loss = 0.8093)\n",
      "Saving model (epoch = 9778, MSE loss = 0.8092)\n",
      "Saving model (epoch = 9788, MSE loss = 0.8092)\n",
      "Saving model (epoch = 9798, MSE loss = 0.8092)\n",
      "Saving model (epoch = 9808, MSE loss = 0.8092)\n",
      "Saving model (epoch = 9818, MSE loss = 0.8092)\n",
      "Saving model (epoch = 9828, MSE loss = 0.8092)\n",
      "Saving model (epoch = 9838, MSE loss = 0.8092)\n",
      "Saving model (epoch = 9848, MSE loss = 0.8092)\n",
      "Saving model (epoch = 9888, MSE loss = 0.8092)\n",
      "Saving model (epoch = 9908, MSE loss = 0.8092)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs+1):\n",
    "  # Split into train/test\n",
    "  x_train, y_train, x_test, y_test = train_test_split(x, y, test_size=0.3, random_state=epoch%10) # random_state=epoch%10:K-Flod效果\n",
    "  \n",
    "  train(epoch)\n",
    "  test(epoch)\n",
    "  if ES_counter == ES_patience:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e77375e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-25T04:23:37.645451Z",
     "iopub.status.busy": "2022-09-25T04:23:37.644985Z",
     "iopub.status.idle": "2022-09-25T04:23:37.968892Z",
     "shell.execute_reply": "2022-09-25T04:23:37.967665Z"
    },
    "papermill": {
     "duration": 0.382748,
     "end_time": "2022-09-25T04:23:37.971964",
     "exception": false,
     "start_time": "2022-09-25T04:23:37.589216",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(range(len(train_losses)), train_losses, color='red')\n",
    "plt.plot(range(len(train_losses)), test_losses, color='blue')\n",
    "plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77863663",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-25T04:23:38.076739Z",
     "iopub.status.busy": "2022-09-25T04:23:38.076351Z",
     "iopub.status.idle": "2022-09-25T04:23:38.088284Z",
     "shell.execute_reply": "2022-09-25T04:23:38.086919Z"
    },
    "papermill": {
     "duration": 0.066559,
     "end_time": "2022-09-25T04:23:38.091099",
     "exception": false,
     "start_time": "2022-09-25T04:23:38.024540",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_network = Net().cuda()\n",
    "network_state_dict = torch.load('/kaggle/model.pth')\n",
    "best_network.load_state_dict(network_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "307aa9c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-25T04:23:38.199761Z",
     "iopub.status.busy": "2022-09-25T04:23:38.198026Z",
     "iopub.status.idle": "2022-09-25T04:23:38.216947Z",
     "shell.execute_reply": "2022-09-25T04:23:38.215729Z"
    },
    "papermill": {
     "duration": 0.074887,
     "end_time": "2022-09-25T04:23:38.219737",
     "exception": false,
     "start_time": "2022-09-25T04:23:38.144850",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_network.eval()\n",
    "\n",
    "test_df = test_df.iloc[:,cols]\n",
    "\n",
    "# Standardization Feature\n",
    "for col in test_df.columns:\n",
    "  test_df[col] = (test_df[col] - mean_std[col][0]) / mean_std[col][1]\n",
    "\n",
    "test_X = test_df.values #Pandas to Numpy\n",
    "test_X = torch.Tensor(test_X).cuda() #Numpy to Tensor\n",
    "pred = best_network(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f18efd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-25T04:23:38.325639Z",
     "iopub.status.busy": "2022-09-25T04:23:38.325288Z",
     "iopub.status.idle": "2022-09-25T04:23:38.349580Z",
     "shell.execute_reply": "2022-09-25T04:23:38.347573Z"
    },
    "papermill": {
     "duration": 0.081327,
     "end_time": "2022-09-25T04:23:38.353179",
     "exception": false,
     "start_time": "2022-09-25T04:23:38.271852",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "print('Saving results')\n",
    "with open('/kaggle/working/submission.csv', \"w\") as fp:\n",
    "    writer = csv.writer(fp)\n",
    "    writer.writerow(['id', 'tested_positive'])\n",
    "    for i, p in enumerate(pred):\n",
    "        writer.writerow([i, p.item()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 303.889662,
   "end_time": "2022-09-25T04:23:39.930697",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-09-25T04:18:36.041035",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
