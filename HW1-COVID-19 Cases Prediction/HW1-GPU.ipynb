{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":234.222995,"end_time":"2022-09-24T17:14:50.778699","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2022-09-24T17:10:56.555704","version":"2.3.4"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### ---My Best Score : 0.89125---\n### Strong baseline : 0.89266\n### Medium baseline : 1.28359\n### Simple baseline : 2.03004","metadata":{}},{"cell_type":"code","source":"# PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# For data preprocess\nimport numpy as np\nimport pandas as pd\nimport csv\nimport math\n\n# For plotting\nimport matplotlib.pyplot as plt\n\nmyseed = 15 # set a random seed\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nnp.random.seed(myseed)\ntorch.manual_seed(myseed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(myseed)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-09-24T17:11:04.370499Z","iopub.status.busy":"2022-09-24T17:11:04.369620Z","iopub.status.idle":"2022-09-24T17:11:06.128951Z","shell.execute_reply":"2022-09-24T17:11:06.127624Z"},"papermill":{"duration":1.772579,"end_time":"2022-09-24T17:11:06.132038","exception":false,"start_time":"2022-09-24T17:11:04.359459","status":"completed"},"tags":[]},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n#import numpy as np\n\ndf = pd.read_csv('/kaggle/input/ml2021spring-hw1/covid.train.csv')\ntest_df = pd.read_csv('/kaggle/input/ml2021spring-hw1/covid.test.csv')\ncombine_df = pd.concat([df, test_df])\n\nx = df[df.columns[1:94]]\ny = df[df.columns[94]]\n\ndf.head()","metadata":{"execution":{"iopub.execute_input":"2022-09-24T17:11:06.147052Z","iopub.status.busy":"2022-09-24T17:11:06.146004Z","iopub.status.idle":"2022-09-24T17:11:06.352464Z","shell.execute_reply":"2022-09-24T17:11:06.351406Z"},"papermill":{"duration":0.217679,"end_time":"2022-09-24T17:11:06.355382","exception":false,"start_time":"2022-09-24T17:11:06.137703","status":"completed"},"tags":[]},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>AL</th>\n","      <th>AK</th>\n","      <th>AZ</th>\n","      <th>AR</th>\n","      <th>CA</th>\n","      <th>CO</th>\n","      <th>CT</th>\n","      <th>FL</th>\n","      <th>GA</th>\n","      <th>...</th>\n","      <th>restaurant.2</th>\n","      <th>spent_time.2</th>\n","      <th>large_event.2</th>\n","      <th>public_transit.2</th>\n","      <th>anxious.2</th>\n","      <th>depressed.2</th>\n","      <th>felt_isolated.2</th>\n","      <th>worried_become_ill.2</th>\n","      <th>worried_finances.2</th>\n","      <th>tested_positive.2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>23.812411</td>\n","      <td>43.430423</td>\n","      <td>16.151527</td>\n","      <td>1.602635</td>\n","      <td>15.409449</td>\n","      <td>12.088688</td>\n","      <td>16.702086</td>\n","      <td>53.991549</td>\n","      <td>43.604229</td>\n","      <td>20.704935</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>23.682974</td>\n","      <td>43.196313</td>\n","      <td>16.123386</td>\n","      <td>1.641863</td>\n","      <td>15.230063</td>\n","      <td>11.809047</td>\n","      <td>16.506973</td>\n","      <td>54.185521</td>\n","      <td>42.665766</td>\n","      <td>21.292911</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>23.593983</td>\n","      <td>43.362200</td>\n","      <td>16.159971</td>\n","      <td>1.677523</td>\n","      <td>15.717207</td>\n","      <td>12.355918</td>\n","      <td>16.273294</td>\n","      <td>53.637069</td>\n","      <td>42.972417</td>\n","      <td>21.166656</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>22.576992</td>\n","      <td>42.954574</td>\n","      <td>15.544373</td>\n","      <td>1.578030</td>\n","      <td>15.295650</td>\n","      <td>12.218123</td>\n","      <td>16.045504</td>\n","      <td>52.446223</td>\n","      <td>42.907472</td>\n","      <td>19.896607</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>22.091433</td>\n","      <td>43.290957</td>\n","      <td>15.214655</td>\n","      <td>1.641667</td>\n","      <td>14.778802</td>\n","      <td>12.417256</td>\n","      <td>16.134238</td>\n","      <td>52.560315</td>\n","      <td>43.321985</td>\n","      <td>20.178428</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 95 columns</p>\n","</div>"],"text/plain":["   id   AL   AK   AZ   AR   CA   CO   CT   FL   GA  ...  restaurant.2  \\\n","0   0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...     23.812411   \n","1   1  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...     23.682974   \n","2   2  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...     23.593983   \n","3   3  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...     22.576992   \n","4   4  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...     22.091433   \n","\n","   spent_time.2  large_event.2  public_transit.2  anxious.2  depressed.2  \\\n","0     43.430423      16.151527          1.602635  15.409449    12.088688   \n","1     43.196313      16.123386          1.641863  15.230063    11.809047   \n","2     43.362200      16.159971          1.677523  15.717207    12.355918   \n","3     42.954574      15.544373          1.578030  15.295650    12.218123   \n","4     43.290957      15.214655          1.641667  14.778802    12.417256   \n","\n","   felt_isolated.2  worried_become_ill.2  worried_finances.2  \\\n","0        16.702086             53.991549           43.604229   \n","1        16.506973             54.185521           42.665766   \n","2        16.273294             53.637069           42.972417   \n","3        16.045504             52.446223           42.907472   \n","4        16.134238             52.560315           43.321985   \n","\n","   tested_positive.2  \n","0          20.704935  \n","1          21.292911  \n","2          21.166656  \n","3          19.896607  \n","4          20.178428  \n","\n","[5 rows x 95 columns]"]},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression\nfrom sklearn.model_selection import train_test_split\n\nbestfeatures = SelectKBest(score_func=f_regression, k=14)\nfit = bestfeatures.fit(x,y)\ncols = bestfeatures.get_support(indices=True)\n\ndf = df.iloc[:,cols]\n\ndf.head()","metadata":{"execution":{"iopub.execute_input":"2022-09-24T17:11:06.371923Z","iopub.status.busy":"2022-09-24T17:11:06.371465Z","iopub.status.idle":"2022-09-24T17:11:07.327955Z","shell.execute_reply":"2022-09-24T17:11:07.327041Z"},"papermill":{"duration":0.967847,"end_time":"2022-09-24T17:11:07.331471","exception":false,"start_time":"2022-09-24T17:11:06.363624","status":"completed"},"tags":[]},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>WI</th>\n","      <th>cli</th>\n","      <th>ili</th>\n","      <th>hh_cmnty_cli</th>\n","      <th>worried_finances</th>\n","      <th>tested_positive</th>\n","      <th>cli.1</th>\n","      <th>ili.1</th>\n","      <th>hh_cmnty_cli.1</th>\n","      <th>worried_finances.1</th>\n","      <th>tested_positive.1</th>\n","      <th>cli.2</th>\n","      <th>ili.2</th>\n","      <th>hh_cmnty_cli.2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0</td>\n","      <td>0.814610</td>\n","      <td>0.771356</td>\n","      <td>25.648907</td>\n","      <td>43.279629</td>\n","      <td>19.586492</td>\n","      <td>0.838995</td>\n","      <td>0.807767</td>\n","      <td>25.679101</td>\n","      <td>43.622728</td>\n","      <td>20.151838</td>\n","      <td>0.897802</td>\n","      <td>0.887893</td>\n","      <td>26.060544</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.0</td>\n","      <td>0.838995</td>\n","      <td>0.807767</td>\n","      <td>25.679101</td>\n","      <td>43.622728</td>\n","      <td>20.151838</td>\n","      <td>0.897802</td>\n","      <td>0.887893</td>\n","      <td>26.060544</td>\n","      <td>43.604229</td>\n","      <td>20.704935</td>\n","      <td>0.972842</td>\n","      <td>0.965496</td>\n","      <td>25.754087</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.0</td>\n","      <td>0.897802</td>\n","      <td>0.887893</td>\n","      <td>26.060544</td>\n","      <td>43.604229</td>\n","      <td>20.704935</td>\n","      <td>0.972842</td>\n","      <td>0.965496</td>\n","      <td>25.754087</td>\n","      <td>42.665766</td>\n","      <td>21.292911</td>\n","      <td>0.955306</td>\n","      <td>0.963079</td>\n","      <td>25.947015</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.0</td>\n","      <td>0.972842</td>\n","      <td>0.965496</td>\n","      <td>25.754087</td>\n","      <td>42.665766</td>\n","      <td>21.292911</td>\n","      <td>0.955306</td>\n","      <td>0.963079</td>\n","      <td>25.947015</td>\n","      <td>42.972417</td>\n","      <td>21.166656</td>\n","      <td>0.947513</td>\n","      <td>0.968764</td>\n","      <td>26.350501</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.0</td>\n","      <td>0.955306</td>\n","      <td>0.963079</td>\n","      <td>25.947015</td>\n","      <td>42.972417</td>\n","      <td>21.166656</td>\n","      <td>0.947513</td>\n","      <td>0.968764</td>\n","      <td>26.350501</td>\n","      <td>42.907472</td>\n","      <td>19.896607</td>\n","      <td>0.883833</td>\n","      <td>0.893020</td>\n","      <td>26.480624</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    WI       cli       ili  hh_cmnty_cli  worried_finances  tested_positive  \\\n","0  0.0  0.814610  0.771356     25.648907         43.279629        19.586492   \n","1  0.0  0.838995  0.807767     25.679101         43.622728        20.151838   \n","2  0.0  0.897802  0.887893     26.060544         43.604229        20.704935   \n","3  0.0  0.972842  0.965496     25.754087         42.665766        21.292911   \n","4  0.0  0.955306  0.963079     25.947015         42.972417        21.166656   \n","\n","      cli.1     ili.1  hh_cmnty_cli.1  worried_finances.1  tested_positive.1  \\\n","0  0.838995  0.807767       25.679101           43.622728          20.151838   \n","1  0.897802  0.887893       26.060544           43.604229          20.704935   \n","2  0.972842  0.965496       25.754087           42.665766          21.292911   \n","3  0.955306  0.963079       25.947015           42.972417          21.166656   \n","4  0.947513  0.968764       26.350501           42.907472          19.896607   \n","\n","      cli.2     ili.2  hh_cmnty_cli.2  \n","0  0.897802  0.887893       26.060544  \n","1  0.972842  0.965496       25.754087  \n","2  0.955306  0.963079       25.947015  \n","3  0.947513  0.968764       26.350501  \n","4  0.883833  0.893020       26.480624  "]},"metadata":{}}]},{"cell_type":"code","source":"dfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(x.columns)\n\nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score'] \n\nprint(featureScores.nlargest(15,'Score')) # Print 15 best features","metadata":{"execution":{"iopub.execute_input":"2022-09-24T17:11:07.358010Z","iopub.status.busy":"2022-09-24T17:11:07.357330Z","iopub.status.idle":"2022-09-24T17:11:07.375782Z","shell.execute_reply":"2022-09-24T17:11:07.374803Z"},"papermill":{"duration":0.038415,"end_time":"2022-09-24T17:11:07.382294","exception":false,"start_time":"2022-09-24T17:11:07.343879","status":"completed"},"tags":[]},"execution_count":4,"outputs":[{"name":"stdout","output_type":"stream","text":"                 Specs          Score\n\n75   tested_positive.1  148069.658278\n\n57     tested_positive   69603.872591\n\n42        hh_cmnty_cli    9235.492094\n\n60      hh_cmnty_cli.1    9209.019558\n\n78      hh_cmnty_cli.2    9097.375172\n\n43      nohh_cmnty_cli    8395.421300\n\n61    nohh_cmnty_cli.1    8343.255927\n\n79    nohh_cmnty_cli.2    8208.176435\n\n40                 cli    6388.906849\n\n58               cli.1    6374.548000\n\n76               cli.2    6250.008702\n\n41                 ili    5998.922880\n\n59               ili.1    5937.588576\n\n77               ili.2    5796.947672\n\n92  worried_finances.2     833.613191\n"}]},{"cell_type":"code","source":"mean_std={}\n\n# Standardization Feature\nfor col in df.columns:\n  mean_std[col] = (combine_df[col].mean(), combine_df[col].std())\n  df[col] = (df[col] - mean_std[col][0]) / mean_std[col][1]\n\n# Pandas to Numpy\nx = df[df.columns].values\ny = y.values\n\n# Numpy to Tensor\nx = torch.Tensor(x).cuda()\ny = torch.Tensor(y).cuda()","metadata":{"execution":{"iopub.execute_input":"2022-09-24T17:11:07.408126Z","iopub.status.busy":"2022-09-24T17:11:07.407598Z","iopub.status.idle":"2022-09-24T17:11:10.347371Z","shell.execute_reply":"2022-09-24T17:11:10.346405Z"},"papermill":{"duration":2.955172,"end_time":"2022-09-24T17:11:10.349716","exception":false,"start_time":"2022-09-24T17:11:07.394544","status":"completed"},"tags":[]},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.execute_input":"2022-09-24T17:11:10.361065Z","iopub.status.busy":"2022-09-24T17:11:10.360754Z","iopub.status.idle":"2022-09-24T17:11:10.380197Z","shell.execute_reply":"2022-09-24T17:11:10.379320Z"},"papermill":{"duration":0.028034,"end_time":"2022-09-24T17:11:10.382670","exception":false,"start_time":"2022-09-24T17:11:10.354636","status":"completed"},"tags":[]},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>WI</th>\n","      <th>cli</th>\n","      <th>ili</th>\n","      <th>hh_cmnty_cli</th>\n","      <th>worried_finances</th>\n","      <th>tested_positive</th>\n","      <th>cli.1</th>\n","      <th>ili.1</th>\n","      <th>hh_cmnty_cli.1</th>\n","      <th>worried_finances.1</th>\n","      <th>tested_positive.1</th>\n","      <th>cli.2</th>\n","      <th>ili.2</th>\n","      <th>hh_cmnty_cli.2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-0.160266</td>\n","      <td>-0.411747</td>\n","      <td>-0.566136</td>\n","      <td>-0.401512</td>\n","      <td>-0.236095</td>\n","      <td>0.438190</td>\n","      <td>-0.361583</td>\n","      <td>-0.487464</td>\n","      <td>-0.407982</td>\n","      <td>-0.173065</td>\n","      <td>0.505859</td>\n","      <td>-0.229376</td>\n","      <td>-0.305425</td>\n","      <td>-0.375758</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-0.160266</td>\n","      <td>-0.353448</td>\n","      <td>-0.479792</td>\n","      <td>-0.398237</td>\n","      <td>-0.168869</td>\n","      <td>0.511783</td>\n","      <td>-0.221076</td>\n","      <td>-0.297780</td>\n","      <td>-0.366502</td>\n","      <td>-0.176694</td>\n","      <td>0.577989</td>\n","      <td>-0.050064</td>\n","      <td>-0.121692</td>\n","      <td>-0.409173</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-0.160266</td>\n","      <td>-0.212855</td>\n","      <td>-0.289779</td>\n","      <td>-0.356871</td>\n","      <td>-0.172493</td>\n","      <td>0.583781</td>\n","      <td>-0.041779</td>\n","      <td>-0.114071</td>\n","      <td>-0.399828</td>\n","      <td>-0.360786</td>\n","      <td>0.654668</td>\n","      <td>-0.091968</td>\n","      <td>-0.127415</td>\n","      <td>-0.388136</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>-0.160266</td>\n","      <td>-0.033449</td>\n","      <td>-0.105750</td>\n","      <td>-0.390105</td>\n","      <td>-0.356376</td>\n","      <td>0.660320</td>\n","      <td>-0.083680</td>\n","      <td>-0.119793</td>\n","      <td>-0.378848</td>\n","      <td>-0.300632</td>\n","      <td>0.638203</td>\n","      <td>-0.110588</td>\n","      <td>-0.113955</td>\n","      <td>-0.344142</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-0.160266</td>\n","      <td>-0.075375</td>\n","      <td>-0.111482</td>\n","      <td>-0.369183</td>\n","      <td>-0.296291</td>\n","      <td>0.643885</td>\n","      <td>-0.102298</td>\n","      <td>-0.106335</td>\n","      <td>-0.334971</td>\n","      <td>-0.313372</td>\n","      <td>0.472574</td>\n","      <td>-0.262754</td>\n","      <td>-0.293286</td>\n","      <td>-0.329954</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         WI       cli       ili  hh_cmnty_cli  worried_finances  \\\n","0 -0.160266 -0.411747 -0.566136     -0.401512         -0.236095   \n","1 -0.160266 -0.353448 -0.479792     -0.398237         -0.168869   \n","2 -0.160266 -0.212855 -0.289779     -0.356871         -0.172493   \n","3 -0.160266 -0.033449 -0.105750     -0.390105         -0.356376   \n","4 -0.160266 -0.075375 -0.111482     -0.369183         -0.296291   \n","\n","   tested_positive     cli.1     ili.1  hh_cmnty_cli.1  worried_finances.1  \\\n","0         0.438190 -0.361583 -0.487464       -0.407982           -0.173065   \n","1         0.511783 -0.221076 -0.297780       -0.366502           -0.176694   \n","2         0.583781 -0.041779 -0.114071       -0.399828           -0.360786   \n","3         0.660320 -0.083680 -0.119793       -0.378848           -0.300632   \n","4         0.643885 -0.102298 -0.106335       -0.334971           -0.313372   \n","\n","   tested_positive.1     cli.2     ili.2  hh_cmnty_cli.2  \n","0           0.505859 -0.229376 -0.305425       -0.375758  \n","1           0.577989 -0.050064 -0.121692       -0.409173  \n","2           0.654668 -0.091968 -0.127415       -0.388136  \n","3           0.638203 -0.110588 -0.113955       -0.344142  \n","4           0.472574 -0.262754 -0.293286       -0.329954  "]},"metadata":{}}]},{"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(14, 32)\n        #self.bn1 = nn.BatchNorm1d(32) # 使用BatchNorm1d效果不好\n        #self.dropout1 = nn.Dropout(0.05) # 使用Dropout效果不好\n        self.fc2 = nn.Linear(32, 1)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        #x = self.bn1(x)\n        #x = self.dropout1(x)\n        x = F.relu(x)\n        x = self.fc2(x)\n        return x","metadata":{"execution":{"iopub.execute_input":"2022-09-24T17:11:10.394178Z","iopub.status.busy":"2022-09-24T17:11:10.393902Z","iopub.status.idle":"2022-09-24T17:11:10.400552Z","shell.execute_reply":"2022-09-24T17:11:10.399725Z"},"papermill":{"duration":0.014761,"end_time":"2022-09-24T17:11:10.402453","exception":false,"start_time":"2022-09-24T17:11:10.387692","status":"completed"},"tags":[]},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"network = Net()\nnetwork.cuda()\noptimizer = torch.optim.RAdam(network.parameters(), lr=7.5e-5, weight_decay=7e-3) # 使用RAdam效果比Adam更好","metadata":{"execution":{"iopub.execute_input":"2022-09-24T17:11:10.413580Z","iopub.status.busy":"2022-09-24T17:11:10.412738Z","iopub.status.idle":"2022-09-24T17:11:10.421387Z","shell.execute_reply":"2022-09-24T17:11:10.420585Z"},"papermill":{"duration":0.016173,"end_time":"2022-09-24T17:11:10.423273","exception":false,"start_time":"2022-09-24T17:11:10.407100","status":"completed"},"tags":[]},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"epochs = 10000\nbatch_size = 128\n\ntrain_losses = []\ntest_losses = []\n\n# Early Stopping parameters\nES_patience = 500\nES_counter = 0 \nbest_epoch = 0\nbest_loss = 1000","metadata":{"execution":{"iopub.execute_input":"2022-09-24T17:11:10.434327Z","iopub.status.busy":"2022-09-24T17:11:10.433416Z","iopub.status.idle":"2022-09-24T17:11:10.440515Z","shell.execute_reply":"2022-09-24T17:11:10.439659Z"},"papermill":{"duration":0.014441,"end_time":"2022-09-24T17:11:10.442449","exception":false,"start_time":"2022-09-24T17:11:10.428008","status":"completed"},"tags":[]},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def train(epoch):\n  network.train()\n  for i in range(len(x_train)//batch_size):\n    optimizer.zero_grad()\n    pred = network(x_train[batch_size*i:batch_size*(i+1)])\n    loss = F.mse_loss(pred.view(-1), x_test[batch_size*i:batch_size*(i+1)])\n    loss.backward()\n    optimizer.step()\n\n  train_losses.append(loss.item())","metadata":{"execution":{"iopub.execute_input":"2022-09-24T17:11:10.453323Z","iopub.status.busy":"2022-09-24T17:11:10.452595Z","iopub.status.idle":"2022-09-24T17:11:10.458676Z","shell.execute_reply":"2022-09-24T17:11:10.457749Z"},"papermill":{"duration":0.013429,"end_time":"2022-09-24T17:11:10.460608","exception":false,"start_time":"2022-09-24T17:11:10.447179","status":"completed"},"tags":[]},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def test(epoch):\n  global best_epoch, best_loss, ES_counter\n\n  network.eval()\n  with torch.no_grad():\n    pred = network(y_train)\n    loss = F.mse_loss(pred.view(-1), y_test)\n    \n  # Early Stopping\n  if best_loss > loss:\n    ES_counter, best_epoch, best_loss = 0, epoch, loss\n    # Save best model\n    torch.save(network.state_dict(), '/kaggle/model.pth')\n    print('Saving model (epoch = {:4d}, MSE loss = {:.4f})'.format(epoch, loss))\n  else:\n    ES_counter += 1\n    if ES_counter == ES_patience:\n      print('---Early Stopping--- (Best epoch = {:4d}, Best MSE loss = {:.4f})'.format(best_epoch, best_loss))\n  # Early Stopping\n\n  test_losses.append(loss.item())","metadata":{"execution":{"iopub.execute_input":"2022-09-24T17:11:10.471954Z","iopub.status.busy":"2022-09-24T17:11:10.470583Z","iopub.status.idle":"2022-09-24T17:11:10.477532Z","shell.execute_reply":"2022-09-24T17:11:10.476715Z"},"papermill":{"duration":0.0142,"end_time":"2022-09-24T17:11:10.479423","exception":false,"start_time":"2022-09-24T17:11:10.465223","status":"completed"},"tags":[]},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"for epoch in range(1, epochs+1):\n  # Split into train/test\n  x_train, y_train, x_test, y_test = train_test_split(x, y, test_size=0.3, random_state=epoch%10) # random_state=epoch%10:K-Flod效果\n  \n  train(epoch)\n  test(epoch)\n  if ES_counter == ES_patience:\n    break","metadata":{"execution":{"iopub.execute_input":"2022-09-24T17:11:10.506427Z","iopub.status.busy":"2022-09-24T17:11:10.505660Z","iopub.status.idle":"2022-09-24T17:14:48.786042Z","shell.execute_reply":"2022-09-24T17:14:48.782642Z"},"papermill":{"duration":218.317796,"end_time":"2022-09-24T17:14:48.818073","exception":false,"start_time":"2022-09-24T17:11:10.500277","status":"completed"},"tags":[]},"execution_count":13,"outputs":[{"name":"stdout","output_type":"stream","text":"Saving model (epoch =    1, MSE loss = 317.7927)\n\nSaving model (epoch =    6, MSE loss = 315.9405)\n\nSaving model (epoch =   16, MSE loss = 313.8748)\n\nSaving model (epoch =   20, MSE loss = 313.6397)\n\nSaving model (epoch =   26, MSE loss = 310.8149)\n\nSaving model (epoch =   30, MSE loss = 310.4000)\n\nSaving model (epoch =   36, MSE loss = 306.8941)\n\nSaving model (epoch =   40, MSE loss = 306.3477)\n\nSaving model (epoch =   41, MSE loss = 306.1722)\n\nSaving model (epoch =   46, MSE loss = 302.0892)\n\nSaving model (epoch =   50, MSE loss = 301.4042)\n\nSaving model (epoch =   51, MSE loss = 300.9808)\n\nSaving model (epoch =   56, MSE loss = 296.2608)\n\nSaving model (epoch =   60, MSE loss = 295.4333)\n\nSaving model (epoch =   61, MSE loss = 294.7320)\n\nSaving model (epoch =   66, MSE loss = 289.3549)\n\nSaving model (epoch =   70, MSE loss = 288.4584)\n\nSaving model (epoch =   71, MSE loss = 287.4515)\n\nSaving model (epoch =   74, MSE loss = 287.1920)\n\nSaving model (epoch =   76, MSE loss = 281.4252)\n\nSaving model (epoch =   80, MSE loss = 280.5255)\n\nSaving model (epoch =   81, MSE loss = 279.1893)\n\nSaving model (epoch =   84, MSE loss = 278.1521)\n\nSaving model (epoch =   86, MSE loss = 272.4867)\n\nSaving model (epoch =   90, MSE loss = 271.6357)\n\nSaving model (epoch =   91, MSE loss = 269.9362)\n\nSaving model (epoch =   94, MSE loss = 268.0659)\n\nSaving model (epoch =   96, MSE loss = 262.5493)\n\nSaving model (epoch =  100, MSE loss = 261.8000)\n\nSaving model (epoch =  101, MSE loss = 259.7058)\n\nSaving model (epoch =  104, MSE loss = 256.9775)\n\nSaving model (epoch =  106, MSE loss = 251.6584)\n\nSaving model (epoch =  110, MSE loss = 251.0676)\n\nSaving model (epoch =  111, MSE loss = 248.5660)\n\nSaving model (epoch =  114, MSE loss = 244.9724)\n\nSaving model (epoch =  116, MSE loss = 239.9053)\n\nSaving model (epoch =  120, MSE loss = 239.5535)\n\nSaving model (epoch =  121, MSE loss = 236.6411)\n\nSaving model (epoch =  124, MSE loss = 232.2076)\n\nSaving model (epoch =  126, MSE loss = 227.4531)\n\nSaving model (epoch =  130, MSE loss = 227.4352)\n\nSaving model (epoch =  131, MSE loss = 224.1214)\n\nSaving model (epoch =  134, MSE loss = 218.8979)\n\nSaving model (epoch =  136, MSE loss = 214.5111)\n\nSaving model (epoch =  141, MSE loss = 211.2235)\n\nSaving model (epoch =  144, MSE loss = 205.2750)\n\nSaving model (epoch =  146, MSE loss = 201.3008)\n\nSaving model (epoch =  151, MSE loss = 198.1621)\n\nSaving model (epoch =  154, MSE loss = 191.5704)\n\nSaving model (epoch =  156, MSE loss = 188.0395)\n\nSaving model (epoch =  161, MSE loss = 185.1502)\n\nSaving model (epoch =  164, MSE loss = 178.0059)\n\nSaving model (epoch =  166, MSE loss = 174.9398)\n\nSaving model (epoch =  171, MSE loss = 172.3934)\n\nSaving model (epoch =  174, MSE loss = 164.7942)\n\nSaving model (epoch =  176, MSE loss = 162.2070)\n\nSaving model (epoch =  181, MSE loss = 160.0875)\n\nSaving model (epoch =  184, MSE loss = 152.1418)\n\nSaving model (epoch =  186, MSE loss = 150.0374)\n\nSaving model (epoch =  191, MSE loss = 148.4180)\n\nSaving model (epoch =  194, MSE loss = 140.2413)\n\nSaving model (epoch =  196, MSE loss = 138.6138)\n\nSaving model (epoch =  201, MSE loss = 137.5557)\n\nSaving model (epoch =  204, MSE loss = 129.2668)\n\nSaving model (epoch =  206, MSE loss = 128.0951)\n\nSaving model (epoch =  211, MSE loss = 127.6400)\n\nSaving model (epoch =  214, MSE loss = 119.3490)\n\nSaving model (epoch =  216, MSE loss = 118.6025)\n\nSaving model (epoch =  224, MSE loss = 110.5781)\n\nSaving model (epoch =  226, MSE loss = 110.2175)\n\nSaving model (epoch =  234, MSE loss = 102.9934)\n\nSaving model (epoch =  236, MSE loss = 102.9698)\n\nSaving model (epoch =  244, MSE loss = 96.5833)\n\nSaving model (epoch =  254, MSE loss = 91.2871)\n\nSaving model (epoch =  264, MSE loss = 87.0007)\n\nSaving model (epoch =  274, MSE loss = 83.5877)\n\nSaving model (epoch =  284, MSE loss = 80.8900)\n\nSaving model (epoch =  294, MSE loss = 78.7396)\n\nSaving model (epoch =  304, MSE loss = 76.9863)\n\nSaving model (epoch =  314, MSE loss = 75.4974)\n\nSaving model (epoch =  324, MSE loss = 74.1693)\n\nSaving model (epoch =  334, MSE loss = 72.9243)\n\nSaving model (epoch =  344, MSE loss = 71.7096)\n\nSaving model (epoch =  354, MSE loss = 70.4917)\n\nSaving model (epoch =  364, MSE loss = 69.2451)\n\nSaving model (epoch =  374, MSE loss = 67.9566)\n\nSaving model (epoch =  384, MSE loss = 66.6186)\n\nSaving model (epoch =  394, MSE loss = 65.2299)\n\nSaving model (epoch =  404, MSE loss = 63.7890)\n\nSaving model (epoch =  414, MSE loss = 62.2922)\n\nSaving model (epoch =  424, MSE loss = 60.7444)\n\nSaving model (epoch =  434, MSE loss = 59.1450)\n\nSaving model (epoch =  444, MSE loss = 57.4931)\n\nSaving model (epoch =  448, MSE loss = 57.4529)\n\nSaving model (epoch =  454, MSE loss = 55.7835)\n\nSaving model (epoch =  458, MSE loss = 55.6835)\n\nSaving model (epoch =  464, MSE loss = 54.0177)\n\nSaving model (epoch =  468, MSE loss = 53.8666)\n\nSaving model (epoch =  474, MSE loss = 52.2028)\n\nSaving model (epoch =  478, MSE loss = 52.0053)\n\nSaving model (epoch =  484, MSE loss = 50.3515)\n\nSaving model (epoch =  488, MSE loss = 50.1143)\n\nSaving model (epoch =  494, MSE loss = 48.4671)\n\nSaving model (epoch =  498, MSE loss = 48.1995)\n\nSaving model (epoch =  504, MSE loss = 46.5576)\n\nSaving model (epoch =  508, MSE loss = 46.2657)\n\nSaving model (epoch =  514, MSE loss = 44.6293)\n\nSaving model (epoch =  518, MSE loss = 44.3184)\n\nSaving model (epoch =  524, MSE loss = 42.6905)\n\nSaving model (epoch =  528, MSE loss = 42.3676)\n\nSaving model (epoch =  534, MSE loss = 40.7457)\n\nSaving model (epoch =  538, MSE loss = 40.4168)\n\nSaving model (epoch =  544, MSE loss = 38.7969)\n\nSaving model (epoch =  548, MSE loss = 38.4735)\n\nSaving model (epoch =  554, MSE loss = 36.8581)\n\nSaving model (epoch =  558, MSE loss = 36.5377)\n\nSaving model (epoch =  564, MSE loss = 34.9348)\n\nSaving model (epoch =  568, MSE loss = 34.6235)\n\nSaving model (epoch =  572, MSE loss = 34.5345)\n\nSaving model (epoch =  574, MSE loss = 33.0358)\n\nSaving model (epoch =  578, MSE loss = 32.7385)\n\nSaving model (epoch =  582, MSE loss = 32.5153)\n\nSaving model (epoch =  584, MSE loss = 31.1707)\n\nSaving model (epoch =  588, MSE loss = 30.8873)\n\nSaving model (epoch =  592, MSE loss = 30.5456)\n\nSaving model (epoch =  594, MSE loss = 29.3428)\n\nSaving model (epoch =  598, MSE loss = 29.0714)\n\nSaving model (epoch =  602, MSE loss = 28.6330)\n\nSaving model (epoch =  604, MSE loss = 27.5625)\n\nSaving model (epoch =  608, MSE loss = 27.3043)\n\nSaving model (epoch =  612, MSE loss = 26.7870)\n\nSaving model (epoch =  614, MSE loss = 25.8391)\n\nSaving model (epoch =  618, MSE loss = 25.5908)\n\nSaving model (epoch =  622, MSE loss = 25.0139)\n\nSaving model (epoch =  624, MSE loss = 24.1741)\n\nSaving model (epoch =  628, MSE loss = 23.9334)\n\nSaving model (epoch =  632, MSE loss = 23.3191)\n\nSaving model (epoch =  634, MSE loss = 22.5771)\n\nSaving model (epoch =  638, MSE loss = 22.3449)\n\nSaving model (epoch =  642, MSE loss = 21.7086)\n\nSaving model (epoch =  644, MSE loss = 21.0592)\n\nSaving model (epoch =  648, MSE loss = 20.8317)\n\nSaving model (epoch =  652, MSE loss = 20.1797)\n\nSaving model (epoch =  654, MSE loss = 19.6116)\n\nSaving model (epoch =  658, MSE loss = 19.3951)\n\nSaving model (epoch =  662, MSE loss = 18.7412)\n\nSaving model (epoch =  664, MSE loss = 18.2405)\n\nSaving model (epoch =  668, MSE loss = 18.0359)\n\nSaving model (epoch =  672, MSE loss = 17.3834)\n\nSaving model (epoch =  674, MSE loss = 16.9503)\n\nSaving model (epoch =  678, MSE loss = 16.7467)\n\nSaving model (epoch =  682, MSE loss = 16.1032)\n\nSaving model (epoch =  684, MSE loss = 15.7332)\n\nSaving model (epoch =  688, MSE loss = 15.5273)\n\nSaving model (epoch =  692, MSE loss = 14.9000)\n\nSaving model (epoch =  694, MSE loss = 14.5852)\n\nSaving model (epoch =  698, MSE loss = 14.3884)\n\nSaving model (epoch =  702, MSE loss = 13.7853)\n\nSaving model (epoch =  704, MSE loss = 13.5227)\n\nSaving model (epoch =  708, MSE loss = 13.3441)\n\nSaving model (epoch =  712, MSE loss = 12.7811)\n\nSaving model (epoch =  714, MSE loss = 12.5714)\n\nSaving model (epoch =  718, MSE loss = 12.4006)\n\nSaving model (epoch =  722, MSE loss = 11.8708)\n\nSaving model (epoch =  724, MSE loss = 11.6998)\n\nSaving model (epoch =  728, MSE loss = 11.5276)\n\nSaving model (epoch =  732, MSE loss = 11.0320)\n\nSaving model (epoch =  734, MSE loss = 10.8969)\n\nSaving model (epoch =  738, MSE loss = 10.7187)\n\nSaving model (epoch =  742, MSE loss = 10.2524)\n\nSaving model (epoch =  744, MSE loss = 10.1443)\n\nSaving model (epoch =  748, MSE loss = 9.9531)\n\nSaving model (epoch =  752, MSE loss = 9.5091)\n\nSaving model (epoch =  754, MSE loss = 9.4213)\n\nSaving model (epoch =  758, MSE loss = 9.2244)\n\nSaving model (epoch =  762, MSE loss = 8.8138)\n\nSaving model (epoch =  764, MSE loss = 8.7445)\n\nSaving model (epoch =  768, MSE loss = 8.5414)\n\nSaving model (epoch =  772, MSE loss = 8.1665)\n\nSaving model (epoch =  774, MSE loss = 8.1166)\n\nSaving model (epoch =  778, MSE loss = 7.9066)\n\nSaving model (epoch =  782, MSE loss = 7.5620)\n\nSaving model (epoch =  784, MSE loss = 7.5353)\n\nSaving model (epoch =  788, MSE loss = 7.3111)\n\nSaving model (epoch =  792, MSE loss = 6.9935)\n\nSaving model (epoch =  794, MSE loss = 6.9873)\n\nSaving model (epoch =  798, MSE loss = 6.7492)\n\nSaving model (epoch =  802, MSE loss = 6.4619)\n\nSaving model (epoch =  808, MSE loss = 6.2238)\n\nSaving model (epoch =  812, MSE loss = 5.9649)\n\nSaving model (epoch =  818, MSE loss = 5.7326)\n\nSaving model (epoch =  822, MSE loss = 5.4982)\n\nSaving model (epoch =  826, MSE loss = 5.4861)\n\nSaving model (epoch =  828, MSE loss = 5.2753)\n\nSaving model (epoch =  832, MSE loss = 5.0654)\n\nSaving model (epoch =  836, MSE loss = 5.0307)\n\nSaving model (epoch =  838, MSE loss = 4.8510)\n\nSaving model (epoch =  842, MSE loss = 4.6653)\n\nSaving model (epoch =  846, MSE loss = 4.6090)\n\nSaving model (epoch =  848, MSE loss = 4.4599)\n\nSaving model (epoch =  852, MSE loss = 4.2975)\n\nSaving model (epoch =  856, MSE loss = 4.2246)\n\nSaving model (epoch =  858, MSE loss = 4.1044)\n\nSaving model (epoch =  862, MSE loss = 3.9625)\n\nSaving model (epoch =  866, MSE loss = 3.8772)\n\nSaving model (epoch =  868, MSE loss = 3.7838)\n\nSaving model (epoch =  872, MSE loss = 3.6623)\n\nSaving model (epoch =  876, MSE loss = 3.5672)\n\nSaving model (epoch =  878, MSE loss = 3.4951)\n\nSaving model (epoch =  882, MSE loss = 3.3906)\n\nSaving model (epoch =  886, MSE loss = 3.2866)\n\nSaving model (epoch =  888, MSE loss = 3.2315)\n\nSaving model (epoch =  892, MSE loss = 3.1438)\n\nSaving model (epoch =  896, MSE loss = 3.0299)\n\nSaving model (epoch =  898, MSE loss = 2.9900)\n\nSaving model (epoch =  902, MSE loss = 2.9186)\n\nSaving model (epoch =  906, MSE loss = 2.7978)\n\nSaving model (epoch =  908, MSE loss = 2.7725)\n\nSaving model (epoch =  912, MSE loss = 2.7171)\n\nSaving model (epoch =  916, MSE loss = 2.5914)\n\nSaving model (epoch =  918, MSE loss = 2.5782)\n\nSaving model (epoch =  922, MSE loss = 2.5390)\n\nSaving model (epoch =  926, MSE loss = 2.4080)\n\nSaving model (epoch =  928, MSE loss = 2.4052)\n\nSaving model (epoch =  932, MSE loss = 2.3807)\n\nSaving model (epoch =  936, MSE loss = 2.2477)\n\nSaving model (epoch =  942, MSE loss = 2.2412)\n\nSaving model (epoch =  946, MSE loss = 2.1078)\n\nSaving model (epoch =  956, MSE loss = 1.9850)\n\nSaving model (epoch =  966, MSE loss = 1.8781)\n\nSaving model (epoch =  976, MSE loss = 1.7852)\n\nSaving model (epoch =  986, MSE loss = 1.7049)\n\nSaving model (epoch =  996, MSE loss = 1.6353)\n\nSaving model (epoch = 1006, MSE loss = 1.5739)\n\nSaving model (epoch = 1016, MSE loss = 1.5185)\n\nSaving model (epoch = 1026, MSE loss = 1.4696)\n\nSaving model (epoch = 1036, MSE loss = 1.4273)\n\nSaving model (epoch = 1046, MSE loss = 1.3904)\n\nSaving model (epoch = 1056, MSE loss = 1.3580)\n\nSaving model (epoch = 1066, MSE loss = 1.3290)\n\nSaving model (epoch = 1068, MSE loss = 1.3284)\n\nSaving model (epoch = 1076, MSE loss = 1.3023)\n\nSaving model (epoch = 1078, MSE loss = 1.2971)\n\nSaving model (epoch = 1086, MSE loss = 1.2779)\n\nSaving model (epoch = 1088, MSE loss = 1.2682)\n\nSaving model (epoch = 1096, MSE loss = 1.2555)\n\nSaving model (epoch = 1098, MSE loss = 1.2412)\n\nSaving model (epoch = 1106, MSE loss = 1.2355)\n\nSaving model (epoch = 1108, MSE loss = 1.2167)\n\nSaving model (epoch = 1118, MSE loss = 1.1939)\n\nSaving model (epoch = 1128, MSE loss = 1.1726)\n\nSaving model (epoch = 1138, MSE loss = 1.1531)\n\nSaving model (epoch = 1148, MSE loss = 1.1350)\n\nSaving model (epoch = 1158, MSE loss = 1.1182)\n\nSaving model (epoch = 1168, MSE loss = 1.1024)\n\nSaving model (epoch = 1178, MSE loss = 1.0874)\n\nSaving model (epoch = 1188, MSE loss = 1.0735)\n\nSaving model (epoch = 1198, MSE loss = 1.0605)\n\nSaving model (epoch = 1208, MSE loss = 1.0484)\n\nSaving model (epoch = 1218, MSE loss = 1.0369)\n\nSaving model (epoch = 1228, MSE loss = 1.0264)\n\nSaving model (epoch = 1238, MSE loss = 1.0166)\n\nSaving model (epoch = 1248, MSE loss = 1.0075)\n\nSaving model (epoch = 1258, MSE loss = 0.9989)\n\nSaving model (epoch = 1268, MSE loss = 0.9908)\n\nSaving model (epoch = 1278, MSE loss = 0.9829)\n\nSaving model (epoch = 1288, MSE loss = 0.9755)\n\nSaving model (epoch = 1298, MSE loss = 0.9688)\n\nSaving model (epoch = 1308, MSE loss = 0.9625)\n\nSaving model (epoch = 1318, MSE loss = 0.9565)\n\nSaving model (epoch = 1328, MSE loss = 0.9509)\n\nSaving model (epoch = 1338, MSE loss = 0.9455)\n\nSaving model (epoch = 1348, MSE loss = 0.9403)\n\nSaving model (epoch = 1358, MSE loss = 0.9355)\n\nSaving model (epoch = 1368, MSE loss = 0.9311)\n\nSaving model (epoch = 1378, MSE loss = 0.9269)\n\nSaving model (epoch = 1388, MSE loss = 0.9229)\n\nSaving model (epoch = 1398, MSE loss = 0.9188)\n\nSaving model (epoch = 1408, MSE loss = 0.9149)\n\nSaving model (epoch = 1418, MSE loss = 0.9112)\n\nSaving model (epoch = 1428, MSE loss = 0.9077)\n\nSaving model (epoch = 1438, MSE loss = 0.9045)\n\nSaving model (epoch = 1448, MSE loss = 0.9014)\n\nSaving model (epoch = 1458, MSE loss = 0.8986)\n\nSaving model (epoch = 1468, MSE loss = 0.8958)\n\nSaving model (epoch = 1478, MSE loss = 0.8932)\n\nSaving model (epoch = 1488, MSE loss = 0.8906)\n\nSaving model (epoch = 1498, MSE loss = 0.8881)\n\nSaving model (epoch = 1508, MSE loss = 0.8857)\n\nSaving model (epoch = 1518, MSE loss = 0.8835)\n\nSaving model (epoch = 1528, MSE loss = 0.8815)\n\nSaving model (epoch = 1538, MSE loss = 0.8795)\n\nSaving model (epoch = 1548, MSE loss = 0.8777)\n\nSaving model (epoch = 1558, MSE loss = 0.8759)\n\nSaving model (epoch = 1568, MSE loss = 0.8741)\n\nSaving model (epoch = 1578, MSE loss = 0.8723)\n\nSaving model (epoch = 1588, MSE loss = 0.8707)\n\nSaving model (epoch = 1598, MSE loss = 0.8692)\n\nSaving model (epoch = 1608, MSE loss = 0.8678)\n\nSaving model (epoch = 1618, MSE loss = 0.8664)\n\nSaving model (epoch = 1628, MSE loss = 0.8651)\n\nSaving model (epoch = 1638, MSE loss = 0.8638)\n\nSaving model (epoch = 1648, MSE loss = 0.8625)\n\nSaving model (epoch = 1658, MSE loss = 0.8613)\n\nSaving model (epoch = 1668, MSE loss = 0.8602)\n\nSaving model (epoch = 1678, MSE loss = 0.8590)\n\nSaving model (epoch = 1688, MSE loss = 0.8580)\n\nSaving model (epoch = 1698, MSE loss = 0.8569)\n\nSaving model (epoch = 1708, MSE loss = 0.8559)\n\nSaving model (epoch = 1718, MSE loss = 0.8550)\n\nSaving model (epoch = 1728, MSE loss = 0.8540)\n\nSaving model (epoch = 1738, MSE loss = 0.8531)\n\nSaving model (epoch = 1748, MSE loss = 0.8522)\n\nSaving model (epoch = 1758, MSE loss = 0.8513)\n\nSaving model (epoch = 1768, MSE loss = 0.8505)\n\nSaving model (epoch = 1778, MSE loss = 0.8497)\n\nSaving model (epoch = 1788, MSE loss = 0.8489)\n\nSaving model (epoch = 1798, MSE loss = 0.8482)\n\nSaving model (epoch = 1808, MSE loss = 0.8475)\n\nSaving model (epoch = 1818, MSE loss = 0.8468)\n\nSaving model (epoch = 1828, MSE loss = 0.8461)\n\nSaving model (epoch = 1838, MSE loss = 0.8453)\n\nSaving model (epoch = 1848, MSE loss = 0.8447)\n\nSaving model (epoch = 1858, MSE loss = 0.8441)\n\nSaving model (epoch = 1868, MSE loss = 0.8434)\n\nSaving model (epoch = 1878, MSE loss = 0.8429)\n\nSaving model (epoch = 1888, MSE loss = 0.8423)\n\nSaving model (epoch = 1898, MSE loss = 0.8418)\n\nSaving model (epoch = 1908, MSE loss = 0.8412)\n\nSaving model (epoch = 1918, MSE loss = 0.8407)\n\nSaving model (epoch = 1928, MSE loss = 0.8403)\n\nSaving model (epoch = 1938, MSE loss = 0.8398)\n\nSaving model (epoch = 1948, MSE loss = 0.8393)\n\nSaving model (epoch = 1958, MSE loss = 0.8388)\n\nSaving model (epoch = 1968, MSE loss = 0.8384)\n\nSaving model (epoch = 1978, MSE loss = 0.8380)\n\nSaving model (epoch = 1988, MSE loss = 0.8375)\n\nSaving model (epoch = 1998, MSE loss = 0.8371)\n\nSaving model (epoch = 2008, MSE loss = 0.8367)\n\nSaving model (epoch = 2018, MSE loss = 0.8364)\n\nSaving model (epoch = 2028, MSE loss = 0.8360)\n\nSaving model (epoch = 2038, MSE loss = 0.8357)\n\nSaving model (epoch = 2048, MSE loss = 0.8354)\n\nSaving model (epoch = 2058, MSE loss = 0.8351)\n\nSaving model (epoch = 2068, MSE loss = 0.8348)\n\nSaving model (epoch = 2078, MSE loss = 0.8345)\n\nSaving model (epoch = 2088, MSE loss = 0.8342)\n\nSaving model (epoch = 2098, MSE loss = 0.8339)\n\nSaving model (epoch = 2108, MSE loss = 0.8336)\n\nSaving model (epoch = 2118, MSE loss = 0.8333)\n\nSaving model (epoch = 2128, MSE loss = 0.8331)\n\nSaving model (epoch = 2138, MSE loss = 0.8328)\n\nSaving model (epoch = 2148, MSE loss = 0.8326)\n\nSaving model (epoch = 2158, MSE loss = 0.8324)\n\nSaving model (epoch = 2168, MSE loss = 0.8321)\n\nSaving model (epoch = 2178, MSE loss = 0.8319)\n\nSaving model (epoch = 2188, MSE loss = 0.8317)\n\nSaving model (epoch = 2198, MSE loss = 0.8315)\n\nSaving model (epoch = 2208, MSE loss = 0.8313)\n\nSaving model (epoch = 2218, MSE loss = 0.8311)\n\nSaving model (epoch = 2228, MSE loss = 0.8309)\n\nSaving model (epoch = 2238, MSE loss = 0.8307)\n\nSaving model (epoch = 2248, MSE loss = 0.8305)\n\nSaving model (epoch = 2258, MSE loss = 0.8303)\n\nSaving model (epoch = 2268, MSE loss = 0.8302)\n\nSaving model (epoch = 2278, MSE loss = 0.8300)\n\nSaving model (epoch = 2288, MSE loss = 0.8298)\n\nSaving model (epoch = 2298, MSE loss = 0.8297)\n\nSaving model (epoch = 2308, MSE loss = 0.8295)\n\nSaving model (epoch = 2318, MSE loss = 0.8294)\n\nSaving model (epoch = 2328, MSE loss = 0.8292)\n\nSaving model (epoch = 2338, MSE loss = 0.8291)\n\nSaving model (epoch = 2348, MSE loss = 0.8290)\n\nSaving model (epoch = 2358, MSE loss = 0.8289)\n\nSaving model (epoch = 2368, MSE loss = 0.8287)\n\nSaving model (epoch = 2378, MSE loss = 0.8286)\n\nSaving model (epoch = 2388, MSE loss = 0.8285)\n\nSaving model (epoch = 2398, MSE loss = 0.8284)\n\nSaving model (epoch = 2408, MSE loss = 0.8283)\n\nSaving model (epoch = 2418, MSE loss = 0.8282)\n\nSaving model (epoch = 2428, MSE loss = 0.8281)\n\nSaving model (epoch = 2438, MSE loss = 0.8280)\n\nSaving model (epoch = 2448, MSE loss = 0.8279)\n\nSaving model (epoch = 2458, MSE loss = 0.8278)\n\nSaving model (epoch = 2468, MSE loss = 0.8277)\n\nSaving model (epoch = 2478, MSE loss = 0.8276)\n\nSaving model (epoch = 2488, MSE loss = 0.8274)\n\nSaving model (epoch = 2498, MSE loss = 0.8273)\n\nSaving model (epoch = 2508, MSE loss = 0.8273)\n\nSaving model (epoch = 2518, MSE loss = 0.8272)\n\nSaving model (epoch = 2528, MSE loss = 0.8271)\n\nSaving model (epoch = 2538, MSE loss = 0.8270)\n\nSaving model (epoch = 2548, MSE loss = 0.8269)\n\nSaving model (epoch = 2558, MSE loss = 0.8268)\n\nSaving model (epoch = 2568, MSE loss = 0.8267)\n\nSaving model (epoch = 2578, MSE loss = 0.8266)\n\nSaving model (epoch = 2588, MSE loss = 0.8266)\n\nSaving model (epoch = 2598, MSE loss = 0.8265)\n\nSaving model (epoch = 2608, MSE loss = 0.8264)\n\nSaving model (epoch = 2618, MSE loss = 0.8263)\n\nSaving model (epoch = 2628, MSE loss = 0.8262)\n\nSaving model (epoch = 2638, MSE loss = 0.8261)\n\nSaving model (epoch = 2648, MSE loss = 0.8260)\n\nSaving model (epoch = 2658, MSE loss = 0.8258)\n\nSaving model (epoch = 2668, MSE loss = 0.8257)\n\nSaving model (epoch = 2678, MSE loss = 0.8256)\n\nSaving model (epoch = 2688, MSE loss = 0.8256)\n\nSaving model (epoch = 2698, MSE loss = 0.8255)\n\nSaving model (epoch = 2708, MSE loss = 0.8254)\n\nSaving model (epoch = 2718, MSE loss = 0.8253)\n\nSaving model (epoch = 2728, MSE loss = 0.8252)\n\nSaving model (epoch = 2738, MSE loss = 0.8251)\n\nSaving model (epoch = 2748, MSE loss = 0.8251)\n\nSaving model (epoch = 2758, MSE loss = 0.8250)\n\nSaving model (epoch = 2768, MSE loss = 0.8249)\n\nSaving model (epoch = 2778, MSE loss = 0.8248)\n\nSaving model (epoch = 2788, MSE loss = 0.8248)\n\nSaving model (epoch = 2798, MSE loss = 0.8247)\n\nSaving model (epoch = 2808, MSE loss = 0.8246)\n\nSaving model (epoch = 2818, MSE loss = 0.8246)\n\nSaving model (epoch = 2828, MSE loss = 0.8245)\n\nSaving model (epoch = 2838, MSE loss = 0.8244)\n\nSaving model (epoch = 2848, MSE loss = 0.8244)\n\nSaving model (epoch = 2858, MSE loss = 0.8243)\n\nSaving model (epoch = 2868, MSE loss = 0.8242)\n\nSaving model (epoch = 2878, MSE loss = 0.8242)\n\nSaving model (epoch = 2888, MSE loss = 0.8241)\n\nSaving model (epoch = 2898, MSE loss = 0.8241)\n\nSaving model (epoch = 2908, MSE loss = 0.8240)\n\nSaving model (epoch = 2918, MSE loss = 0.8240)\n\nSaving model (epoch = 2928, MSE loss = 0.8239)\n\nSaving model (epoch = 2938, MSE loss = 0.8239)\n\nSaving model (epoch = 2948, MSE loss = 0.8238)\n\nSaving model (epoch = 2958, MSE loss = 0.8238)\n\nSaving model (epoch = 2968, MSE loss = 0.8237)\n\nSaving model (epoch = 2978, MSE loss = 0.8237)\n\nSaving model (epoch = 2988, MSE loss = 0.8236)\n\nSaving model (epoch = 2998, MSE loss = 0.8236)\n\nSaving model (epoch = 3008, MSE loss = 0.8235)\n\nSaving model (epoch = 3018, MSE loss = 0.8235)\n\nSaving model (epoch = 3028, MSE loss = 0.8234)\n\nSaving model (epoch = 3038, MSE loss = 0.8234)\n\nSaving model (epoch = 3048, MSE loss = 0.8233)\n\nSaving model (epoch = 3058, MSE loss = 0.8233)\n\nSaving model (epoch = 3068, MSE loss = 0.8232)\n\nSaving model (epoch = 3078, MSE loss = 0.8232)\n\nSaving model (epoch = 3088, MSE loss = 0.8231)\n\nSaving model (epoch = 3098, MSE loss = 0.8230)\n\nSaving model (epoch = 3108, MSE loss = 0.8230)\n\nSaving model (epoch = 3118, MSE loss = 0.8230)\n\nSaving model (epoch = 3128, MSE loss = 0.8229)\n\nSaving model (epoch = 3138, MSE loss = 0.8229)\n\nSaving model (epoch = 3148, MSE loss = 0.8229)\n\nSaving model (epoch = 3158, MSE loss = 0.8228)\n\nSaving model (epoch = 3168, MSE loss = 0.8228)\n\nSaving model (epoch = 3178, MSE loss = 0.8227)\n\nSaving model (epoch = 3188, MSE loss = 0.8227)\n\nSaving model (epoch = 3198, MSE loss = 0.8226)\n\nSaving model (epoch = 3208, MSE loss = 0.8226)\n\nSaving model (epoch = 3218, MSE loss = 0.8225)\n\nSaving model (epoch = 3228, MSE loss = 0.8225)\n\nSaving model (epoch = 3238, MSE loss = 0.8225)\n\nSaving model (epoch = 3248, MSE loss = 0.8224)\n\nSaving model (epoch = 3258, MSE loss = 0.8224)\n\nSaving model (epoch = 3268, MSE loss = 0.8223)\n\nSaving model (epoch = 3278, MSE loss = 0.8223)\n\nSaving model (epoch = 3288, MSE loss = 0.8222)\n\nSaving model (epoch = 3298, MSE loss = 0.8222)\n\nSaving model (epoch = 3308, MSE loss = 0.8222)\n\nSaving model (epoch = 3318, MSE loss = 0.8222)\n\nSaving model (epoch = 3328, MSE loss = 0.8221)\n\nSaving model (epoch = 3338, MSE loss = 0.8220)\n\nSaving model (epoch = 3348, MSE loss = 0.8220)\n\nSaving model (epoch = 3358, MSE loss = 0.8219)\n\nSaving model (epoch = 3368, MSE loss = 0.8219)\n\nSaving model (epoch = 3378, MSE loss = 0.8219)\n\nSaving model (epoch = 3388, MSE loss = 0.8218)\n\nSaving model (epoch = 3398, MSE loss = 0.8217)\n\nSaving model (epoch = 3408, MSE loss = 0.8217)\n\nSaving model (epoch = 3418, MSE loss = 0.8217)\n\nSaving model (epoch = 3428, MSE loss = 0.8216)\n\nSaving model (epoch = 3438, MSE loss = 0.8216)\n\nSaving model (epoch = 3448, MSE loss = 0.8215)\n\nSaving model (epoch = 3458, MSE loss = 0.8214)\n\nSaving model (epoch = 3468, MSE loss = 0.8214)\n\nSaving model (epoch = 3478, MSE loss = 0.8213)\n\nSaving model (epoch = 3488, MSE loss = 0.8213)\n\nSaving model (epoch = 3498, MSE loss = 0.8212)\n\nSaving model (epoch = 3508, MSE loss = 0.8212)\n\nSaving model (epoch = 3518, MSE loss = 0.8211)\n\nSaving model (epoch = 3528, MSE loss = 0.8211)\n\nSaving model (epoch = 3538, MSE loss = 0.8210)\n\nSaving model (epoch = 3548, MSE loss = 0.8209)\n\nSaving model (epoch = 3558, MSE loss = 0.8209)\n\nSaving model (epoch = 3568, MSE loss = 0.8209)\n\nSaving model (epoch = 3578, MSE loss = 0.8208)\n\nSaving model (epoch = 3588, MSE loss = 0.8208)\n\nSaving model (epoch = 3598, MSE loss = 0.8207)\n\nSaving model (epoch = 3608, MSE loss = 0.8206)\n\nSaving model (epoch = 3618, MSE loss = 0.8206)\n\nSaving model (epoch = 3628, MSE loss = 0.8206)\n\nSaving model (epoch = 3638, MSE loss = 0.8205)\n\nSaving model (epoch = 3648, MSE loss = 0.8205)\n\nSaving model (epoch = 3658, MSE loss = 0.8204)\n\nSaving model (epoch = 3668, MSE loss = 0.8204)\n\nSaving model (epoch = 3678, MSE loss = 0.8204)\n\nSaving model (epoch = 3688, MSE loss = 0.8203)\n\nSaving model (epoch = 3698, MSE loss = 0.8203)\n\nSaving model (epoch = 3708, MSE loss = 0.8203)\n\nSaving model (epoch = 3718, MSE loss = 0.8202)\n\nSaving model (epoch = 3728, MSE loss = 0.8202)\n\nSaving model (epoch = 3738, MSE loss = 0.8201)\n\nSaving model (epoch = 3748, MSE loss = 0.8201)\n\nSaving model (epoch = 3758, MSE loss = 0.8201)\n\nSaving model (epoch = 3768, MSE loss = 0.8200)\n\nSaving model (epoch = 3778, MSE loss = 0.8200)\n\nSaving model (epoch = 3788, MSE loss = 0.8200)\n\nSaving model (epoch = 3798, MSE loss = 0.8199)\n\nSaving model (epoch = 3808, MSE loss = 0.8199)\n\nSaving model (epoch = 3818, MSE loss = 0.8199)\n\nSaving model (epoch = 3828, MSE loss = 0.8198)\n\nSaving model (epoch = 3838, MSE loss = 0.8198)\n\nSaving model (epoch = 3848, MSE loss = 0.8198)\n\nSaving model (epoch = 3858, MSE loss = 0.8198)\n\nSaving model (epoch = 3868, MSE loss = 0.8198)\n\nSaving model (epoch = 3878, MSE loss = 0.8198)\n\nSaving model (epoch = 3888, MSE loss = 0.8197)\n\nSaving model (epoch = 3898, MSE loss = 0.8197)\n\nSaving model (epoch = 3908, MSE loss = 0.8197)\n\nSaving model (epoch = 3918, MSE loss = 0.8196)\n\nSaving model (epoch = 3928, MSE loss = 0.8196)\n\nSaving model (epoch = 3938, MSE loss = 0.8196)\n\nSaving model (epoch = 3948, MSE loss = 0.8195)\n\nSaving model (epoch = 3958, MSE loss = 0.8195)\n\nSaving model (epoch = 3968, MSE loss = 0.8195)\n\nSaving model (epoch = 3978, MSE loss = 0.8195)\n\nSaving model (epoch = 3988, MSE loss = 0.8195)\n\nSaving model (epoch = 3998, MSE loss = 0.8194)\n\nSaving model (epoch = 4008, MSE loss = 0.8194)\n\nSaving model (epoch = 4028, MSE loss = 0.8194)\n\nSaving model (epoch = 4038, MSE loss = 0.8194)\n\nSaving model (epoch = 4048, MSE loss = 0.8193)\n\nSaving model (epoch = 4058, MSE loss = 0.8193)\n\nSaving model (epoch = 4068, MSE loss = 0.8193)\n\nSaving model (epoch = 4088, MSE loss = 0.8192)\n\nSaving model (epoch = 4098, MSE loss = 0.8192)\n\nSaving model (epoch = 4108, MSE loss = 0.8192)\n\nSaving model (epoch = 4118, MSE loss = 0.8192)\n\nSaving model (epoch = 4128, MSE loss = 0.8191)\n\nSaving model (epoch = 4138, MSE loss = 0.8191)\n\nSaving model (epoch = 4148, MSE loss = 0.8191)\n\nSaving model (epoch = 4158, MSE loss = 0.8191)\n\nSaving model (epoch = 4168, MSE loss = 0.8191)\n\nSaving model (epoch = 4178, MSE loss = 0.8190)\n\nSaving model (epoch = 4188, MSE loss = 0.8190)\n\nSaving model (epoch = 4198, MSE loss = 0.8190)\n\nSaving model (epoch = 4218, MSE loss = 0.8189)\n\nSaving model (epoch = 4228, MSE loss = 0.8189)\n\nSaving model (epoch = 4248, MSE loss = 0.8189)\n\nSaving model (epoch = 4258, MSE loss = 0.8189)\n\nSaving model (epoch = 4278, MSE loss = 0.8189)\n\nSaving model (epoch = 4288, MSE loss = 0.8189)\n\nSaving model (epoch = 4308, MSE loss = 0.8188)\n\nSaving model (epoch = 4318, MSE loss = 0.8188)\n\nSaving model (epoch = 4338, MSE loss = 0.8188)\n\nSaving model (epoch = 4368, MSE loss = 0.8188)\n\nSaving model (epoch = 4398, MSE loss = 0.8188)\n\nSaving model (epoch = 4418, MSE loss = 0.8188)\n\nSaving model (epoch = 4438, MSE loss = 0.8188)\n\nSaving model (epoch = 4448, MSE loss = 0.8188)\n\nSaving model (epoch = 4458, MSE loss = 0.8187)\n\nSaving model (epoch = 4468, MSE loss = 0.8187)\n\nSaving model (epoch = 4478, MSE loss = 0.8187)\n\nSaving model (epoch = 4488, MSE loss = 0.8187)\n\nSaving model (epoch = 4498, MSE loss = 0.8187)\n\nSaving model (epoch = 4518, MSE loss = 0.8186)\n\nSaving model (epoch = 4528, MSE loss = 0.8186)\n\nSaving model (epoch = 4538, MSE loss = 0.8186)\n\nSaving model (epoch = 4548, MSE loss = 0.8186)\n\nSaving model (epoch = 4568, MSE loss = 0.8185)\n\nSaving model (epoch = 4588, MSE loss = 0.8185)\n\nSaving model (epoch = 4608, MSE loss = 0.8185)\n\nSaving model (epoch = 4618, MSE loss = 0.8185)\n\nSaving model (epoch = 4628, MSE loss = 0.8184)\n\nSaving model (epoch = 4648, MSE loss = 0.8184)\n\nSaving model (epoch = 4658, MSE loss = 0.8184)\n\nSaving model (epoch = 4668, MSE loss = 0.8184)\n\nSaving model (epoch = 4678, MSE loss = 0.8183)\n\nSaving model (epoch = 4688, MSE loss = 0.8183)\n\nSaving model (epoch = 4698, MSE loss = 0.8183)\n\nSaving model (epoch = 4708, MSE loss = 0.8183)\n\nSaving model (epoch = 4718, MSE loss = 0.8182)\n\nSaving model (epoch = 4728, MSE loss = 0.8182)\n\nSaving model (epoch = 4738, MSE loss = 0.8182)\n\nSaving model (epoch = 4748, MSE loss = 0.8182)\n\nSaving model (epoch = 4758, MSE loss = 0.8181)\n\nSaving model (epoch = 4768, MSE loss = 0.8181)\n\nSaving model (epoch = 4778, MSE loss = 0.8181)\n\nSaving model (epoch = 4788, MSE loss = 0.8180)\n\nSaving model (epoch = 4798, MSE loss = 0.8180)\n\nSaving model (epoch = 4808, MSE loss = 0.8180)\n\nSaving model (epoch = 4818, MSE loss = 0.8179)\n\nSaving model (epoch = 4828, MSE loss = 0.8179)\n\nSaving model (epoch = 4838, MSE loss = 0.8179)\n\nSaving model (epoch = 4848, MSE loss = 0.8178)\n\nSaving model (epoch = 4858, MSE loss = 0.8178)\n\nSaving model (epoch = 4868, MSE loss = 0.8178)\n\nSaving model (epoch = 4878, MSE loss = 0.8177)\n\nSaving model (epoch = 4888, MSE loss = 0.8177)\n\nSaving model (epoch = 4898, MSE loss = 0.8177)\n\nSaving model (epoch = 4908, MSE loss = 0.8176)\n\nSaving model (epoch = 4918, MSE loss = 0.8176)\n\nSaving model (epoch = 4928, MSE loss = 0.8175)\n\nSaving model (epoch = 4938, MSE loss = 0.8175)\n\nSaving model (epoch = 4948, MSE loss = 0.8174)\n\nSaving model (epoch = 4958, MSE loss = 0.8174)\n\nSaving model (epoch = 4968, MSE loss = 0.8173)\n\nSaving model (epoch = 4978, MSE loss = 0.8173)\n\nSaving model (epoch = 4988, MSE loss = 0.8172)\n\nSaving model (epoch = 4998, MSE loss = 0.8172)\n\nSaving model (epoch = 5008, MSE loss = 0.8171)\n\nSaving model (epoch = 5018, MSE loss = 0.8171)\n\nSaving model (epoch = 5028, MSE loss = 0.8170)\n\nSaving model (epoch = 5038, MSE loss = 0.8170)\n\nSaving model (epoch = 5048, MSE loss = 0.8169)\n\nSaving model (epoch = 5058, MSE loss = 0.8169)\n\nSaving model (epoch = 5068, MSE loss = 0.8168)\n\nSaving model (epoch = 5078, MSE loss = 0.8168)\n\nSaving model (epoch = 5088, MSE loss = 0.8168)\n\nSaving model (epoch = 5098, MSE loss = 0.8167)\n\nSaving model (epoch = 5108, MSE loss = 0.8167)\n\nSaving model (epoch = 5118, MSE loss = 0.8166)\n\nSaving model (epoch = 5128, MSE loss = 0.8166)\n\nSaving model (epoch = 5138, MSE loss = 0.8165)\n\nSaving model (epoch = 5148, MSE loss = 0.8165)\n\nSaving model (epoch = 5158, MSE loss = 0.8164)\n\nSaving model (epoch = 5168, MSE loss = 0.8164)\n\nSaving model (epoch = 5178, MSE loss = 0.8163)\n\nSaving model (epoch = 5188, MSE loss = 0.8163)\n\nSaving model (epoch = 5198, MSE loss = 0.8163)\n\nSaving model (epoch = 5208, MSE loss = 0.8162)\n\nSaving model (epoch = 5218, MSE loss = 0.8161)\n\nSaving model (epoch = 5228, MSE loss = 0.8161)\n\nSaving model (epoch = 5238, MSE loss = 0.8160)\n\nSaving model (epoch = 5248, MSE loss = 0.8160)\n\nSaving model (epoch = 5258, MSE loss = 0.8159)\n\nSaving model (epoch = 5268, MSE loss = 0.8159)\n\nSaving model (epoch = 5278, MSE loss = 0.8158)\n\nSaving model (epoch = 5288, MSE loss = 0.8158)\n\nSaving model (epoch = 5298, MSE loss = 0.8157)\n\nSaving model (epoch = 5308, MSE loss = 0.8157)\n\nSaving model (epoch = 5318, MSE loss = 0.8156)\n\nSaving model (epoch = 5328, MSE loss = 0.8156)\n\nSaving model (epoch = 5338, MSE loss = 0.8155)\n\nSaving model (epoch = 5348, MSE loss = 0.8155)\n\nSaving model (epoch = 5358, MSE loss = 0.8154)\n\nSaving model (epoch = 5368, MSE loss = 0.8154)\n\nSaving model (epoch = 5378, MSE loss = 0.8154)\n\nSaving model (epoch = 5388, MSE loss = 0.8153)\n\nSaving model (epoch = 5398, MSE loss = 0.8153)\n\nSaving model (epoch = 5408, MSE loss = 0.8153)\n\nSaving model (epoch = 5418, MSE loss = 0.8152)\n\nSaving model (epoch = 5428, MSE loss = 0.8152)\n\nSaving model (epoch = 5438, MSE loss = 0.8151)\n\nSaving model (epoch = 5448, MSE loss = 0.8151)\n\nSaving model (epoch = 5458, MSE loss = 0.8150)\n\nSaving model (epoch = 5468, MSE loss = 0.8150)\n\nSaving model (epoch = 5478, MSE loss = 0.8150)\n\nSaving model (epoch = 5488, MSE loss = 0.8149)\n\nSaving model (epoch = 5498, MSE loss = 0.8149)\n\nSaving model (epoch = 5508, MSE loss = 0.8149)\n\nSaving model (epoch = 5518, MSE loss = 0.8148)\n\nSaving model (epoch = 5528, MSE loss = 0.8148)\n\nSaving model (epoch = 5538, MSE loss = 0.8148)\n\nSaving model (epoch = 5548, MSE loss = 0.8147)\n\nSaving model (epoch = 5558, MSE loss = 0.8147)\n\nSaving model (epoch = 5568, MSE loss = 0.8147)\n\nSaving model (epoch = 5578, MSE loss = 0.8146)\n\nSaving model (epoch = 5588, MSE loss = 0.8146)\n\nSaving model (epoch = 5598, MSE loss = 0.8146)\n\nSaving model (epoch = 5608, MSE loss = 0.8145)\n\nSaving model (epoch = 5618, MSE loss = 0.8145)\n\nSaving model (epoch = 5628, MSE loss = 0.8144)\n\nSaving model (epoch = 5638, MSE loss = 0.8144)\n\nSaving model (epoch = 5648, MSE loss = 0.8143)\n\nSaving model (epoch = 5658, MSE loss = 0.8142)\n\nSaving model (epoch = 5668, MSE loss = 0.8142)\n\nSaving model (epoch = 5678, MSE loss = 0.8141)\n\nSaving model (epoch = 5688, MSE loss = 0.8141)\n\nSaving model (epoch = 5698, MSE loss = 0.8140)\n\nSaving model (epoch = 5708, MSE loss = 0.8140)\n\nSaving model (epoch = 5718, MSE loss = 0.8140)\n\nSaving model (epoch = 5728, MSE loss = 0.8139)\n\nSaving model (epoch = 5738, MSE loss = 0.8139)\n\nSaving model (epoch = 5748, MSE loss = 0.8139)\n\nSaving model (epoch = 5758, MSE loss = 0.8138)\n\nSaving model (epoch = 5768, MSE loss = 0.8138)\n\nSaving model (epoch = 5778, MSE loss = 0.8137)\n\nSaving model (epoch = 5788, MSE loss = 0.8137)\n\nSaving model (epoch = 5798, MSE loss = 0.8136)\n\nSaving model (epoch = 5808, MSE loss = 0.8136)\n\nSaving model (epoch = 5818, MSE loss = 0.8136)\n\nSaving model (epoch = 5828, MSE loss = 0.8135)\n\nSaving model (epoch = 5838, MSE loss = 0.8135)\n\nSaving model (epoch = 5848, MSE loss = 0.8135)\n\nSaving model (epoch = 5858, MSE loss = 0.8135)\n\nSaving model (epoch = 5898, MSE loss = 0.8134)\n\nSaving model (epoch = 5908, MSE loss = 0.8134)\n\nSaving model (epoch = 5918, MSE loss = 0.8134)\n\nSaving model (epoch = 5928, MSE loss = 0.8134)\n\nSaving model (epoch = 5938, MSE loss = 0.8134)\n\nSaving model (epoch = 5948, MSE loss = 0.8134)\n\nSaving model (epoch = 5958, MSE loss = 0.8133)\n\nSaving model (epoch = 5968, MSE loss = 0.8133)\n\nSaving model (epoch = 5978, MSE loss = 0.8133)\n\nSaving model (epoch = 5988, MSE loss = 0.8133)\n\nSaving model (epoch = 5998, MSE loss = 0.8132)\n\nSaving model (epoch = 6008, MSE loss = 0.8132)\n\nSaving model (epoch = 6018, MSE loss = 0.8132)\n\nSaving model (epoch = 6028, MSE loss = 0.8132)\n\nSaving model (epoch = 6038, MSE loss = 0.8132)\n\nSaving model (epoch = 6048, MSE loss = 0.8131)\n\nSaving model (epoch = 6058, MSE loss = 0.8131)\n\nSaving model (epoch = 6068, MSE loss = 0.8131)\n\nSaving model (epoch = 6078, MSE loss = 0.8131)\n\nSaving model (epoch = 6088, MSE loss = 0.8131)\n\nSaving model (epoch = 6098, MSE loss = 0.8130)\n\nSaving model (epoch = 6108, MSE loss = 0.8130)\n\nSaving model (epoch = 6118, MSE loss = 0.8130)\n\nSaving model (epoch = 6128, MSE loss = 0.8130)\n\nSaving model (epoch = 6138, MSE loss = 0.8129)\n\nSaving model (epoch = 6148, MSE loss = 0.8129)\n\nSaving model (epoch = 6158, MSE loss = 0.8129)\n\nSaving model (epoch = 6168, MSE loss = 0.8129)\n\nSaving model (epoch = 6178, MSE loss = 0.8129)\n\nSaving model (epoch = 6188, MSE loss = 0.8129)\n\nSaving model (epoch = 6208, MSE loss = 0.8129)\n\nSaving model (epoch = 6218, MSE loss = 0.8129)\n\nSaving model (epoch = 6228, MSE loss = 0.8128)\n\nSaving model (epoch = 6238, MSE loss = 0.8128)\n\nSaving model (epoch = 6258, MSE loss = 0.8128)\n\nSaving model (epoch = 6278, MSE loss = 0.8128)\n\nSaving model (epoch = 6298, MSE loss = 0.8128)\n\nSaving model (epoch = 6308, MSE loss = 0.8127)\n\nSaving model (epoch = 6328, MSE loss = 0.8127)\n\nSaving model (epoch = 6348, MSE loss = 0.8127)\n\nSaving model (epoch = 6358, MSE loss = 0.8127)\n\nSaving model (epoch = 6368, MSE loss = 0.8127)\n\nSaving model (epoch = 6378, MSE loss = 0.8127)\n\nSaving model (epoch = 6428, MSE loss = 0.8127)\n\nSaving model (epoch = 6448, MSE loss = 0.8127)\n\nSaving model (epoch = 6468, MSE loss = 0.8126)\n\nSaving model (epoch = 6498, MSE loss = 0.8126)\n\nSaving model (epoch = 6518, MSE loss = 0.8126)\n\nSaving model (epoch = 6538, MSE loss = 0.8126)\n\nSaving model (epoch = 6578, MSE loss = 0.8126)\n\nSaving model (epoch = 6588, MSE loss = 0.8126)\n\nSaving model (epoch = 6598, MSE loss = 0.8126)\n\nSaving model (epoch = 6608, MSE loss = 0.8126)\n\nSaving model (epoch = 6618, MSE loss = 0.8126)\n\nSaving model (epoch = 6628, MSE loss = 0.8126)\n\nSaving model (epoch = 6648, MSE loss = 0.8125)\n\nSaving model (epoch = 6668, MSE loss = 0.8125)\n\nSaving model (epoch = 6688, MSE loss = 0.8125)\n\nSaving model (epoch = 6708, MSE loss = 0.8125)\n\nSaving model (epoch = 6718, MSE loss = 0.8125)\n\nSaving model (epoch = 6728, MSE loss = 0.8125)\n\nSaving model (epoch = 6738, MSE loss = 0.8125)\n\nSaving model (epoch = 6758, MSE loss = 0.8125)\n\nSaving model (epoch = 6778, MSE loss = 0.8125)\n\nSaving model (epoch = 6798, MSE loss = 0.8125)\n\nSaving model (epoch = 6828, MSE loss = 0.8125)\n\nSaving model (epoch = 6848, MSE loss = 0.8125)\n\nSaving model (epoch = 6858, MSE loss = 0.8125)\n\nSaving model (epoch = 6878, MSE loss = 0.8125)\n\nSaving model (epoch = 6908, MSE loss = 0.8124)\n\nSaving model (epoch = 6918, MSE loss = 0.8124)\n\nSaving model (epoch = 6928, MSE loss = 0.8124)\n\nSaving model (epoch = 6938, MSE loss = 0.8124)\n\nSaving model (epoch = 6948, MSE loss = 0.8123)\n\nSaving model (epoch = 6958, MSE loss = 0.8123)\n\nSaving model (epoch = 6968, MSE loss = 0.8123)\n\nSaving model (epoch = 6978, MSE loss = 0.8123)\n\nSaving model (epoch = 6988, MSE loss = 0.8123)\n\nSaving model (epoch = 6998, MSE loss = 0.8123)\n\nSaving model (epoch = 7008, MSE loss = 0.8123)\n\nSaving model (epoch = 7018, MSE loss = 0.8123)\n\nSaving model (epoch = 7028, MSE loss = 0.8123)\n\nSaving model (epoch = 7038, MSE loss = 0.8123)\n\nSaving model (epoch = 7048, MSE loss = 0.8122)\n\nSaving model (epoch = 7058, MSE loss = 0.8122)\n\nSaving model (epoch = 7068, MSE loss = 0.8122)\n\nSaving model (epoch = 7078, MSE loss = 0.8122)\n\nSaving model (epoch = 7098, MSE loss = 0.8122)\n\nSaving model (epoch = 7108, MSE loss = 0.8122)\n\nSaving model (epoch = 7118, MSE loss = 0.8122)\n\nSaving model (epoch = 7128, MSE loss = 0.8122)\n\nSaving model (epoch = 7138, MSE loss = 0.8121)\n\nSaving model (epoch = 7148, MSE loss = 0.8121)\n\nSaving model (epoch = 7158, MSE loss = 0.8121)\n\nSaving model (epoch = 7168, MSE loss = 0.8121)\n\nSaving model (epoch = 7188, MSE loss = 0.8121)\n\nSaving model (epoch = 7198, MSE loss = 0.8121)\n\nSaving model (epoch = 7208, MSE loss = 0.8121)\n\nSaving model (epoch = 7238, MSE loss = 0.8121)\n\nSaving model (epoch = 7248, MSE loss = 0.8120)\n\nSaving model (epoch = 7258, MSE loss = 0.8120)\n\nSaving model (epoch = 7268, MSE loss = 0.8120)\n\nSaving model (epoch = 7278, MSE loss = 0.8120)\n\nSaving model (epoch = 7298, MSE loss = 0.8119)\n\nSaving model (epoch = 7308, MSE loss = 0.8119)\n\nSaving model (epoch = 7318, MSE loss = 0.8119)\n\nSaving model (epoch = 7328, MSE loss = 0.8119)\n\nSaving model (epoch = 7338, MSE loss = 0.8119)\n\nSaving model (epoch = 7348, MSE loss = 0.8119)\n\nSaving model (epoch = 7358, MSE loss = 0.8118)\n\nSaving model (epoch = 7378, MSE loss = 0.8118)\n\nSaving model (epoch = 7388, MSE loss = 0.8118)\n\nSaving model (epoch = 7398, MSE loss = 0.8118)\n\nSaving model (epoch = 7408, MSE loss = 0.8118)\n\nSaving model (epoch = 7418, MSE loss = 0.8118)\n\nSaving model (epoch = 7428, MSE loss = 0.8117)\n\nSaving model (epoch = 7438, MSE loss = 0.8117)\n\nSaving model (epoch = 7448, MSE loss = 0.8117)\n\nSaving model (epoch = 7458, MSE loss = 0.8117)\n\nSaving model (epoch = 7468, MSE loss = 0.8117)\n\nSaving model (epoch = 7478, MSE loss = 0.8117)\n\nSaving model (epoch = 7488, MSE loss = 0.8116)\n\nSaving model (epoch = 7508, MSE loss = 0.8116)\n\nSaving model (epoch = 7528, MSE loss = 0.8116)\n\nSaving model (epoch = 7538, MSE loss = 0.8116)\n\nSaving model (epoch = 7578, MSE loss = 0.8116)\n\nSaving model (epoch = 7598, MSE loss = 0.8115)\n\nSaving model (epoch = 7618, MSE loss = 0.8115)\n\nSaving model (epoch = 7628, MSE loss = 0.8115)\n\nSaving model (epoch = 7648, MSE loss = 0.8115)\n\nSaving model (epoch = 7668, MSE loss = 0.8115)\n\nSaving model (epoch = 7678, MSE loss = 0.8114)\n\nSaving model (epoch = 7688, MSE loss = 0.8114)\n\nSaving model (epoch = 7698, MSE loss = 0.8114)\n\nSaving model (epoch = 7718, MSE loss = 0.8114)\n\nSaving model (epoch = 7738, MSE loss = 0.8114)\n\nSaving model (epoch = 7748, MSE loss = 0.8114)\n\nSaving model (epoch = 7768, MSE loss = 0.8114)\n\nSaving model (epoch = 7778, MSE loss = 0.8114)\n\nSaving model (epoch = 7798, MSE loss = 0.8113)\n\nSaving model (epoch = 7818, MSE loss = 0.8113)\n\nSaving model (epoch = 7828, MSE loss = 0.8113)\n\nSaving model (epoch = 7848, MSE loss = 0.8113)\n\nSaving model (epoch = 7868, MSE loss = 0.8113)\n\nSaving model (epoch = 7878, MSE loss = 0.8113)\n\nSaving model (epoch = 7898, MSE loss = 0.8113)\n\nSaving model (epoch = 7938, MSE loss = 0.8113)\n\nSaving model (epoch = 7958, MSE loss = 0.8113)\n\nSaving model (epoch = 7978, MSE loss = 0.8112)\n\nSaving model (epoch = 7998, MSE loss = 0.8112)\n\nSaving model (epoch = 8008, MSE loss = 0.8112)\n\nSaving model (epoch = 8018, MSE loss = 0.8112)\n\nSaving model (epoch = 8028, MSE loss = 0.8112)\n\nSaving model (epoch = 8038, MSE loss = 0.8112)\n\nSaving model (epoch = 8058, MSE loss = 0.8112)\n\nSaving model (epoch = 8078, MSE loss = 0.8112)\n\nSaving model (epoch = 8098, MSE loss = 0.8111)\n\nSaving model (epoch = 8108, MSE loss = 0.8111)\n\nSaving model (epoch = 8118, MSE loss = 0.8111)\n\nSaving model (epoch = 8128, MSE loss = 0.8111)\n\nSaving model (epoch = 8138, MSE loss = 0.8111)\n\nSaving model (epoch = 8148, MSE loss = 0.8111)\n\nSaving model (epoch = 8168, MSE loss = 0.8111)\n\nSaving model (epoch = 8178, MSE loss = 0.8110)\n\nSaving model (epoch = 8188, MSE loss = 0.8110)\n\nSaving model (epoch = 8198, MSE loss = 0.8110)\n\nSaving model (epoch = 8238, MSE loss = 0.8110)\n\nSaving model (epoch = 8268, MSE loss = 0.8110)\n\nSaving model (epoch = 8278, MSE loss = 0.8110)\n\nSaving model (epoch = 8298, MSE loss = 0.8110)\n\nSaving model (epoch = 8318, MSE loss = 0.8110)\n\nSaving model (epoch = 8328, MSE loss = 0.8110)\n\nSaving model (epoch = 8348, MSE loss = 0.8109)\n\nSaving model (epoch = 8378, MSE loss = 0.8109)\n\nSaving model (epoch = 8468, MSE loss = 0.8109)\n\nSaving model (epoch = 8498, MSE loss = 0.8109)\n\nSaving model (epoch = 8508, MSE loss = 0.8109)\n\nSaving model (epoch = 8518, MSE loss = 0.8109)\n\nSaving model (epoch = 8528, MSE loss = 0.8109)\n\nSaving model (epoch = 8538, MSE loss = 0.8108)\n\nSaving model (epoch = 8548, MSE loss = 0.8108)\n\nSaving model (epoch = 8558, MSE loss = 0.8108)\n\nSaving model (epoch = 8568, MSE loss = 0.8108)\n\nSaving model (epoch = 8588, MSE loss = 0.8108)\n\nSaving model (epoch = 8598, MSE loss = 0.8108)\n\nSaving model (epoch = 8608, MSE loss = 0.8107)\n\nSaving model (epoch = 8628, MSE loss = 0.8107)\n\nSaving model (epoch = 8638, MSE loss = 0.8107)\n\nSaving model (epoch = 8648, MSE loss = 0.8107)\n\nSaving model (epoch = 8668, MSE loss = 0.8106)\n\nSaving model (epoch = 8678, MSE loss = 0.8106)\n\nSaving model (epoch = 8688, MSE loss = 0.8105)\n\nSaving model (epoch = 8698, MSE loss = 0.8105)\n\nSaving model (epoch = 8708, MSE loss = 0.8104)\n\nSaving model (epoch = 8718, MSE loss = 0.8104)\n\nSaving model (epoch = 8738, MSE loss = 0.8104)\n\nSaving model (epoch = 8748, MSE loss = 0.8104)\n\nSaving model (epoch = 8758, MSE loss = 0.8103)\n\nSaving model (epoch = 8768, MSE loss = 0.8103)\n\nSaving model (epoch = 8778, MSE loss = 0.8103)\n\nSaving model (epoch = 8788, MSE loss = 0.8103)\n\nSaving model (epoch = 8798, MSE loss = 0.8103)\n\nSaving model (epoch = 8808, MSE loss = 0.8103)\n\nSaving model (epoch = 8818, MSE loss = 0.8103)\n\nSaving model (epoch = 8828, MSE loss = 0.8102)\n\nSaving model (epoch = 8848, MSE loss = 0.8102)\n\nSaving model (epoch = 8858, MSE loss = 0.8102)\n\nSaving model (epoch = 8868, MSE loss = 0.8102)\n\nSaving model (epoch = 8878, MSE loss = 0.8102)\n\nSaving model (epoch = 8888, MSE loss = 0.8101)\n\nSaving model (epoch = 8898, MSE loss = 0.8101)\n\nSaving model (epoch = 8918, MSE loss = 0.8101)\n\nSaving model (epoch = 8928, MSE loss = 0.8101)\n\nSaving model (epoch = 8938, MSE loss = 0.8101)\n\nSaving model (epoch = 8948, MSE loss = 0.8101)\n\nSaving model (epoch = 8958, MSE loss = 0.8100)\n\nSaving model (epoch = 8968, MSE loss = 0.8100)\n\nSaving model (epoch = 8978, MSE loss = 0.8100)\n\nSaving model (epoch = 8988, MSE loss = 0.8100)\n\nSaving model (epoch = 8998, MSE loss = 0.8100)\n\nSaving model (epoch = 9008, MSE loss = 0.8099)\n\nSaving model (epoch = 9018, MSE loss = 0.8099)\n\nSaving model (epoch = 9028, MSE loss = 0.8099)\n\nSaving model (epoch = 9048, MSE loss = 0.8099)\n\nSaving model (epoch = 9068, MSE loss = 0.8099)\n\nSaving model (epoch = 9078, MSE loss = 0.8099)\n\nSaving model (epoch = 9088, MSE loss = 0.8098)\n\nSaving model (epoch = 9098, MSE loss = 0.8098)\n\nSaving model (epoch = 9108, MSE loss = 0.8098)\n\nSaving model (epoch = 9128, MSE loss = 0.8098)\n\nSaving model (epoch = 9138, MSE loss = 0.8098)\n\nSaving model (epoch = 9148, MSE loss = 0.8098)\n\nSaving model (epoch = 9158, MSE loss = 0.8098)\n\nSaving model (epoch = 9178, MSE loss = 0.8097)\n\nSaving model (epoch = 9188, MSE loss = 0.8097)\n\nSaving model (epoch = 9198, MSE loss = 0.8097)\n\nSaving model (epoch = 9208, MSE loss = 0.8097)\n\nSaving model (epoch = 9218, MSE loss = 0.8097)\n\nSaving model (epoch = 9238, MSE loss = 0.8097)\n\nSaving model (epoch = 9248, MSE loss = 0.8097)\n\nSaving model (epoch = 9268, MSE loss = 0.8097)\n\nSaving model (epoch = 9288, MSE loss = 0.8096)\n\nSaving model (epoch = 9298, MSE loss = 0.8096)\n\nSaving model (epoch = 9308, MSE loss = 0.8096)\n\nSaving model (epoch = 9328, MSE loss = 0.8096)\n\nSaving model (epoch = 9348, MSE loss = 0.8096)\n\nSaving model (epoch = 9368, MSE loss = 0.8096)\n\nSaving model (epoch = 9388, MSE loss = 0.8096)\n\nSaving model (epoch = 9408, MSE loss = 0.8095)\n\nSaving model (epoch = 9428, MSE loss = 0.8095)\n\nSaving model (epoch = 9448, MSE loss = 0.8095)\n\nSaving model (epoch = 9468, MSE loss = 0.8095)\n\nSaving model (epoch = 9488, MSE loss = 0.8095)\n\nSaving model (epoch = 9508, MSE loss = 0.8094)\n\nSaving model (epoch = 9528, MSE loss = 0.8094)\n\nSaving model (epoch = 9538, MSE loss = 0.8094)\n\nSaving model (epoch = 9548, MSE loss = 0.8094)\n\nSaving model (epoch = 9568, MSE loss = 0.8094)\n\nSaving model (epoch = 9578, MSE loss = 0.8094)\n\nSaving model (epoch = 9588, MSE loss = 0.8094)\n\nSaving model (epoch = 9608, MSE loss = 0.8093)\n\nSaving model (epoch = 9628, MSE loss = 0.8093)\n\nSaving model (epoch = 9648, MSE loss = 0.8093)\n\nSaving model (epoch = 9658, MSE loss = 0.8093)\n\nSaving model (epoch = 9678, MSE loss = 0.8093)\n\nSaving model (epoch = 9728, MSE loss = 0.8093)\n\nSaving model (epoch = 9748, MSE loss = 0.8093)\n\nSaving model (epoch = 9778, MSE loss = 0.8092)\n\nSaving model (epoch = 9788, MSE loss = 0.8092)\n\nSaving model (epoch = 9798, MSE loss = 0.8092)\n\nSaving model (epoch = 9808, MSE loss = 0.8092)\n\nSaving model (epoch = 9818, MSE loss = 0.8092)\n\nSaving model (epoch = 9828, MSE loss = 0.8092)\n\nSaving model (epoch = 9838, MSE loss = 0.8092)\n\nSaving model (epoch = 9848, MSE loss = 0.8092)\n\nSaving model (epoch = 9888, MSE loss = 0.8092)\n\nSaving model (epoch = 9908, MSE loss = 0.8092)\n\n---Early Stopping--- (Best epoch = 9908, Best MSE loss = 0.8092)\n"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfig = plt.figure()\nplt.plot(range(len(train_losses)), train_losses, color='red')\nplt.plot(range(len(train_losses)), test_losses, color='blue')\nplt.legend(['Train Loss', 'Test Loss'], loc='upper right')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')","metadata":{"execution":{"iopub.execute_input":"2022-09-24T17:14:48.884569Z","iopub.status.busy":"2022-09-24T17:14:48.884189Z","iopub.status.idle":"2022-09-24T17:14:49.160283Z","shell.execute_reply":"2022-09-24T17:14:49.159372Z"},"papermill":{"duration":0.311239,"end_time":"2022-09-24T17:14:49.162297","exception":false,"start_time":"2022-09-24T17:14:48.851058","status":"completed"},"tags":[]},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":["Text(0, 0.5, 'Loss')"]},"metadata":{}},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"best_network = Net().cuda()\nnetwork_state_dict = torch.load('/kaggle/model.pth')\nbest_network.load_state_dict(network_state_dict)","metadata":{"execution":{"iopub.execute_input":"2022-09-24T17:14:49.225851Z","iopub.status.busy":"2022-09-24T17:14:49.224205Z","iopub.status.idle":"2022-09-24T17:14:49.236682Z","shell.execute_reply":"2022-09-24T17:14:49.235760Z"},"papermill":{"duration":0.04624,"end_time":"2022-09-24T17:14:49.238853","exception":false,"start_time":"2022-09-24T17:14:49.192613","status":"completed"},"tags":[]},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"best_network.eval()\n\ntest_df = test_df.iloc[:,cols]\n\n# Standardization Feature\nfor col in test_df.columns:\n  test_df[col] = (test_df[col] - mean_std[col][0]) / mean_std[col][1]\n\ntest_X = test_df.values #Pandas to Numpy\ntest_X = torch.Tensor(test_X).cuda() #Numpy to Tensor\npred = best_network(test_X)","metadata":{"execution":{"iopub.execute_input":"2022-09-24T17:14:49.303503Z","iopub.status.busy":"2022-09-24T17:14:49.302744Z","iopub.status.idle":"2022-09-24T17:14:49.318702Z","shell.execute_reply":"2022-09-24T17:14:49.317647Z"},"papermill":{"duration":0.050337,"end_time":"2022-09-24T17:14:49.320706","exception":false,"start_time":"2022-09-24T17:14:49.270369","status":"completed"},"tags":[]},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"import csv\n\nprint('Saving results')\nwith open('/kaggle/working/submission.csv', \"w\") as fp:\n    writer = csv.writer(fp)\n    writer.writerow(['id', 'tested_positive'])\n    for i, p in enumerate(pred):\n        writer.writerow([i, p.item()])","metadata":{"execution":{"iopub.execute_input":"2022-09-24T17:14:49.502512Z","iopub.status.busy":"2022-09-24T17:14:49.500843Z","iopub.status.idle":"2022-09-24T17:14:49.520660Z","shell.execute_reply":"2022-09-24T17:14:49.519343Z"},"papermill":{"duration":0.054102,"end_time":"2022-09-24T17:14:49.523192","exception":false,"start_time":"2022-09-24T17:14:49.469090","status":"completed"},"tags":[]},"execution_count":18,"outputs":[{"name":"stdout","output_type":"stream","text":"Saving results\n"}]}]}