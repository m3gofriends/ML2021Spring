{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":1211.646194,"end_time":"2022-09-28T07:43:49.567693","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2022-09-28T07:23:37.921499","version":"2.3.4"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Strong baseline : 0.76023\n### ---My Best Score : 0.74975---\n### Simple baseline : 0.68334","metadata":{}},{"cell_type":"code","source":"# PyTorch\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# For data preprocess\nimport numpy as np\nimport csv\nimport os\n\n# For plotting\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\n\nmyseed = 1234  # set a random seed for reproducibility\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nnp.random.seed(myseed)\ntorch.manual_seed(myseed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(myseed)","metadata":{"execution":{"iopub.execute_input":"2022-09-28T07:23:45.629461Z","iopub.status.busy":"2022-09-28T07:23:45.627581Z","iopub.status.idle":"2022-09-28T07:23:47.359521Z","shell.execute_reply":"2022-09-28T07:23:47.358542Z"},"papermill":{"duration":1.744908,"end_time":"2022-09-28T07:23:47.362069","exception":false,"start_time":"2022-09-28T07:23:45.617161","status":"completed"},"tags":[]},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"print('Loading data ...')\n\ntrain = np.load('../input/ml2021spring-hw2/timit_11/timit_11/train_11.npy')\ntrain_label = np.load('../input/ml2021spring-hw2/timit_11/timit_11/train_label_11.npy')\ntest = np.load('../input/ml2021spring-hw2/timit_11/timit_11/test_11.npy').reshape((451552, 11, 39))\n\nprint('Size of training data: {}'.format(train.shape))\nprint('Size of testing data: {}'.format(test.shape))","metadata":{"execution":{"iopub.execute_input":"2022-09-28T07:23:47.371794Z","iopub.status.busy":"2022-09-28T07:23:47.370782Z","iopub.status.idle":"2022-09-28T07:24:28.945624Z","shell.execute_reply":"2022-09-28T07:24:28.944658Z"},"papermill":{"duration":41.583591,"end_time":"2022-09-28T07:24:28.949620","exception":false,"start_time":"2022-09-28T07:23:47.366029","status":"completed"},"tags":[]},"execution_count":2,"outputs":[{"name":"stdout","output_type":"stream","text":"Loading data ...\n\nSize of training data: (1229932, 429)\n\nSize of testing data: (451552, 11, 39)\n"}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\n\nclass TIMITDataset(Dataset):\n    def __init__(self, X, y=None):\n        self.data = torch.from_numpy(X).float()\n        if y is not None:\n            y = y.astype(int)\n            self.label = torch.LongTensor(y)\n        else:\n            self.label = None\n\n    def __getitem__(self, idx):\n        if self.label is not None:\n            return self.data[idx], self.label[idx]\n        else:\n            return self.data[idx]\n\n    def __len__(self):\n        return len(self.data)","metadata":{"execution":{"iopub.execute_input":"2022-09-28T07:24:28.967914Z","iopub.status.busy":"2022-09-28T07:24:28.966316Z","iopub.status.idle":"2022-09-28T07:24:28.973976Z","shell.execute_reply":"2022-09-28T07:24:28.972889Z"},"papermill":{"duration":0.014463,"end_time":"2022-09-28T07:24:28.976028","exception":false,"start_time":"2022-09-28T07:24:28.961565","status":"completed"},"tags":[]},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"VAL_RATIO = 0.1\n\npercent = int(train.shape[0] * (1 - VAL_RATIO))\ntrain_x, train_y, val_x, val_y = train[:percent], train_label[:percent], train[percent:], train_label[percent:]\n\ntrain_x = train_x.reshape((1106938, 11, 39))\nval_x = val_x.reshape((122994, 11, 39))\n\nprint('Size of training set: {}'.format(train_x.shape))\nprint('Size of validation set: {}'.format(val_x.shape))","metadata":{"execution":{"iopub.execute_input":"2022-09-28T07:24:28.985458Z","iopub.status.busy":"2022-09-28T07:24:28.984474Z","iopub.status.idle":"2022-09-28T07:24:28.992335Z","shell.execute_reply":"2022-09-28T07:24:28.991194Z"},"papermill":{"duration":0.015287,"end_time":"2022-09-28T07:24:28.995016","exception":false,"start_time":"2022-09-28T07:24:28.979729","status":"completed"},"tags":[]},"execution_count":4,"outputs":[{"name":"stdout","output_type":"stream","text":"Size of training set: (1106938, 11, 39)\n\nSize of validation set: (122994, 11, 39)\n"}]},{"cell_type":"code","source":"BATCH_SIZE = 512\n\nfrom torch.utils.data import DataLoader\n\ntrain_set = TIMITDataset(train_x, train_y)\nval_set = TIMITDataset(val_x, val_y)\ntrain_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True) #only shuffle the training data\nval_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)","metadata":{"execution":{"iopub.execute_input":"2022-09-28T07:24:29.004205Z","iopub.status.busy":"2022-09-28T07:24:29.003929Z","iopub.status.idle":"2022-09-28T07:24:31.096081Z","shell.execute_reply":"2022-09-28T07:24:31.095095Z"},"papermill":{"duration":2.099202,"end_time":"2022-09-28T07:24:31.098423","exception":false,"start_time":"2022-09-28T07:24:28.999221","status":"completed"},"tags":[]},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import gc\n\ndel train, train_label, train_x, train_y, val_x, val_y\ngc.collect()","metadata":{"execution":{"iopub.execute_input":"2022-09-28T07:24:31.107664Z","iopub.status.busy":"2022-09-28T07:24:31.107344Z","iopub.status.idle":"2022-09-28T07:24:31.221756Z","shell.execute_reply":"2022-09-28T07:24:31.220819Z"},"papermill":{"duration":0.121563,"end_time":"2022-09-28T07:24:31.224043","exception":false,"start_time":"2022-09-28T07:24:31.102480","status":"completed"},"tags":[]},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":["129"]},"metadata":{}}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass Classifier(nn.Module):\n    def __init__(self):\n        super(Classifier, self).__init__()\n        self.layer1 = nn.Linear(39, 1024)\n        self.layer2 = nn.Linear(1024, 512)\n        self.layer3 = nn.Linear(5632, 1024)\n        self.out = nn.Linear(1024, 39) \n\n        self.dropout = nn.Dropout(0.5)\n        self.act_fn = nn.ReLU()\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.dropout(x)\n        x = self.act_fn(x)\n\n        x = self.layer2(x)\n        x = self.dropout(x)\n        x = self.act_fn(x)\n\n        x = torch.flatten(x, 1)\n        x = self.layer3(x)\n        x = self.dropout(x)\n        x = self.act_fn(x)\n\n        x = self.out(x)\n        \n        return x","metadata":{"execution":{"iopub.execute_input":"2022-09-28T07:24:31.233787Z","iopub.status.busy":"2022-09-28T07:24:31.233162Z","iopub.status.idle":"2022-09-28T07:24:31.241650Z","shell.execute_reply":"2022-09-28T07:24:31.240796Z"},"papermill":{"duration":0.015559,"end_time":"2022-09-28T07:24:31.243623","exception":false,"start_time":"2022-09-28T07:24:31.228064","status":"completed"},"tags":[]},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#check device\ndef get_device():\n  return 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.execute_input":"2022-09-28T07:24:31.252532Z","iopub.status.busy":"2022-09-28T07:24:31.252218Z","iopub.status.idle":"2022-09-28T07:24:31.257682Z","shell.execute_reply":"2022-09-28T07:24:31.256864Z"},"papermill":{"duration":0.012179,"end_time":"2022-09-28T07:24:31.259708","exception":false,"start_time":"2022-09-28T07:24:31.247529","status":"completed"},"tags":[]},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# get device \ndevice = get_device()\nprint(f'DEVICE: {device}')\n\n# training parameters\nnum_epoch = 200               # number of training epoch\nlearning_rate = 0.0001       # learning rate\n\n# the path where checkpoint saved\nmodel_path = './model.ckpt'\n\n# create model, define a loss function, and optimizer\nmodel = Classifier().to(device)\ncriterion = nn.CrossEntropyLoss() \noptimizer = torch.optim.RAdam(model.parameters(),lr=learning_rate, weight_decay=1e-7)","metadata":{"execution":{"iopub.execute_input":"2022-09-28T07:24:31.276939Z","iopub.status.busy":"2022-09-28T07:24:31.275899Z","iopub.status.idle":"2022-09-28T07:24:34.644838Z","shell.execute_reply":"2022-09-28T07:24:34.643879Z"},"papermill":{"duration":3.376135,"end_time":"2022-09-28T07:24:34.647161","exception":false,"start_time":"2022-09-28T07:24:31.271026","status":"completed"},"tags":[]},"execution_count":9,"outputs":[{"name":"stdout","output_type":"stream","text":"DEVICE: cuda\n"}]},{"cell_type":"code","source":"ES_patience = 10\nES_counter = 0","metadata":{"execution":{"iopub.execute_input":"2022-09-28T07:24:34.656626Z","iopub.status.busy":"2022-09-28T07:24:34.656312Z","iopub.status.idle":"2022-09-28T07:24:34.660661Z","shell.execute_reply":"2022-09-28T07:24:34.659651Z"},"papermill":{"duration":0.011501,"end_time":"2022-09-28T07:24:34.662920","exception":false,"start_time":"2022-09-28T07:24:34.651419","status":"completed"},"tags":[]},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# start training\n\nbest_loss = 1000.0\nfor epoch in range(num_epoch):\n    train_acc = 0.0\n    train_loss = 0.0\n    val_acc = 0.0\n    val_loss = 0.0\n\n    # training\n    model.train() # set the model to training mode\n    for i, data in enumerate(train_loader):\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad() \n        outputs = model(inputs) \n        batch_loss = criterion(outputs, labels)\n        _, train_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n        batch_loss.backward() \n        optimizer.step() \n\n        train_acc += (train_pred.cpu() == labels.cpu()).sum().item()\n        train_loss += batch_loss.item()\n\n    # validation\n    if len(val_set) > 0:\n        model.eval() # set the model to evaluation mode\n        with torch.no_grad():\n            for i, data in enumerate(val_loader):\n                inputs, labels = data\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                batch_loss = criterion(outputs, labels) \n                _, val_pred = torch.max(outputs, 1) \n            \n                val_acc += (val_pred.cpu() == labels.cpu()).sum().item() # get the index of the class with the highest probability\n                val_loss += batch_loss.item()\n\n            print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f} | Val Acc: {:3.6f} loss: {:3.6f}'.format(\n                epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader), val_acc/len(val_set), val_loss/len(val_loader)\n            ))\n\n            # if the model improves, save a checkpoint at this epoch\n            if val_loss/len(val_loader) < best_loss:\n                ES_counter = -1\n                best_loss = val_loss/len(val_loader)\n                torch.save(model.state_dict(), model_path)\n                print('saving model with loss {:.3f}'.format(best_loss))\n    else:\n        print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f}'.format(\n            epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader)\n        ))\n    ES_counter += 1  \n    if ES_counter == ES_patience:\n        print('---Early Stopping---')\n        break","metadata":{"execution":{"iopub.execute_input":"2022-09-28T07:24:34.672010Z","iopub.status.busy":"2022-09-28T07:24:34.671735Z","iopub.status.idle":"2022-09-28T07:43:43.901096Z","shell.execute_reply":"2022-09-28T07:43:43.898674Z"},"papermill":{"duration":1149.237048,"end_time":"2022-09-28T07:43:43.903797","exception":false,"start_time":"2022-09-28T07:24:34.666749","status":"completed"},"tags":[]},"execution_count":11,"outputs":[{"name":"stdout","output_type":"stream","text":"[001/200] Train Acc: 0.505274 Loss: 1.680497 | Val Acc: 0.657073 loss: 1.107851\n\nsaving model with loss 1.108\n\n[002/200] Train Acc: 0.635395 Loss: 1.160648 | Val Acc: 0.698644 loss: 0.945956\n\nsaving model with loss 0.946\n\n[003/200] Train Acc: 0.669523 Loss: 1.036131 | Val Acc: 0.717279 loss: 0.872202\n\nsaving model with loss 0.872\n\n[004/200] Train Acc: 0.687897 Loss: 0.968223 | Val Acc: 0.726629 loss: 0.831120\n\nsaving model with loss 0.831\n\n[005/200] Train Acc: 0.700777 Loss: 0.922011 | Val Acc: 0.734044 loss: 0.805558\n\nsaving model with loss 0.806\n\n[006/200] Train Acc: 0.710426 Loss: 0.886998 | Val Acc: 0.740207 loss: 0.782898\n\nsaving model with loss 0.783\n\n[007/200] Train Acc: 0.718019 Loss: 0.859241 | Val Acc: 0.744459 loss: 0.766966\n\nsaving model with loss 0.767\n\n[008/200] Train Acc: 0.723907 Loss: 0.837101 | Val Acc: 0.748597 loss: 0.752816\n\nsaving model with loss 0.753\n\n[009/200] Train Acc: 0.729597 Loss: 0.816781 | Val Acc: 0.750524 loss: 0.744069\n\nsaving model with loss 0.744\n\n[010/200] Train Acc: 0.734277 Loss: 0.800152 | Val Acc: 0.753273 loss: 0.736608\n\nsaving model with loss 0.737\n\n[011/200] Train Acc: 0.737944 Loss: 0.785085 | Val Acc: 0.754053 loss: 0.729315\n\nsaving model with loss 0.729\n\n[012/200] Train Acc: 0.742497 Loss: 0.771236 | Val Acc: 0.756882 loss: 0.724132\n\nsaving model with loss 0.724\n\n[013/200] Train Acc: 0.745888 Loss: 0.758745 | Val Acc: 0.757078 loss: 0.719869\n\nsaving model with loss 0.720\n\n[014/200] Train Acc: 0.748912 Loss: 0.747284 | Val Acc: 0.758744 loss: 0.714311\n\nsaving model with loss 0.714\n\n[015/200] Train Acc: 0.751503 Loss: 0.737717 | Val Acc: 0.759606 loss: 0.714601\n\n[016/200] Train Acc: 0.754187 Loss: 0.727422 | Val Acc: 0.761647 loss: 0.711913\n\nsaving model with loss 0.712\n\n[017/200] Train Acc: 0.756775 Loss: 0.718720 | Val Acc: 0.763021 loss: 0.705280\n\nsaving model with loss 0.705\n\n[018/200] Train Acc: 0.759598 Loss: 0.710480 | Val Acc: 0.762070 loss: 0.705077\n\nsaving model with loss 0.705\n\n[019/200] Train Acc: 0.761241 Loss: 0.702608 | Val Acc: 0.762883 loss: 0.703540\n\nsaving model with loss 0.704\n\n[020/200] Train Acc: 0.763260 Loss: 0.696088 | Val Acc: 0.761590 loss: 0.703682\n\n[021/200] Train Acc: 0.765458 Loss: 0.689812 | Val Acc: 0.764484 loss: 0.699915\n\nsaving model with loss 0.700\n\n[022/200] Train Acc: 0.767388 Loss: 0.682257 | Val Acc: 0.762745 loss: 0.702255\n\n[023/200] Train Acc: 0.768602 Loss: 0.674952 | Val Acc: 0.764468 loss: 0.699340\n\nsaving model with loss 0.699\n\n[024/200] Train Acc: 0.771072 Loss: 0.669967 | Val Acc: 0.764070 loss: 0.699714\n\n[025/200] Train Acc: 0.772198 Loss: 0.664181 | Val Acc: 0.765615 loss: 0.695601\n\nsaving model with loss 0.696\n\n[026/200] Train Acc: 0.773453 Loss: 0.659622 | Val Acc: 0.765322 loss: 0.695846\n\n[027/200] Train Acc: 0.775856 Loss: 0.652837 | Val Acc: 0.765590 loss: 0.696097\n\n[028/200] Train Acc: 0.777000 Loss: 0.648479 | Val Acc: 0.766013 loss: 0.695239\n\nsaving model with loss 0.695\n\n[029/200] Train Acc: 0.778599 Loss: 0.644530 | Val Acc: 0.766696 loss: 0.697010\n\n[030/200] Train Acc: 0.779340 Loss: 0.639507 | Val Acc: 0.765858 loss: 0.697772\n\n[031/200] Train Acc: 0.780544 Loss: 0.635359 | Val Acc: 0.765663 loss: 0.696046\n\n[032/200] Train Acc: 0.781605 Loss: 0.630860 | Val Acc: 0.767558 loss: 0.696747\n\n[033/200] Train Acc: 0.782550 Loss: 0.627680 | Val Acc: 0.767387 loss: 0.694802\n\nsaving model with loss 0.695\n\n[034/200] Train Acc: 0.783718 Loss: 0.623557 | Val Acc: 0.766420 loss: 0.697081\n\n[035/200] Train Acc: 0.784533 Loss: 0.620818 | Val Acc: 0.766517 loss: 0.695660\n\n[036/200] Train Acc: 0.785766 Loss: 0.617078 | Val Acc: 0.766826 loss: 0.696293\n\n[037/200] Train Acc: 0.787143 Loss: 0.612378 | Val Acc: 0.766883 loss: 0.697611\n\n[038/200] Train Acc: 0.788223 Loss: 0.609900 | Val Acc: 0.766525 loss: 0.696166\n\n[039/200] Train Acc: 0.789304 Loss: 0.606726 | Val Acc: 0.766541 loss: 0.697053\n\n[040/200] Train Acc: 0.790206 Loss: 0.603575 | Val Acc: 0.767208 loss: 0.696916\n\n[041/200] Train Acc: 0.790919 Loss: 0.600112 | Val Acc: 0.766907 loss: 0.697757\n\n[042/200] Train Acc: 0.791734 Loss: 0.596695 | Val Acc: 0.767224 loss: 0.695907\n\n[043/200] Train Acc: 0.792714 Loss: 0.594551 | Val Acc: 0.766460 loss: 0.699656\n\n---Early Stopping---\n"}]},{"cell_type":"code","source":"# create testing dataset\ntest_set = TIMITDataset(test, None)\ntest_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n\n# create model and load weights from checkpoint\nmodel = Classifier().to(device)\nmodel.load_state_dict(torch.load(model_path))","metadata":{"execution":{"iopub.execute_input":"2022-09-28T07:43:43.918968Z","iopub.status.busy":"2022-09-28T07:43:43.918031Z","iopub.status.idle":"2022-09-28T07:43:44.614475Z","shell.execute_reply":"2022-09-28T07:43:44.613516Z"},"papermill":{"duration":0.70599,"end_time":"2022-09-28T07:43:44.616623","exception":false,"start_time":"2022-09-28T07:43:43.910633","status":"completed"},"tags":[]},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{}}]},{"cell_type":"code","source":"predict = []\nmodel.eval() # set the model to evaluation mode\nwith torch.no_grad():\n    for i, data in enumerate(test_loader):\n        inputs = data\n        inputs = inputs.to(device)\n        outputs = model(inputs)\n        _, test_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n\n        for y in test_pred.cpu().numpy():\n            predict.append(y)","metadata":{"execution":{"iopub.execute_input":"2022-09-28T07:43:44.630350Z","iopub.status.busy":"2022-09-28T07:43:44.630054Z","iopub.status.idle":"2022-09-28T07:43:47.893339Z","shell.execute_reply":"2022-09-28T07:43:47.892352Z"},"papermill":{"duration":3.273061,"end_time":"2022-09-28T07:43:47.895974","exception":false,"start_time":"2022-09-28T07:43:44.622913","status":"completed"},"tags":[]},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"with open('submission.csv', 'w') as f:\n    f.write('Id,Class\\n')\n    for i, y in enumerate(predict):\n        f.write('{},{}\\n'.format(i, y))","metadata":{"execution":{"iopub.execute_input":"2022-09-28T07:43:47.909767Z","iopub.status.busy":"2022-09-28T07:43:47.909431Z","iopub.status.idle":"2022-09-28T07:43:48.233030Z","shell.execute_reply":"2022-09-28T07:43:48.232064Z"},"papermill":{"duration":0.333056,"end_time":"2022-09-28T07:43:48.235350","exception":false,"start_time":"2022-09-28T07:43:47.902294","status":"completed"},"tags":[]},"execution_count":14,"outputs":[]}]}